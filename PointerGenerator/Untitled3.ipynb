{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "stHVxNZGP2-u"
      },
      "source": [
        "# !pip install -U -q PyDrive\n",
        "\n",
        "# from pydrive.auth import GoogleAuth\n",
        "# from pydrive.drive import GoogleDrive\n",
        "# from google.colab import auth\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "# # Authenticate and create the PyDrive client.\n",
        "# # This only needs to be done once per notebook.\n",
        "# auth.authenticate_user()\n",
        "# gauth = GoogleAuth()\n",
        "# gauth.credentials = GoogleCredentials.get_application_default()\n",
        "# drive = GoogleDrive(gauth)\n",
        "\n",
        "# file_id = '0BzQ6rtO2VN95a0c3TlZCWkl3aU0' # URL id. \n",
        "# downloaded = drive.CreateFile({'id': file_id})\n",
        "# downloaded.GetContentFile('finished_files.zip')\n",
        "# !unzip finished_files.zip\n",
        "\n",
        "# !pip install pyrouge\n",
        "# !mkdir log\n",
        "# !pip install tensorflow==1.15.0\n",
        "# !pip install tensorflow-gpu==1.15.0"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9gVYLcCP8Jp"
      },
      "source": [
        "import torch\n",
        "\n",
        "import config\n",
        "from numpy import random\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_packed_sequence\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import tensorflow as tf\n",
        "import math\n",
        "\n",
        "use_cuda = config.use_gpu and torch.cuda.is_available()\n",
        "\n",
        "random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(123)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKKzikd7RCVa"
      },
      "source": [
        "class BasicModule(nn.Module):\n",
        "    def __init__(self, init='uniform'):\n",
        "        super(BasicModule, self).__init__()\n",
        "        self.init = init\n",
        "\n",
        "    def init_params(self):\n",
        "        for param in self.parameters():\n",
        "            if param.requires_grad and len(param.shape) > 0:\n",
        "                stddev = 1 / math.sqrt(param.shape[0])\n",
        "                if self.init == 'uniform':\n",
        "                    torch.nn.init.uniform_(param, a=-0.05, b=0.05)\n",
        "                elif self.init == 'normal':\n",
        "                    torch.nn.init.normal_(param, std=stddev)\n",
        "                elif self.init == 'truncated_normal':\n",
        "                    self.truncated_normal_(param, mean=0,std=stddev)\n",
        "\n",
        "    def truncated_normal_(self, tensor, mean=0, std=1.):\n",
        "        size = tensor.shape\n",
        "        tmp = tensor.new_empty(size + (4,)).normal_()\n",
        "        valid = (tmp < 2) & (tmp > -2)\n",
        "        ind = valid.max(-1, keepdim=True)[1]\n",
        "        tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n",
        "        tensor.data.mul_(std).add_(mean)\n",
        "        return tensor"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DE_cVBrNQOiU"
      },
      "source": [
        "class Model(object):\n",
        "    def __init__(self, model_path=None, is_eval=False, is_tran = False):\n",
        "        encoder = Encoder()\n",
        "        decoder = Decoder()\n",
        "        reduce_state = ReduceState()\n",
        "        if is_tran:\n",
        "            encoder = TranEncoder(config.vocab_size, config.max_enc_steps, config.emb_dim,\n",
        "                 config.n_layers, config.n_head, config.d_k, config.d_v, config.d_model, config.d_inner)\n",
        "\n",
        "        # shared the embedding between encoder and decoder\n",
        "        decoder.tgt_word_emb.weight = encoder.src_word_emb.weight\n",
        "\n",
        "        if is_eval:\n",
        "            encoder = encoder.eval()\n",
        "            decoder = decoder.eval()\n",
        "            reduce_state = reduce_state.eval()\n",
        "\n",
        "        if use_cuda:\n",
        "            encoder = encoder.cuda()\n",
        "            decoder = decoder.cuda()\n",
        "            reduce_state = reduce_state.cuda()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.reduce_state = reduce_state\n",
        "\n",
        "        if model_path is not None:\n",
        "            state = torch.load(model_path, map_location=lambda storage, location: storage)\n",
        "            self.encoder.load_state_dict(state['encoder_state_dict'])\n",
        "            self.decoder.load_state_dict(state['decoder_state_dict'], strict=False)\n",
        "            self.reduce_state.load_state_dict(state['reduce_state_dict'])\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4QbzH4wQQmf"
      },
      "source": [
        "class Encoder(BasicModule):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.src_word_emb = nn.Embedding(config.vocab_size, config.emb_dim)\n",
        "        self.lstm = nn.LSTM(config.emb_dim, config.hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(config.hidden_dim * 2, config.hidden_dim * 2, bias=False)\n",
        "\n",
        "        self.init_params()\n",
        "\n",
        "    # seq_lens should be in descending order\n",
        "    def forward(self, input, seq_lens):\n",
        "        embedded = self.src_word_emb(input)\n",
        "\n",
        "        packed = pack_padded_sequence(embedded, seq_lens, batch_first=True)\n",
        "        output, hidden = self.lstm(packed)\n",
        "\n",
        "        encoder_outputs, _ = pad_packed_sequence(output, batch_first=True)  # h dim = B x l x n\n",
        "        encoder_outputs = encoder_outputs.contiguous()\n",
        "\n",
        "        encoder_feature = encoder_outputs.view(-1, 2 * config.hidden_dim)   # B*l x 2*hidden_dim\n",
        "        encoder_feature = self.fc(encoder_feature)\n",
        "\n",
        "        return encoder_outputs, encoder_feature, hidden"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVmilN7aRhlD"
      },
      "source": [
        "class Attention(BasicModule):\n",
        "    def __init__(self):\n",
        "        super(Attention, self).__init__()\n",
        "\n",
        "        self.fc = nn.Linear(config.hidden_dim * 2, 1, bias=False)\n",
        "        self.dec_fc = nn.Linear(config.hidden_dim * 2, config.hidden_dim * 2)\n",
        "        if config.is_coverage:\n",
        "            self.con_fc = nn.Linear(1, config.hidden_dim * 2, bias=False)\n",
        "\n",
        "        self.init_params()\n",
        "\n",
        "    def forward(self, s_t, enc_out, enc_fea, enc_padding_mask, coverage):\n",
        "        b, l, n = list(enc_out.size())\n",
        "\n",
        "        dec_fea = self.dec_fc(s_t)  # B x 2*hidden_dim\n",
        "        dec_fea_expanded = dec_fea.unsqueeze(1).expand(b, l, n).contiguous()  # B x l x 2*hidden_dim\n",
        "        dec_fea_expanded = dec_fea_expanded.view(-1, n)     # B*l x 2*hidden_dim\n",
        "\n",
        "        att_features = enc_fea + dec_fea_expanded           # B*l x 2*hidden_dim\n",
        "        if config.is_coverage:\n",
        "            coverage_inp = coverage.view(-1, 1)             # B*l x 1\n",
        "            coverage_fea = self.con_fc(coverage_inp)        # B*l x 2*hidden_dim\n",
        "            att_features = att_features + coverage_fea\n",
        "\n",
        "        e = torch.tanh(att_features)                        # B*l x 2*hidden_dim\n",
        "        scores = self.fc(e)                                 # B*l x 1\n",
        "        scores = scores.view(-1, l)                         # B x l\n",
        "\n",
        "        attn_dist_ = F.softmax(scores, dim=1) * enc_padding_mask  # B x l\n",
        "        normalization_factor = attn_dist_.sum(1, keepdim=True)\n",
        "        attn_dist = attn_dist_ / normalization_factor\n",
        "\n",
        "        attn_dist = attn_dist.unsqueeze(1)                        # B x 1 x l\n",
        "        c_t = torch.bmm(attn_dist, enc_out)                       # B x 1 x n\n",
        "        c_t = c_t.view(-1, config.hidden_dim * 2)                 # B x 2*hidden_dim\n",
        "\n",
        "        attn_dist = attn_dist.view(-1, l)                         # B x l\n",
        "\n",
        "        if config.is_coverage:\n",
        "            coverage = coverage.view(-1, l)\n",
        "            coverage = coverage + attn_dist\n",
        "\n",
        "        return c_t, attn_dist, coverage"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9pXte-aQpM9"
      },
      "source": [
        "class ReduceState(BasicModule):\n",
        "    def __init__(self):\n",
        "        super(ReduceState, self).__init__()\n",
        "\n",
        "        self.reduce_h = nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
        "        self.reduce_c = nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
        "        self.init_params()\n",
        "\n",
        "\n",
        "    def forward(self, hidden):\n",
        "        h, c = hidden  # h, c dim = 2 x b x hidden_dim\n",
        "        h_in = h.transpose(0, 1).contiguous().view(-1, config.hidden_dim * 2)\n",
        "        hidden_reduced_h = F.relu(self.reduce_h(h_in))\n",
        "        c_in = c.transpose(0, 1).contiguous().view(-1, config.hidden_dim * 2)\n",
        "        hidden_reduced_c = F.relu(self.reduce_c(c_in))\n",
        "\n",
        "        return (hidden_reduced_h.unsqueeze(0), hidden_reduced_c.unsqueeze(0))  # h, c dim = 1 x b x hidden_dim"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzLd_oC8RdKi"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DEx5mN3RF3Z"
      },
      "source": [
        "class Decoder(BasicModule):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.attention_network = Attention()\n",
        "        # decoder\n",
        "        self.tgt_word_emb = nn.Embedding(config.vocab_size, config.emb_dim)\n",
        "        self.con_fc = nn.Linear(config.hidden_dim * 2 + config.emb_dim, config.emb_dim)\n",
        "        self.lstm = nn.LSTM(config.emb_dim, config.hidden_dim, batch_first=True, bidirectional=False)\n",
        "\n",
        "        if config.pointer_gen:\n",
        "            self.p_gen_fc = nn.Linear(config.hidden_dim * 4 + config.emb_dim, 1)\n",
        "\n",
        "        # p_vocab\n",
        "        self.fc1 = nn.Linear(config.hidden_dim * 3, config.hidden_dim)\n",
        "        self.fc2 = nn.Linear(config.hidden_dim, config.vocab_size)\n",
        "\n",
        "        self.init_params()\n",
        "\n",
        "    def forward(self, y_t, s_t, enc_out, enc_fea, enc_padding_mask,\n",
        "                c_t, extra_zeros, enc_batch_extend_vocab, coverage, step):\n",
        "\n",
        "        if not self.training and step == 0:\n",
        "            dec_h, dec_c = s_t\n",
        "            s_t_hat = torch.cat((dec_h.view(-1, config.hidden_dim),\n",
        "                                 dec_c.view(-1, config.hidden_dim)), 1)  # B x 2*hidden_dim\n",
        "            c_t, _, coverage_next = self.attention_network(s_t_hat, enc_out, enc_fea,\n",
        "                                                           enc_padding_mask, coverage)\n",
        "            coverage = coverage_next\n",
        "\n",
        "        y_t_embd = self.tgt_word_emb(y_t)\n",
        "        x = self.con_fc(torch.cat((c_t, y_t_embd), 1))\n",
        "        lstm_out, s_t = self.lstm(x.unsqueeze(1), s_t)\n",
        "\n",
        "        dec_h, dec_c = s_t\n",
        "        s_t_hat = torch.cat((dec_h.view(-1, config.hidden_dim),\n",
        "                             dec_c.view(-1, config.hidden_dim)), 1)     # B x 2*hidden_dim\n",
        "        c_t, attn_dist, coverage_next = self.attention_network(s_t_hat, enc_out, enc_fea,\n",
        "                                                               enc_padding_mask, coverage)\n",
        "\n",
        "        if self.training or step > 0:\n",
        "            coverage = coverage_next\n",
        "\n",
        "        p_gen = None\n",
        "        if config.pointer_gen:\n",
        "            p_gen_inp = torch.cat((c_t, s_t_hat, x), 1)  # B x (2*2*hidden_dim + emb_dim)\n",
        "            p_gen = self.p_gen_fc(p_gen_inp)\n",
        "            p_gen = torch.sigmoid(p_gen)\n",
        "\n",
        "        output = torch.cat((lstm_out.view(-1, config.hidden_dim), c_t), 1)  # B x hidden_dim * 3\n",
        "        output = self.fc1(output)  # B x hidden_dim\n",
        "        # output = F.relu(output)\n",
        "\n",
        "        output = self.fc2(output)  # B x vocab_size\n",
        "        vocab_dist = F.softmax(output, dim=1)\n",
        "\n",
        "        if config.pointer_gen:\n",
        "            vocab_dist_ = p_gen * vocab_dist\n",
        "            attn_dist_ = (1 - p_gen) * attn_dist\n",
        "\n",
        "            if extra_zeros is not None:\n",
        "                vocab_dist_ = torch.cat([vocab_dist_, extra_zeros], 1)\n",
        "\n",
        "            final_dist = vocab_dist_.scatter_add(1, enc_batch_extend_vocab, attn_dist_)\n",
        "        else:\n",
        "            final_dist = vocab_dist\n",
        "\n",
        "        return final_dist, s_t, c_t, attn_dist, p_gen, coverage"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk8dO3Z-RLnO"
      },
      "source": [
        "from dataset import Vocab\n",
        "from dataset import Batcher\n",
        "from utils import get_input_from_batch\n",
        "from utils import get_output_from_batch\n",
        "from utils import calc_running_avg_loss\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import time\n",
        "import os"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEhi3kqmSGn1"
      },
      "source": [
        "use_cuda = config.use_gpu and torch.cuda.is_available()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wITrwkEWSjvO"
      },
      "source": [
        "class Train(object):\n",
        "    def __init__(self):\n",
        "        print(config.vocab_path)\n",
        "        self.vocab = Vocab(config.vocab_path, config.vocab_size)\n",
        "        self.batcher = Batcher(self.vocab, config.train_data_path,\n",
        "                               config.batch_size, single_pass=False, mode='train')\n",
        "        time.sleep(10)\n",
        "\n",
        "        train_dir = os.path.join(config.log_root, 'train_%d' % (int(time.time())))\n",
        "        if not os.path.exists(train_dir):\n",
        "            os.mkdir(train_dir)\n",
        "\n",
        "        self.model_dir = os.path.join(train_dir, 'models')\n",
        "        if not os.path.exists(self.model_dir):\n",
        "            os.mkdir(self.model_dir)\n",
        "\n",
        "        self.summary_writer = tf.summary.FileWriter(train_dir)\n",
        "\n",
        "    def save_model(self, running_avg_loss, iter):\n",
        "        state = {\n",
        "            'iter': iter,\n",
        "            'encoder_state_dict': self.model.encoder.state_dict(),\n",
        "            'decoder_state_dict': self.model.decoder.state_dict(),\n",
        "            'reduce_state_dict': self.model.reduce_state.state_dict(),\n",
        "            'optimizer': self.optimizer.state_dict(),\n",
        "            'current_loss': running_avg_loss\n",
        "        }\n",
        "        model_save_path = os.path.join(self.model_dir, 'model_%d_%d' % (iter, int(time.time())))\n",
        "        torch.save(state, model_save_path)\n",
        "\n",
        "    def setup_train(self, model_path=None):\n",
        "        self.model = Model(model_path)\n",
        "        initial_lr = config.lr_coverage if config.is_coverage else config.lr\n",
        "\n",
        "        params = list(self.model.encoder.parameters()) + list(self.model.decoder.parameters()) + \\\n",
        "                 list(self.model.reduce_state.parameters())\n",
        "        total_params = sum([param[0].nelement() for param in params])\n",
        "        print('The Number of params of model: %.3f million' % (total_params))  # million\n",
        "        self.optimizer = optim.Adagrad(params, lr=initial_lr, initial_accumulator_value=config.adagrad_init_acc)\n",
        "\n",
        "        start_iter, start_loss = 0, 0\n",
        "\n",
        "        if model_path is not None:\n",
        "            state = torch.load(model_path, map_location=lambda storage, location: storage)\n",
        "            start_iter = state['iter']\n",
        "            start_loss = state['current_loss']\n",
        "\n",
        "            if not config.is_coverage:\n",
        "                self.optimizer.load_state_dict(state['optimizer'])\n",
        "                if use_cuda:\n",
        "                    for state in self.optimizer.state.values():\n",
        "                        for k, v in state.items():\n",
        "                            if torch.is_tensor(v):\n",
        "                                state[k] = v.cuda()\n",
        "\n",
        "        return start_iter, start_loss\n",
        "\n",
        "    def train_one_batch(self, batch):\n",
        "        enc_batch, enc_lens, enc_pos, enc_padding_mask, enc_batch_extend_vocab, \\\n",
        "        extra_zeros, c_t, coverage = get_input_from_batch(batch, use_cuda)\n",
        "        dec_batch, dec_lens, dec_pos, dec_padding_mask, max_dec_len, tgt_batch = \\\n",
        "            get_output_from_batch(batch, use_cuda)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        if not config.tran:\n",
        "            enc_out, enc_fea, enc_h = self.model.encoder(enc_batch, enc_lens)\n",
        "        else:\n",
        "            enc_out, enc_fea, enc_h = self.model.encoder(enc_batch, enc_pos)\n",
        "\n",
        "        s_t = self.model.reduce_state(enc_h)\n",
        "\n",
        "        step_losses, cove_losses = [], []\n",
        "        for di in range(min(max_dec_len, config.max_dec_steps)):\n",
        "            y_t = dec_batch[:, di]  # Teacher forcing\n",
        "            final_dist, s_t, c_t, attn_dist, p_gen, next_coverage = \\\n",
        "                self.model.decoder(y_t, s_t, enc_out, enc_fea, enc_padding_mask, c_t,\n",
        "                                   extra_zeros, enc_batch_extend_vocab, coverage, di)\n",
        "            tgt = tgt_batch[:, di]\n",
        "            step_mask = dec_padding_mask[:, di]\n",
        "            gold_probs = torch.gather(final_dist, 1, tgt.unsqueeze(1)).squeeze()\n",
        "            step_loss = -torch.log(gold_probs + config.eps)\n",
        "            if config.is_coverage:\n",
        "                step_coverage_loss = torch.sum(torch.min(attn_dist, coverage), 1)\n",
        "                step_loss = step_loss + config.cov_loss_wt * step_coverage_loss\n",
        "                cove_losses.append(step_coverage_loss * step_mask)\n",
        "                coverage = next_coverage\n",
        "\n",
        "            step_loss = step_loss * step_mask\n",
        "            step_losses.append(step_loss)\n",
        "\n",
        "        sum_losses = torch.sum(torch.stack(step_losses, 1), 1)\n",
        "        batch_avg_loss = sum_losses / dec_lens\n",
        "        loss = torch.mean(batch_avg_loss)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        clip_grad_norm_(self.model.encoder.parameters(), config.max_grad_norm)\n",
        "        clip_grad_norm_(self.model.decoder.parameters(), config.max_grad_norm)\n",
        "        clip_grad_norm_(self.model.reduce_state.parameters(), config.max_grad_norm)\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "        if config.is_coverage:\n",
        "            cove_losses = torch.sum(torch.stack(cove_losses, 1), 1)\n",
        "            batch_cove_loss = cove_losses / dec_lens\n",
        "            batch_cove_loss = torch.mean(batch_cove_loss)\n",
        "            return loss.item(), batch_cove_loss.item()\n",
        "\n",
        "        return loss.item(), 0.\n",
        "\n",
        "    def run(self, n_iters, model_path=None):\n",
        "        iter, running_avg_loss = self.setup_train(model_path)\n",
        "        start = time.time()\n",
        "        interval = 100\n",
        "\n",
        "        while iter < n_iters:\n",
        "            batch = self.batcher.next_batch()\n",
        "            loss, cove_loss = self.train_one_batch(batch)\n",
        "\n",
        "            running_avg_loss = calc_running_avg_loss(loss, running_avg_loss, self.summary_writer, iter)\n",
        "            iter += 1\n",
        "\n",
        "            if iter % interval == 0:\n",
        "                self.summary_writer.flush()\n",
        "                print(\n",
        "                    'step: %d, second: %.2f , loss: %f, cover_loss: %f' % (iter, time.time() - start, loss, cove_loss))\n",
        "                start = time.time()\n",
        "            if iter % 5000 == 0:\n",
        "                self.save_model(running_avg_loss, iter)\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wW9A_RejSsTi",
        "outputId": "6bbf4bbc-b9a6-4f32-ee83-185885b05ba2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "train_processor = Train()\n",
        "train_processor.run(config.max_iterations)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./finished_files/vocab\n",
            "HERERERERERE\n",
            "Finished constructing vocabulary of 50 total words. Last word added: their\n",
            "INFO:tensorflow:Bucket queue size: 15, Input queue size: 0\n",
            "The Number of params of model: 314.000 million\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKCoIIeBUDjq"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mk-yhgglVJnO"
      },
      "source": [
        "import config\n",
        "print(config.beam_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ubO_tBaV23z"
      },
      "source": [
        "!cat config.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxM_EN8QWOUx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}