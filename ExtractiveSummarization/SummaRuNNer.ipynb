{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IRE_25GB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHiloSDrTlZc"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext.data import Field\n",
        "from torchtext.data import BucketIterator\n",
        "\n",
        "# from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import time\n",
        "import math\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import gensim\n",
        "\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Parameter\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device = torch.device('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LXF_u9eTRyF",
        "outputId": "8425e1a3-d3b1-4c84-d22a-c2811652a7e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPU0rzacTSPL"
      },
      "source": [
        "from gensim import models\n",
        "model = models.KeyedVectors.load_word2vec_format('/content/gdrive/My Drive/IRE_PROJECT/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtVfBAy9Tr9i"
      },
      "source": [
        "train_dataset={'story':[],'labels':[]}\n",
        "for line in open('/content/gdrive/My Drive/IRE_PROJECT/train_data.json'):\n",
        "  data=json.loads(line)\n",
        "  # print(data['story'])\n",
        "  # print(data['labels'])\n",
        "  train_dataset['story'].append(data['story'])\n",
        "  train_dataset['labels'].append(data['labels'])\n",
        "# train=json.loads(pathlib.Path('/content/train_data.json').read_text())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjE5v_XWwc7Q"
      },
      "source": [
        "validation_dataset={'story':[],'labels':[]}\n",
        "for line in open('/content/gdrive/My Drive/IRE_PROJECT/validation_data.json'):\n",
        "  data=json.loads(line)\n",
        "  # print(data['story'])\n",
        "  # print(data['labels'])\n",
        "  validation_dataset['story'].append(data['story'])\n",
        "  validation_dataset['labels'].append(data['labels'])\n",
        "# train=json.loads(pathlib.Path('/content/train_data.json').read_text())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwQXy5EqTvA1"
      },
      "source": [
        "train_df = pd.DataFrame(train_dataset, columns=[\"story\", \"labels\"])\n",
        "validation_df=pd.DataFrame(validation_dataset, columns=[\"story\", \"labels\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjRqssoIT9lY",
        "outputId": "ba5893b7-c31d-4deb-cd2c-6570cf529240",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "train_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>story</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[Paris ( CNN ) -- Flamboyant fashion designer ...</td>\n",
              "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[( CNN ), -- I spent much of Sunday in touch w...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[New York ( CNN ) --, The head of the Internat...</td>\n",
              "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[( CNN ) --, All the self-righteous huffing an...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[Los Angeles ( CNN ) --, Tour manager Paul Gon...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3235</th>\n",
              "      <td>[( CNN ), -- South Korea will attempt a second...</td>\n",
              "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3236</th>\n",
              "      <td>[( Mental Floss ) --, It 's hard to walk down ...</td>\n",
              "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3237</th>\n",
              "      <td>[Yangon , Myanmar ( CNN ) -- Myanmar oppositio...</td>\n",
              "      <td>[1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3238</th>\n",
              "      <td>[( CNN ) -- Not only is Sheikha Lubna Al Qasim...</td>\n",
              "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3239</th>\n",
              "      <td>[Rio de Janeiro ( CNN ) --, A massive crowd of...</td>\n",
              "      <td>[0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3240 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  story                                             labels\n",
              "0     [Paris ( CNN ) -- Flamboyant fashion designer ...  [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...\n",
              "1     [( CNN ), -- I spent much of Sunday in touch w...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "2     [New York ( CNN ) --, The head of the Internat...  [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...\n",
              "3     [( CNN ) --, All the self-righteous huffing an...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, ...\n",
              "4     [Los Angeles ( CNN ) --, Tour manager Paul Gon...  [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "...                                                 ...                                                ...\n",
              "3235  [( CNN ), -- South Korea will attempt a second...  [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...\n",
              "3236  [( Mental Floss ) --, It 's hard to walk down ...  [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...\n",
              "3237  [Yangon , Myanmar ( CNN ) -- Myanmar oppositio...  [1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...\n",
              "3238  [( CNN ) -- Not only is Sheikha Lubna Al Qasim...  [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...\n",
              "3239  [Rio de Janeiro ( CNN ) --, A massive crowd of...  [0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "\n",
              "[3240 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxwWx_mOyam0",
        "outputId": "eee52efa-103d-48e2-f764-43dab30001af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "validation_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>story</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[( CNN ) --, Sporting a black t-shirt proudly ...</td>\n",
              "      <td>[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[WASHINGTON ( CNN ) -- How does the American p...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[( CNN ) --, Basketball star Dennis Rodman see...</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[( CNN ), -- Check with your airline before yo...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[RIYADH , Saudi Arabia --, Ahmad al Shayea is ...</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>355</th>\n",
              "      <td>[( CNN ) --, The World Health Organization rai...</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>356</th>\n",
              "      <td>[Dali , Guizhou , China ( CNN ) -- Shi Wenchan...</td>\n",
              "      <td>[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>357</th>\n",
              "      <td>[Cairo , Egypt ( CNN ) -- One man died and ano...</td>\n",
              "      <td>[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>358</th>\n",
              "      <td>[London ( CNN ) -- The cleanup operation conti...</td>\n",
              "      <td>[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>359</th>\n",
              "      <td>[Washington ( CNN ) -- What a difference ., Ba...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>360 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 story                                             labels\n",
              "0    [( CNN ) --, Sporting a black t-shirt proudly ...  [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...\n",
              "1    [WASHINGTON ( CNN ) -- How does the American p...  [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, ...\n",
              "2    [( CNN ) --, Basketball star Dennis Rodman see...  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...\n",
              "3    [( CNN ), -- Check with your airline before yo...  [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...\n",
              "4    [RIYADH , Saudi Arabia --, Ahmad al Shayea is ...  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...\n",
              "..                                                 ...                                                ...\n",
              "355  [( CNN ) --, The World Health Organization rai...  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...\n",
              "356  [Dali , Guizhou , China ( CNN ) -- Shi Wenchan...  [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...\n",
              "357  [Cairo , Egypt ( CNN ) -- One man died and ano...  [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "358  [London ( CNN ) -- The cleanup operation conti...  [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\n",
              "359  [Washington ( CNN ) -- What a difference ., Ba...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, ...\n",
              "\n",
              "[360 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ne0kfTYTxGg"
      },
      "source": [
        "weights = model.vectors\n",
        "weights = np.insert(weights, 0,np.zeros(300),axis = 0)\n",
        "weights = np.insert(weights, 0,np.zeros(300),axis = 0)\n",
        "weights = torch.FloatTensor(weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09N-JpHvUABM"
      },
      "source": [
        "word2id = {model.index2word[i]:i+2 for i in range(len(model.index2word))}\n",
        "word2id['<pad>'] = 0\n",
        "word2id['<unk>'] = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjejcmkdWtMa"
      },
      "source": [
        "class Dataset():\n",
        "    def __init__(self, data_list):\n",
        "        self._data = data_list\n",
        "    def __len__(self):\n",
        "        return len(self._data)\n",
        "        \n",
        "    def __call__(self, batch_size, shuffle = True):\n",
        "        max_len = len(self)\n",
        "        \n",
        "        indices = [i for i in range(0,max_len,batch_size)]\n",
        "        \n",
        "        if shuffle:\n",
        "            indices = np.random.randint(0,max_len-1,math.ceil(max_len/batch_size))\n",
        "            \n",
        "        batchs = [self._data[index:index + batch_size] for index in indices]\n",
        "        return batchs\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self._data.iloc[index,:].to_numpy()\n",
        "    \n",
        "class DataLoader():\n",
        "    def __init__(self, dataset, batch_size = 1, shuffle = True):\n",
        "        assert isinstance(dataset, Dataset)\n",
        "        assert len(dataset) >= batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "    def __iter__(self):\n",
        "        return iter(self.dataset(self.batch_size, self.shuffle))\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyu4n2zx_nXq"
      },
      "source": [
        "class RNN_RNN(nn.Module):\n",
        "    def __init__(self, embed=None):\n",
        "        super(RNN_RNN, self).__init__()\n",
        "        self.model_name = 'RNN_RNN'\n",
        "        # self.args = args\n",
        "        \n",
        "        V = 3000002\n",
        "        D = 300\n",
        "        H = 200\n",
        "        S = 10\n",
        "        P_V = 100\n",
        "        P_D = 50\n",
        "        self.abs_pos_embed = nn.Embedding(P_V,P_D)\n",
        "        self.rel_pos_embed = nn.Embedding(S,P_D)\n",
        "        self.embed = nn.Embedding(V,D,padding_idx=0)\n",
        "        if embed is not None:\n",
        "            self.embed.weight.data.copy_(embed)\n",
        "\n",
        "        self.word_RNN = nn.GRU(\n",
        "                        input_size = D,\n",
        "                        hidden_size = H,\n",
        "                        batch_first = True,\n",
        "                        bidirectional = True\n",
        "                        )\n",
        "        self.sent_RNN = nn.GRU(\n",
        "                        input_size = 2*H,\n",
        "                        hidden_size = H,\n",
        "                        batch_first = True,\n",
        "                        bidirectional = True\n",
        "                        )\n",
        "        self.fc = nn.Linear(2*H,2*H)\n",
        "\n",
        "        # Parameters of Classification Layer\n",
        "        self.content = nn.Linear(2*H,1,bias=False)\n",
        "        self.salience = nn.Bilinear(2*H,2*H,1,bias=False)\n",
        "        self.novelty = nn.Bilinear(2*H,2*H,1,bias=False)\n",
        "        self.abs_pos = nn.Linear(P_D,1,bias=False)\n",
        "        self.rel_pos = nn.Linear(P_D,1,bias=False)\n",
        "        self.bias = nn.Parameter(torch.FloatTensor(1).uniform_(-0.1,0.1))\n",
        "\n",
        "    def max_pool1d(self,x,seq_lens):\n",
        "        # x:[N,L,O_in]\n",
        "        out = []\n",
        "        for index,t in enumerate(x):\n",
        "            #print(index)\n",
        "            t = t[:seq_lens[index],:]\n",
        "            t = torch.t(t).unsqueeze(0)\n",
        "            out.append(F.max_pool1d(t,t.size(2)))\n",
        "            # print(out[-1])\n",
        "        out = torch.cat(out).squeeze(2)\n",
        "        return out\n",
        "        \n",
        "    def avg_pool1d(self,x,seq_lens):\n",
        "        # x:[N,L,O_in]\n",
        "        out = []\n",
        "        for index,t in enumerate(x):\n",
        "            t = t[:seq_lens[index],:]\n",
        "            t = torch.t(t).unsqueeze(0)\n",
        "            out.append(F.avg_pool1d(t,t.size(2)))\n",
        "        \n",
        "        out = torch.cat(out).squeeze(2)\n",
        "        return out\n",
        "\n",
        "\n",
        "    def pad_doc(self,words_out,doc_lens):\n",
        "        pad_dim = words_out.size(1)\n",
        "        max_doc_len = max(doc_lens)\n",
        "        sent_input = []\n",
        "        start = 0\n",
        "        for doc_len in doc_lens:\n",
        "            stop = start + doc_len\n",
        "            valid = words_out[start:stop]                                       # (doc_len,2*H)\n",
        "            start = stop\n",
        "            if doc_len == max_doc_len:\n",
        "                sent_input.append(valid.unsqueeze(0))\n",
        "            else:\n",
        "                pad = Variable(torch.zeros(max_doc_len-doc_len,pad_dim))\n",
        "                #if self.args.device is not None:\n",
        "                pad = pad.cuda()\n",
        "                sent_input.append(torch.cat([valid,pad]).unsqueeze(0))          # (1,max_len,2*H)\n",
        "        sent_input = torch.cat(sent_input,dim=0)                                # (B,max_len,2*H)\n",
        "        return sent_input\n",
        "\n",
        "    def forward(self,x,doc_lens):\n",
        "        sent_lens = torch.sum(torch.sign(x),dim=1).data \n",
        "        x = self.embed(x)                                                      # (N,L,D)\n",
        "        # word level GRU\n",
        "        H = 200\n",
        "\n",
        "        x = self.word_RNN(x)[0]                                                 # (N,2*H,L)\n",
        "        # print(x.size() , sent_lens)\n",
        "        \n",
        "        #word_out = self.avg_pool1d(x,sent_lens)\n",
        "        word_out = self.max_pool1d(x,sent_lens)\n",
        "        # make sent features(pad with zeros)\n",
        "        x = self.pad_doc(word_out,doc_lens)\n",
        "        # print(x.size())\n",
        "        # sent level GRU\n",
        "        sent_out = self.sent_RNN(x)[0]                                           # (B,max_doc_len,2*H)\n",
        "        #docs = self.avg_pool1d(sent_out,doc_lens)                               # (B,2*H)\n",
        "        # print(sent_out.size(),doc_lens)\n",
        "\n",
        "        docs = self.max_pool1d(sent_out,doc_lens)                                # (B,2*H)\n",
        "        probs = []\n",
        "        for index,doc_len in enumerate(doc_lens):\n",
        "            valid_hidden = sent_out[index,:doc_len,:]                            # (doc_len,2*H)\n",
        "            doc = F.tanh(self.fc(docs[index])).unsqueeze(0)\n",
        "            s = Variable(torch.zeros(1,2*H))\n",
        "            # if self.args.device is not None:\n",
        "            s = s.cuda()\n",
        "            for position, h in enumerate(valid_hidden):\n",
        "                h = h.view(1, -1)                                                # (1,2*H)\n",
        "                # get position embeddings\n",
        "                abs_index = Variable(torch.LongTensor([[position]]))\n",
        "                #if self.args.device is not None:\n",
        "                \n",
        "                abs_index = abs_index.cuda()\n",
        "                abs_features = self.abs_pos_embed(abs_index).squeeze(0)\n",
        "                \n",
        "                rel_index = int(round((position + 1) * 9.0 / doc_len))\n",
        "                rel_index = Variable(torch.LongTensor([[rel_index]]))\n",
        "                # if self.args.device is not None:\n",
        "                rel_index = rel_index.cuda()\n",
        "                rel_features = self.rel_pos_embed(rel_index).squeeze(0)\n",
        "                \n",
        "                # classification layer\n",
        "                content = self.content(h) \n",
        "                salience = self.salience(h,doc)\n",
        "                novelty = -1 * self.novelty(h,F.tanh(s))\n",
        "                abs_p = self.abs_pos(abs_features)\n",
        "                rel_p = self.rel_pos(rel_features)\n",
        "                prob = F.sigmoid(content + salience + novelty + abs_p + rel_p + self.bias)\n",
        "                s = s + torch.mm(prob,h)\n",
        "                probs.append(prob)\n",
        "        return torch.cat(probs).squeeze()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG1sW-mEqyjZ",
        "outputId": "0382c36e-a5f7-4a0a-d30e-0a9084cc5483",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "net = RNN_RNN(weights)\n",
        "net.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN_RNN(\n",
              "  (abs_pos_embed): Embedding(100, 50)\n",
              "  (rel_pos_embed): Embedding(10, 50)\n",
              "  (embed): Embedding(3000002, 300, padding_idx=0)\n",
              "  (word_RNN): GRU(300, 200, batch_first=True, bidirectional=True)\n",
              "  (sent_RNN): GRU(400, 200, batch_first=True, bidirectional=True)\n",
              "  (fc): Linear(in_features=400, out_features=400, bias=True)\n",
              "  (content): Linear(in_features=400, out_features=1, bias=False)\n",
              "  (salience): Bilinear(in1_features=400, in2_features=400, out_features=1, bias=False)\n",
              "  (novelty): Bilinear(in1_features=400, in2_features=400, out_features=1, bias=False)\n",
              "  (abs_pos): Linear(in_features=50, out_features=1, bias=False)\n",
              "  (rel_pos): Linear(in_features=50, out_features=1, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7sY4o7pdWon"
      },
      "source": [
        "import copy\n",
        "class Batch:\n",
        "  def __init__(self,batch,word2id):\n",
        "    self.batch=copy.deepcopy(batch.to_numpy())\n",
        "    self.word2id=word2id\n",
        "\n",
        "  def preprocess(self):\n",
        "    idx=0\n",
        "    doc_lens = []\n",
        "    features = []\n",
        "    targets = []\n",
        "\n",
        "    sent_list = []\n",
        "    for doc in self.batch[:,0]:\n",
        "        sent_list += doc\n",
        "        doc_lens.append(len(doc))\n",
        "        targets += list(self.batch[idx,1])\n",
        "        idx+=1\n",
        "\n",
        "    maxlen=0\n",
        "    for sentence in sent_list:\n",
        "      s=sentence.split()\n",
        "      maxlen=max(maxlen,len(s))\n",
        "  \n",
        "    print(maxlen)\n",
        "\n",
        "    for sentence in sent_list:\n",
        "      words = sentence.strip().split()\n",
        "      sentence = [self.word2id[word] if word in self.word2id else 1 for word in words]\n",
        "      sentence += [0 for _ in range(maxlen - len(sentence))]\n",
        "      features.append(sentence)\n",
        "\n",
        "      \n",
        "\n",
        "    # print(doc_lens)\n",
        "    features = torch.LongTensor(features)    \n",
        "    targets = torch.LongTensor(targets)\n",
        "    \n",
        "    return features,targets,doc_lens\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZorpX9DWwPG",
        "outputId": "9f8930dd-e9be-4395-a1d6-6c4aee75a52a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
        "loss_sum = 0\n",
        "min_eval_loss = float('Inf')\n",
        "train_loader = DataLoader(Dataset(train_df),batch_size=16)\n",
        "validation_loader=DataLoader(Dataset(validation_df),shuffle=False)\n",
        "# for step, docs in enumerate(train_loader):\n",
        "#         for doc in Batch(docs,word2id).preprocess():\n",
        " \n",
        "counter=0\n",
        "for epoch in range(5):\n",
        "    for step, docs in enumerate(train_loader):\n",
        "        features,targets,doc_lens = Batch(docs,word2id).preprocess()\n",
        "\n",
        "        features,targets = Variable(features), Variable(targets.float())\n",
        "        features = features.cuda()\n",
        "        targets = targets.cuda()\n",
        "\n",
        "        probs = net(features,doc_lens)\n",
        "        loss = criterion(probs,targets)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_grad_norm(net.parameters(), args.max_norm)\n",
        "        optimizer.step()\n",
        "        # if args.debug:\n",
        "        #       print('Batch ID:%d Loss:%f' %(i,loss.data[0]))\n",
        "        #       continue\n",
        "        # if step % args.report_every == 0:\n",
        "        #       cur_loss = eval(net,vocab,val_iter,criterion)\n",
        "        #       if cur_loss < min_loss:\n",
        "        #           min_loss = cur_loss\n",
        "        #           best_path = net.save()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "61\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([0.3566, 0.6892, 0.6399, 0.4993, 0.6113, 0.6647, 0.6456, 0.6278, 0.7208,\n",
            "        0.7855, 0.6755, 0.4561, 0.6992, 0.7255, 0.7829, 0.8333, 0.8704, 0.9035,\n",
            "        0.7487, 0.8503, 0.6277, 0.8273, 0.7010, 0.5976, 0.7475, 0.8792, 0.6497,\n",
            "        0.4936, 0.3389, 0.6875, 0.6295, 0.4883, 0.3644, 0.4166, 0.3620, 0.4688,\n",
            "        0.6242, 0.7073, 0.6563, 0.4761, 0.6289, 0.6496, 0.7326, 0.7948, 0.8087,\n",
            "        0.8293, 0.6600, 0.7880, 0.6366, 0.8633, 0.8037, 0.6026, 0.6493, 0.8591,\n",
            "        0.8042, 0.8025, 0.6640, 0.8718, 0.7460, 0.7918, 0.7613, 0.7880, 0.8387,\n",
            "        0.6427, 0.6069, 0.6630, 0.7292, 0.7727, 0.5579, 0.4574, 0.6639, 0.7164,\n",
            "        0.9053, 0.8810, 0.7452, 0.8815, 0.7905, 0.7755, 0.8069, 0.8759, 0.7736,\n",
            "        0.9539, 0.8463, 0.8770, 0.9571, 0.9551, 0.9036, 0.7069, 0.8741, 0.8736,\n",
            "        0.8422, 0.6299, 0.7847, 0.7314, 0.7561, 0.7640, 0.8261, 0.7142, 0.7635,\n",
            "        0.7396, 0.7622, 0.7666, 0.9109, 0.9436, 0.7836, 0.6727, 0.6914, 0.6505,\n",
            "        0.6795, 0.4121, 0.6509, 0.7142, 0.6004, 0.6869, 0.7625, 0.7143, 0.3458,\n",
            "        0.7020, 0.6275, 0.4877, 0.3751, 0.4442, 0.3992, 0.7077, 0.8194, 0.8919,\n",
            "        0.8381, 0.7008, 0.8148, 0.7694, 0.6610, 0.7246, 0.7978, 0.8267, 0.5983,\n",
            "        0.7376, 0.5910, 0.8425, 0.8064, 0.7556, 0.8264, 0.9377, 0.9003, 0.8926,\n",
            "        0.8728, 0.9470, 0.9034, 0.9078, 0.8680, 0.6935, 0.8036, 0.7090, 0.7686,\n",
            "        0.7926, 0.7886, 0.8274, 0.6547, 0.6843, 0.7468, 0.7731, 0.7166, 0.7740,\n",
            "        0.5762, 0.3469, 0.6725, 0.7908, 0.6795, 0.5825, 0.5469, 0.4289, 0.5744,\n",
            "        0.7255, 0.9005, 0.8936, 0.8200, 0.8923, 0.8032, 0.7411, 0.7852, 0.8275,\n",
            "        0.6938, 0.3502, 0.7038, 0.6430, 0.4791, 0.3691, 0.4497, 0.4105, 0.4926,\n",
            "        0.6762, 0.8822, 0.8038, 0.7209, 0.8609, 0.8785, 0.7958, 0.7803, 0.7785,\n",
            "        0.7560, 0.5247, 0.6069, 0.5486, 0.8174, 0.6905, 0.3764, 0.4909, 0.7877,\n",
            "        0.6396, 0.5941, 0.4338, 0.7446, 0.7433, 0.7977, 0.7856, 0.8217, 0.8719,\n",
            "        0.6891, 0.7888, 0.7969, 0.8653, 0.8853, 0.7010, 0.6619, 0.7739, 0.5088,\n",
            "        0.6907, 0.7171, 0.5677, 0.7934, 0.7167, 0.8320, 0.8120, 0.8300, 0.6225,\n",
            "        0.8543, 0.5352, 0.5486, 0.7128, 0.6851, 0.4447, 0.1542, 0.3465, 0.6984,\n",
            "        0.6529, 0.4820, 0.3730, 0.4619, 0.4071, 0.5203, 0.6622, 0.7600, 0.6995,\n",
            "        0.7352, 0.8457, 0.8836, 0.7899, 0.7881, 0.8407, 0.8608, 0.5690, 0.7361,\n",
            "        0.5995, 0.8168, 0.7156, 0.5090, 0.6560, 0.8141, 0.7635, 0.6832, 0.5791,\n",
            "        0.8436, 0.7397, 0.7376, 0.7260, 0.7623, 0.9319, 0.8198, 0.8650, 0.8796,\n",
            "        0.8686, 0.8449, 0.6980, 0.7975, 0.8556, 0.8505, 0.9140, 0.9023, 0.8342,\n",
            "        0.9148, 0.8633, 0.7041, 0.7322, 0.7768, 0.6525, 0.8701, 0.7168, 0.7456,\n",
            "        0.9496, 0.9133, 0.8236, 0.5723, 0.8068, 0.7882, 0.8520, 0.7898, 0.7474,\n",
            "        0.7149, 0.6562, 0.6719, 0.3387, 0.6739, 0.6155, 0.4657, 0.5807, 0.6441,\n",
            "        0.5952, 0.5863, 0.6680, 0.7574, 0.6462, 0.4893, 0.6524, 0.8484, 0.7465,\n",
            "        0.8454, 0.8981, 0.8920, 0.7090, 0.6425, 0.5323, 0.8148, 0.7965, 0.5363,\n",
            "        0.6956, 0.7713, 0.5998, 0.3440, 0.6901, 0.6027, 0.4407, 0.3420, 0.4197,\n",
            "        0.5720, 0.6767, 0.7677, 0.8344, 0.7847, 0.5336, 0.6961, 0.7594, 0.6200,\n",
            "        0.6684, 0.7273, 0.7291, 0.4804, 0.6012, 0.6485, 0.8342, 0.7620, 0.5274,\n",
            "        0.7815, 0.9291, 0.8938, 0.8202, 0.4542, 0.7037, 0.5720, 0.6068, 0.5935,\n",
            "        0.7658, 0.8134, 0.5962, 0.6065, 0.5142, 0.5429, 0.4996, 0.3465, 0.6961,\n",
            "        0.6202, 0.4543, 0.3547, 0.4139, 0.3702, 0.4900, 0.6072, 0.8495, 0.8076,\n",
            "        0.6675, 0.8108, 0.8325, 0.7559, 0.8304, 0.7778, 0.7815, 0.5287, 0.6336,\n",
            "        0.4475, 0.7128, 0.5890, 0.3301, 0.4846, 0.7661, 0.7214, 0.6408, 0.5345,\n",
            "        0.8367, 0.8043, 0.8097, 0.8349, 0.8296, 0.8833, 0.6726, 0.7412, 0.7902,\n",
            "        0.8150, 0.8236, 0.6658, 0.6819, 0.7963, 0.5219, 0.6224, 0.6453, 0.5696,\n",
            "        0.7273, 0.5779, 0.7180, 0.7198, 0.8261, 0.6193, 0.8881, 0.6814, 0.7058,\n",
            "        0.8409, 0.7757, 0.5790, 0.2567, 0.3505, 0.6728, 0.6156, 0.6641, 0.5863,\n",
            "        0.6473, 0.4625, 0.5799, 0.7101, 0.6731, 0.6379, 0.4694, 0.8406, 0.8468,\n",
            "        0.7957, 0.8370, 0.8610, 0.7286, 0.4576, 0.6787, 0.5940, 0.8131, 0.4929,\n",
            "        0.2471, 0.3484, 0.6977, 0.6501, 0.4898, 0.3980, 0.4772, 0.3950, 0.5409,\n",
            "        0.6765, 0.7504, 0.8244, 0.7067, 0.8467, 0.8881, 0.7846, 0.8310, 0.8734,\n",
            "        0.7867, 0.5722, 0.6534, 0.5233, 0.7869, 0.6831, 0.3613, 0.5486, 0.7794,\n",
            "        0.6627, 0.6573, 0.5301, 0.7865, 0.6036, 0.8162, 0.7785, 0.8282, 0.8534,\n",
            "        0.6655, 0.7753, 0.8160, 0.8137, 0.8595, 0.6894, 0.6565, 0.7821, 0.8212,\n",
            "        0.8885, 0.7155, 0.6255, 0.7324, 0.6054, 0.6912, 0.6922, 0.7040, 0.6440,\n",
            "        0.8963, 0.7309, 0.7148, 0.9128, 0.8757, 0.7686, 0.3601, 0.6081, 0.5362,\n",
            "        0.4865, 0.3437, 0.6850, 0.6008, 0.4573, 0.3659, 0.4302, 0.6042, 0.7089,\n",
            "        0.8119, 0.8856, 0.7494, 0.6013, 0.7803, 0.8113, 0.6989, 0.7172, 0.7629,\n",
            "        0.7545, 0.5142, 0.8283, 0.7284, 0.9177, 0.8572, 0.7024, 0.8037, 0.9230,\n",
            "        0.8937, 0.8415, 0.5163, 0.8048, 0.6797, 0.7546, 0.7742, 0.7960, 0.8862,\n",
            "        0.7004, 0.5140, 0.5367, 0.5522, 0.3470, 0.6966, 0.6263, 0.4577, 0.3660,\n",
            "        0.4285, 0.3899, 0.4962, 0.6314, 0.7177, 0.7860, 0.5862, 0.7997, 0.8471,\n",
            "        0.7447, 0.8183, 0.8529, 0.8248, 0.4960, 0.5647, 0.4341, 0.7778, 0.6344,\n",
            "        0.3829, 0.5339, 0.7836, 0.6922, 0.6567, 0.5015, 0.7975, 0.6817, 0.7047,\n",
            "        0.7942, 0.8555, 0.8826, 0.7135, 0.7708, 0.7986, 0.8130, 0.8355, 0.6508,\n",
            "        0.6592, 0.7472, 0.7619, 0.8534, 0.8764, 0.6483, 0.7995, 0.5881, 0.6690,\n",
            "        0.6522, 0.6910, 0.5048, 0.8456, 0.6922, 0.7334, 0.9166, 0.8904, 0.8061,\n",
            "        0.5529, 0.7868, 0.6031, 0.6124, 0.4272, 0.5124, 0.3448, 0.6948, 0.6312,\n",
            "        0.4632, 0.3581, 0.4250, 0.3813, 0.5097, 0.6643, 0.8737, 0.8227, 0.6944,\n",
            "        0.8393, 0.8853, 0.8032, 0.8605, 0.8155, 0.8026, 0.6160, 0.6788, 0.5561,\n",
            "        0.8125, 0.7011, 0.4278, 0.5591, 0.8082, 0.8018, 0.7339, 0.5787, 0.9333,\n",
            "        0.8855, 0.9040, 0.8748, 0.9080, 0.9278, 0.8567, 0.8811, 0.8841, 0.9053,\n",
            "        0.9243, 0.8497, 0.6429, 0.7071, 0.7087, 0.8000, 0.7871, 0.6191, 0.7532,\n",
            "        0.7225, 0.6995, 0.7140, 0.8263, 0.7296, 0.9050, 0.6488, 0.6358, 0.8747,\n",
            "        0.7472, 0.3522, 0.6897, 0.6164, 0.6606, 0.5631, 0.5166, 0.4735, 0.5899,\n",
            "        0.7447, 0.8222, 0.8655, 0.7632, 0.8888, 0.9092, 0.6746, 0.7057, 0.8075,\n",
            "        0.8047, 0.4334, 0.5204, 0.3344, 0.6838, 0.7889, 0.5753, 0.4671, 0.4690,\n",
            "        0.4146, 0.7683, 0.8745, 0.9079, 0.7164, 0.6308, 0.7883, 0.6793],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
            "tensor([0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
            "       device='cuda:0')\n",
            "85\n",
            "tensor([0.3369, 0.6830, 0.6291, 0.4513, 0.3476, 0.4039, 0.3821, 0.5042, 0.6471,\n",
            "        0.8460, 0.7857, 0.6391, 0.7771, 0.8503, 0.7585, 0.8167, 0.7926, 0.7909,\n",
            "        0.5662, 0.6762, 0.5087, 0.7852, 0.7043, 0.4252, 0.6098, 0.8014, 0.7404,\n",
            "        0.6510, 0.5174, 0.8183, 0.8086, 0.8517, 0.8443, 0.8649, 0.9188, 0.7623,\n",
            "        0.8662, 0.8878, 0.9094, 0.8973, 0.7191, 0.7477, 0.7913, 0.6544, 0.7768,\n",
            "        0.8280, 0.6985, 0.8173, 0.6946, 0.7694, 0.7605, 0.8339, 0.6408, 0.8983,\n",
            "        0.6910, 0.6879, 0.8195, 0.8175, 0.6365, 0.2745, 0.3413, 0.6931, 0.6167,\n",
            "        0.4537, 0.3623, 0.4346, 0.6070, 0.7070, 0.7914, 0.8436, 0.7917, 0.5477,\n",
            "        0.7264, 0.7840, 0.6877, 0.7457, 0.7535, 0.7438, 0.4748, 0.6261, 0.4904,\n",
            "        0.8883, 0.8043, 0.5471, 0.6935, 0.9295, 0.8982, 0.8295, 0.7631, 0.9076,\n",
            "        0.5993, 0.6223, 0.6360, 0.7524, 0.8844, 0.6959, 0.6731, 0.6883, 0.6811,\n",
            "        0.5668, 0.3889, 0.3258, 0.3445, 0.6818, 0.6127, 0.4620, 0.3626, 0.6232,\n",
            "        0.5765, 0.6735, 0.6970, 0.7618, 0.7088, 0.5599, 0.7162, 0.7650, 0.6012,\n",
            "        0.8705, 0.9004, 0.9094, 0.8259, 0.8678, 0.8071, 0.9307, 0.7196, 0.4896,\n",
            "        0.6889, 0.9016, 0.8340, 0.7906, 0.6521, 0.7485, 0.4907, 0.3402, 0.6885,\n",
            "        0.6356, 0.4583, 0.3694, 0.4456, 0.3894, 0.5246, 0.6222, 0.6972, 0.8028,\n",
            "        0.7105, 0.8530, 0.8675, 0.7801, 0.8186, 0.8741, 0.8822, 0.5309, 0.6827,\n",
            "        0.5064, 0.8005, 0.7179, 0.4840, 0.6313, 0.7782, 0.6203, 0.6045, 0.5134,\n",
            "        0.8013, 0.6183, 0.6931, 0.6693, 0.9159, 0.9455, 0.8323, 0.8209, 0.8541,\n",
            "        0.8813, 0.8507, 0.7140, 0.6921, 0.7885, 0.8322, 0.8678, 0.8774, 0.7809,\n",
            "        0.7854, 0.6944, 0.7262, 0.7324, 0.7974, 0.6529, 0.8922, 0.8062, 0.8000,\n",
            "        0.9325, 0.8614, 0.8087, 0.5844, 0.8040, 0.7777, 0.5185, 0.4601, 0.6620,\n",
            "        0.5435, 0.3424, 0.6689, 0.6050, 0.4633, 0.3743, 0.6540, 0.6154, 0.7209,\n",
            "        0.8205, 0.8060, 0.7528, 0.6005, 0.7744, 0.7536, 0.5835, 0.6258, 0.7053,\n",
            "        0.8669, 0.6630, 0.7506, 0.7136, 0.8707, 0.7780, 0.5185, 0.5332, 0.8000,\n",
            "        0.7126, 0.6594, 0.5756, 0.8700, 0.7295, 0.7242, 0.5850, 0.5541, 0.3535,\n",
            "        0.6867, 0.6273, 0.4688, 0.5611, 0.6429, 0.6062, 0.7244, 0.7095, 0.7664,\n",
            "        0.6987, 0.5241, 0.7102, 0.7655, 0.8087, 0.8273, 0.8704, 0.9012, 0.7417,\n",
            "        0.8649, 0.5799, 0.8190, 0.6721, 0.4366, 0.7032, 0.8634, 0.7780, 0.5722,\n",
            "        0.4251, 0.3280, 0.6766, 0.7893, 0.5666, 0.4874, 0.4812, 0.4277, 0.7519,\n",
            "        0.8711, 0.9090, 0.7076, 0.6143, 0.7642, 0.6530, 0.3343, 0.6880, 0.7970,\n",
            "        0.5757, 0.4633, 0.4472, 0.4472, 0.7500, 0.8790, 0.9137, 0.7266, 0.5968,\n",
            "        0.7385, 0.7053, 0.3171, 0.8225, 0.6864, 0.4866, 0.6252, 0.7349, 0.4494,\n",
            "        0.6316, 0.6334, 0.3509, 0.6722, 0.6031, 0.4456, 0.3669, 0.6165, 0.5862,\n",
            "        0.6924, 0.6863, 0.7263, 0.6352, 0.4644, 0.5907, 0.6320, 0.4461, 0.5574,\n",
            "        0.8205, 0.8676, 0.7024, 0.8315, 0.7155, 0.9089, 0.8595, 0.4147, 0.5397,\n",
            "        0.7766, 0.7441, 0.7456, 0.6180, 0.8574, 0.5630, 0.4981, 0.3290, 0.8285,\n",
            "        0.6960, 0.5165, 0.6573, 0.7629, 0.5027, 0.7015, 0.6620, 0.3488, 0.6974,\n",
            "        0.6562, 0.5148, 0.4148, 0.4739, 0.4353, 0.5273, 0.6708, 0.7203, 0.6375,\n",
            "        0.4574, 0.8023, 0.8580, 0.7447, 0.8191, 0.8586, 0.8753, 0.6980, 0.7937,\n",
            "        0.5873, 0.8219, 0.7115, 0.4915, 0.5845, 0.8398, 0.7536, 0.6814, 0.5324,\n",
            "        0.7858, 0.7192, 0.7162, 0.7386, 0.7503, 0.8304, 0.6592, 0.6813, 0.8917,\n",
            "        0.9034, 0.8778, 0.6407, 0.6483, 0.7799, 0.7756, 0.8827, 0.9147, 0.8731,\n",
            "        0.9136, 0.8813, 0.8921, 0.8405, 0.8651, 0.7821, 0.8391, 0.6022, 0.7264,\n",
            "        0.8902, 0.8620, 0.7471, 0.5688, 0.7974, 0.8122, 0.7732, 0.7662, 0.8696,\n",
            "        0.8180, 0.8397, 0.8159, 0.8492, 0.6815, 0.6938, 0.6569, 0.6505, 0.4796,\n",
            "        0.3398, 0.6900, 0.6224, 0.4835, 0.3933, 0.4496, 0.6365, 0.7377, 0.8380,\n",
            "        0.8782, 0.7130, 0.5508, 0.7558, 0.8153, 0.6953, 0.6939, 0.7367, 0.7659,\n",
            "        0.6894, 0.8208, 0.7040, 0.9065, 0.8731, 0.7305, 0.8260, 0.9426, 0.7742,\n",
            "        0.6653, 0.5172, 0.8161, 0.6899, 0.7497, 0.7586, 0.7412, 0.7394, 0.4525,\n",
            "        0.4539, 0.3475, 0.6764, 0.6198, 0.4685, 0.5723, 0.6388, 0.4693, 0.6025,\n",
            "        0.7537, 0.7331, 0.6924, 0.5756, 0.8752, 0.8987, 0.8393, 0.8875, 0.8978,\n",
            "        0.9119, 0.5966, 0.6379, 0.6027, 0.8501, 0.7337, 0.3136, 0.4047, 0.3500,\n",
            "        0.6842, 0.6150, 0.4573, 0.5712, 0.6451, 0.5860, 0.5609, 0.7049, 0.7836,\n",
            "        0.6723, 0.5145, 0.7118, 0.8831, 0.7885, 0.8293, 0.8873, 0.8833, 0.5235,\n",
            "        0.6590, 0.5885, 0.8577, 0.7094, 0.4187, 0.4576, 0.7146, 0.3419, 0.6902,\n",
            "        0.6214, 0.4669, 0.3694, 0.4379, 0.5767, 0.7006, 0.7986, 0.8639, 0.7205,\n",
            "        0.5638, 0.7216, 0.7636, 0.6115, 0.6870, 0.7258, 0.7265, 0.4650, 0.7574,\n",
            "        0.6959, 0.9005, 0.8314, 0.7099, 0.8151, 0.9405, 0.9065, 0.7461, 0.6252,\n",
            "        0.8719, 0.7045, 0.8031, 0.7813, 0.8408, 0.9019, 0.5916, 0.6058, 0.5607],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "       device='cuda:0')\n",
            "66\n",
            "tensor([0.3459, 0.6721, 0.6160, 0.4785, 0.3683, 0.6507, 0.6279, 0.7431, 0.8296,\n",
            "        0.7984, 0.7357, 0.5839, 0.7397, 0.7707, 0.6370, 0.6536, 0.7251, 0.8916,\n",
            "        0.7263, 0.8111, 0.7997, 0.9339, 0.8983, 0.7637, 0.6780, 0.8572, 0.8158,\n",
            "        0.7970, 0.7351, 0.8972, 0.7829, 0.8457, 0.6635, 0.6370, 0.3438, 0.6947,\n",
            "        0.6181, 0.4682, 0.3744, 0.4448, 0.3879, 0.7049, 0.8076, 0.8692, 0.8253,\n",
            "        0.7388, 0.7848, 0.8518, 0.7717, 0.8243, 0.8484, 0.8122, 0.5227, 0.6463,\n",
            "        0.4540, 0.7572, 0.8398, 0.6394, 0.7580, 0.9273, 0.8769, 0.8734, 0.7623,\n",
            "        0.9252, 0.8503, 0.8301, 0.6596, 0.7788, 0.8142, 0.5562, 0.5592, 0.6680,\n",
            "        0.7796, 0.7853, 0.5796, 0.5920, 0.5499, 0.5381, 0.6062, 0.3369, 0.6886,\n",
            "        0.6005, 0.4452, 0.3433, 0.4227, 0.3833, 0.4585, 0.7671, 0.8087, 0.7551,\n",
            "        0.6104, 0.8374, 0.8477, 0.6486, 0.7102, 0.7507, 0.7633, 0.5293, 0.6224,\n",
            "        0.4594, 0.6814, 0.6215, 0.3788, 0.5286, 0.7372, 0.7898, 0.7960, 0.6605,\n",
            "        0.8535, 0.7590, 0.8486, 0.8524, 0.8701, 0.9038, 0.7653, 0.7659, 0.6407,\n",
            "        0.6814, 0.7110, 0.4957, 0.3960, 0.5217, 0.6255, 0.7015, 0.7225, 0.5709,\n",
            "        0.6999, 0.6641, 0.6010, 0.5848, 0.6508, 0.3427, 0.6855, 0.5877, 0.4350,\n",
            "        0.3531, 0.4384, 0.4102, 0.7046, 0.7873, 0.8182, 0.7523, 0.6743, 0.8440,\n",
            "        0.8084, 0.6561, 0.7250, 0.7801, 0.7392, 0.4457, 0.4740, 0.3780, 0.7027,\n",
            "        0.6226, 0.6106, 0.7714, 0.8967, 0.8262, 0.8101, 0.7165, 0.9013, 0.7490,\n",
            "        0.8078, 0.8376, 0.6853, 0.7399, 0.4174, 0.5144, 0.5526, 0.6852, 0.7179,\n",
            "        0.5310, 0.4819, 0.6045, 0.5387, 0.5258, 0.5927, 0.4520, 0.3448, 0.6860,\n",
            "        0.6093, 0.4603, 0.3688, 0.4443, 0.6059, 0.7211, 0.8268, 0.8792, 0.8472,\n",
            "        0.6017, 0.7479, 0.7771, 0.6022, 0.6438, 0.6872, 0.7040, 0.4073, 0.5654,\n",
            "        0.6613, 0.8906, 0.8105, 0.6168, 0.7427, 0.8931, 0.8768, 0.8239, 0.4404,\n",
            "        0.7972, 0.6353, 0.6661, 0.6679, 0.7838, 0.8527, 0.6038, 0.5686, 0.4933,\n",
            "        0.5626, 0.5137, 0.3501, 0.7002, 0.6272, 0.4783, 0.3789, 0.4516, 0.3934,\n",
            "        0.5029, 0.8069, 0.8624, 0.8183, 0.6651, 0.7359, 0.7875, 0.5212, 0.6418,\n",
            "        0.6822, 0.6490, 0.4384, 0.5208, 0.3363, 0.6826, 0.5854, 0.3276, 0.5113,\n",
            "        0.7399, 0.8164, 0.8269, 0.7320, 0.9293, 0.7970, 0.8844, 0.8784, 0.8773,\n",
            "        0.9208, 0.7910, 0.8149, 0.6395, 0.6006, 0.5711, 0.4120, 0.3532, 0.4821,\n",
            "        0.5227, 0.6442, 0.6766, 0.5915, 0.7627, 0.5907, 0.4322, 0.4434, 0.6104,\n",
            "        0.3439, 0.6630, 0.6115, 0.4744, 0.5682, 0.6354, 0.6130, 0.7265, 0.7257,\n",
            "        0.7958, 0.7156, 0.5388, 0.7225, 0.7712, 0.7964, 0.8114, 0.8621, 0.8882,\n",
            "        0.7310, 0.8081, 0.4858, 0.7480, 0.6819, 0.4163, 0.5836, 0.8291, 0.7841,\n",
            "        0.4802, 0.3506, 0.3464, 0.6939, 0.6115, 0.4619, 0.3787, 0.4401, 0.3875,\n",
            "        0.6988, 0.8275, 0.8596, 0.8318, 0.7005, 0.7606, 0.7766, 0.6495, 0.7537,\n",
            "        0.7735, 0.7830, 0.5015, 0.6727, 0.4846, 0.7747, 0.7365, 0.6635, 0.7517,\n",
            "        0.9048, 0.8737, 0.8232, 0.7598, 0.9339, 0.8455, 0.8965, 0.8743, 0.7228,\n",
            "        0.8388, 0.6202, 0.6483, 0.6773, 0.7774, 0.7812, 0.5496, 0.5299, 0.6051,\n",
            "        0.5143, 0.6344, 0.6062, 0.3344, 0.6806, 0.6178, 0.4744, 0.3686, 0.4329,\n",
            "        0.3764, 0.4970, 0.6336, 0.7220, 0.7008, 0.6798, 0.8282, 0.8367, 0.7369,\n",
            "        0.7995, 0.8241, 0.7844, 0.6145, 0.7600, 0.5245, 0.7793, 0.6242, 0.4659,\n",
            "        0.6647, 0.8609, 0.7627, 0.6555, 0.4973, 0.7696, 0.6424, 0.6701, 0.5951,\n",
            "        0.6534, 0.7931, 0.5083, 0.7737, 0.8030, 0.8570, 0.8301, 0.7107, 0.5882,\n",
            "        0.6555, 0.7316, 0.8314, 0.8799, 0.8497, 0.8976, 0.7846, 0.7987, 0.8769,\n",
            "        0.9267, 0.6605, 0.8358, 0.5988, 0.5875, 0.8239, 0.7463, 0.5584, 0.4044,\n",
            "        0.7275, 0.6842, 0.6505, 0.5530, 0.7993, 0.7541, 0.7488, 0.7156, 0.5971,\n",
            "        0.5006, 0.5025, 0.3534, 0.3362, 0.6722, 0.6179, 0.4657, 0.5828, 0.6357,\n",
            "        0.6099, 0.6059, 0.7283, 0.7983, 0.7079, 0.5331, 0.6936, 0.7526, 0.7713,\n",
            "        0.8352, 0.8624, 0.9027, 0.7536, 0.8172, 0.4736, 0.7513, 0.6617, 0.5056,\n",
            "        0.6559, 0.8557, 0.6138, 0.5121, 0.3291, 0.6809, 0.7897, 0.6755, 0.4417,\n",
            "        0.4441, 0.3609, 0.7195, 0.7947, 0.8526, 0.6684, 0.5555, 0.7916, 0.8480,\n",
            "        0.5461, 0.3456, 0.6903, 0.6335, 0.4889, 0.3881, 0.4379, 0.4067, 0.5077,\n",
            "        0.6705, 0.8677, 0.8117, 0.7012, 0.8202, 0.8674, 0.7909, 0.8092, 0.7830,\n",
            "        0.7647, 0.4959, 0.5888, 0.5060, 0.8110, 0.5779, 0.3356, 0.5243, 0.7523,\n",
            "        0.6792, 0.6368, 0.4765, 0.8832, 0.7871, 0.7896, 0.8595, 0.9022, 0.9086,\n",
            "        0.8419, 0.8728, 0.8778, 0.8922, 0.8950, 0.6819, 0.4008, 0.5269, 0.5870,\n",
            "        0.6953, 0.7148, 0.6502, 0.7710, 0.7193, 0.7535, 0.8680, 0.9149, 0.7379,\n",
            "        0.9261, 0.6183, 0.7371, 0.8611, 0.7064, 0.3529, 0.7000, 0.6195, 0.4777,\n",
            "        0.3884, 0.6680, 0.6192, 0.6876, 0.8073, 0.8528, 0.7484, 0.5825, 0.7699,\n",
            "        0.7909, 0.6718, 0.7520, 0.7818, 0.7622, 0.7301, 0.8262, 0.7274, 0.9234,\n",
            "        0.8412, 0.6278, 0.8099, 0.9320, 0.7499, 0.7407, 0.5392, 0.8750, 0.7676,\n",
            "        0.8159, 0.7393, 0.7997, 0.7531, 0.3520, 0.3558, 0.7114, 0.6341, 0.4942,\n",
            "        0.4062, 0.6893, 0.6522, 0.7576, 0.8559, 0.9067, 0.8114, 0.6860, 0.8000,\n",
            "        0.7936, 0.6314, 0.7310, 0.8179, 0.8066, 0.7548, 0.7988, 0.7053, 0.9107,\n",
            "        0.7919, 0.6615, 0.7805, 0.9203, 0.7634, 0.7233, 0.5455, 0.8746, 0.7891,\n",
            "        0.8121, 0.7108, 0.7819, 0.7396, 0.3645, 0.3487, 0.6811, 0.6342, 0.4806,\n",
            "        0.3709, 0.6455, 0.6070, 0.7073, 0.7298, 0.7989, 0.7543, 0.6140, 0.7494,\n",
            "        0.7989, 0.6866, 0.7326, 0.8952, 0.8881, 0.7658, 0.8605, 0.7892, 0.9259,\n",
            "        0.8824, 0.5409, 0.6838, 0.8705, 0.8433, 0.8079, 0.6670, 0.8729, 0.6001,\n",
            "        0.5191, 0.3446, 0.6772, 0.6273, 0.4660, 0.3637, 0.6180, 0.5606, 0.6806,\n",
            "        0.7776, 0.7479, 0.6741, 0.5008, 0.6524, 0.7005, 0.5540, 0.6237, 0.8374,\n",
            "        0.8453, 0.6190, 0.7211, 0.6781, 0.8893, 0.7995, 0.3491, 0.4662, 0.7710,\n",
            "        0.6349, 0.6612, 0.5230, 0.8076, 0.6176, 0.5147, 0.4457],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
            "tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 1., 1., 0., 0.], device='cuda:0')\n",
            "60\n",
            "tensor([0.3474, 0.6919, 0.6187, 0.4723, 0.3882, 0.6575, 0.6209, 0.7546, 0.8044,\n",
            "        0.8623, 0.7408, 0.5902, 0.7644, 0.7677, 0.6386, 0.6712, 0.7706, 0.7802,\n",
            "        0.7533, 0.8183, 0.7774, 0.9422, 0.8981, 0.7736, 0.8149, 0.9229, 0.7739,\n",
            "        0.7719, 0.6318, 0.8877, 0.7957, 0.8275, 0.8201, 0.8172, 0.7841, 0.5378,\n",
            "        0.3266, 0.8283, 0.7912, 0.5523, 0.3962, 0.4546, 0.6344, 0.7942, 0.7259,\n",
            "        0.8199, 0.7467, 0.4180, 0.3476, 0.6692, 0.6145, 0.6507, 0.5719, 0.5328,\n",
            "        0.4929, 0.6126, 0.6795, 0.7150, 0.8182, 0.7026, 0.8679, 0.8912, 0.8120,\n",
            "        0.6922, 0.7520, 0.7705, 0.5863, 0.5212, 0.3761, 0.3447, 0.6701, 0.6064,\n",
            "        0.6422, 0.5362, 0.6062, 0.4360, 0.5829, 0.6694, 0.7348, 0.6715, 0.7484,\n",
            "        0.8622, 0.8968, 0.7800, 0.6343, 0.7004, 0.7499, 0.5326, 0.6261, 0.3268,\n",
            "        0.6472, 0.3500, 0.6775, 0.6183, 0.4840, 0.6032, 0.6716, 0.6180, 0.6227,\n",
            "        0.7164, 0.7741, 0.6528, 0.4857, 0.7089, 0.8971, 0.7962, 0.8745, 0.9097,\n",
            "        0.9140, 0.5312, 0.6886, 0.5665, 0.8540, 0.7921, 0.5189, 0.5480, 0.7753,\n",
            "        0.3307, 0.6805, 0.6049, 0.4439, 0.3214, 0.3967, 0.3605, 0.4805, 0.6625,\n",
            "        0.7210, 0.6034, 0.5991, 0.7825, 0.7967, 0.6567, 0.7191, 0.7802, 0.7909,\n",
            "        0.5441, 0.7252, 0.4995, 0.7704, 0.6271, 0.3172, 0.5316, 0.7474, 0.6340,\n",
            "        0.4534, 0.3518, 0.7142, 0.5415, 0.5508, 0.5329, 0.5707, 0.7230, 0.4529,\n",
            "        0.7234, 0.7241, 0.7164, 0.6915, 0.5264, 0.4427, 0.6047, 0.6931, 0.8073,\n",
            "        0.7910, 0.7031, 0.8578, 0.6819, 0.7927, 0.8183, 0.8666, 0.5307, 0.8401,\n",
            "        0.5583, 0.6012, 0.8368, 0.7687, 0.6752, 0.5032, 0.7720, 0.7067, 0.7138,\n",
            "        0.7155, 0.8263, 0.7720, 0.7837, 0.7340, 0.6521, 0.5123, 0.5091, 0.4165,\n",
            "        0.3487, 0.6943, 0.6192, 0.4974, 0.3734, 0.4451, 0.5898, 0.6882, 0.8139,\n",
            "        0.8602, 0.7301, 0.5412, 0.7138, 0.7230, 0.6426, 0.6490, 0.7223, 0.7027,\n",
            "        0.5040, 0.8184, 0.7051, 0.8955, 0.8112, 0.7042, 0.8103, 0.9349, 0.8934,\n",
            "        0.8624, 0.5747, 0.8632, 0.6667, 0.7326, 0.7555, 0.7858, 0.8570, 0.6345,\n",
            "        0.4305, 0.5405, 0.5472, 0.3398, 0.6686, 0.6199, 0.4625, 0.3623, 0.6424,\n",
            "        0.6042, 0.7071, 0.7079, 0.7685, 0.7175, 0.5863, 0.6965, 0.7419, 0.6119,\n",
            "        0.8167, 0.8474, 0.8764, 0.7600, 0.8415, 0.7625, 0.9363, 0.7327, 0.4759,\n",
            "        0.6410, 0.8843, 0.8038, 0.7844, 0.6231, 0.7375, 0.5524, 0.3572, 0.6893,\n",
            "        0.6436, 0.5032, 0.6147, 0.6947, 0.6610, 0.6306, 0.7615, 0.8037, 0.6679,\n",
            "        0.5438, 0.7383, 0.8871, 0.8300, 0.8773, 0.8879, 0.8902, 0.5391, 0.6226,\n",
            "        0.4501, 0.8430, 0.7490, 0.4853, 0.5104, 0.7445, 0.3479, 0.6950, 0.6395,\n",
            "        0.4587, 0.3758, 0.4169, 0.3596, 0.4912, 0.6282, 0.8426, 0.7944, 0.6958,\n",
            "        0.8332, 0.8600, 0.7926, 0.8101, 0.7422, 0.8108, 0.5512, 0.6436, 0.4507,\n",
            "        0.6837, 0.6206, 0.3669, 0.5633, 0.8018, 0.7098, 0.7255, 0.5595, 0.7698,\n",
            "        0.8003, 0.7950, 0.8141, 0.8454, 0.9192, 0.8058, 0.8657, 0.8639, 0.8624,\n",
            "        0.8367, 0.7426, 0.6970, 0.7991, 0.5832, 0.7128, 0.7631, 0.5971, 0.7540,\n",
            "        0.6506, 0.7833, 0.7423, 0.8301, 0.6304, 0.8940, 0.6347, 0.7259, 0.8108,\n",
            "        0.7570, 0.5753, 0.2198, 0.3236, 0.6751, 0.7902, 0.6822, 0.4774, 0.5024,\n",
            "        0.4817, 0.8092, 0.8931, 0.9335, 0.7602, 0.6214, 0.8471, 0.8343, 0.5555,\n",
            "        0.3435, 0.6715, 0.5987, 0.6487, 0.5704, 0.6141, 0.4428, 0.6028, 0.6872,\n",
            "        0.7346, 0.6881, 0.5439, 0.8932, 0.9170, 0.8531, 0.9048, 0.8973, 0.7881,\n",
            "        0.5032, 0.7212, 0.6259, 0.8636, 0.6288, 0.3385, 0.3283, 0.6787, 0.7724,\n",
            "        0.6580, 0.4476, 0.4749, 0.4298, 0.7610, 0.8412, 0.8995, 0.6955, 0.5132,\n",
            "        0.7561, 0.7753, 0.4556, 0.3415, 0.6676, 0.7808, 0.6723, 0.6047, 0.5439,\n",
            "        0.4458, 0.5557, 0.7003, 0.9091, 0.8934, 0.8149, 0.9062, 0.7969, 0.7557,\n",
            "        0.8131, 0.8142, 0.6626, 0.3471, 0.7009, 0.6293, 0.4925, 0.4074, 0.4601,\n",
            "        0.4272, 0.7392, 0.8373, 0.8836, 0.8448, 0.7006, 0.8408, 0.7606, 0.5969,\n",
            "        0.7067, 0.7527, 0.6999, 0.4918, 0.6229, 0.4417, 0.8066, 0.7239, 0.4492,\n",
            "        0.7708, 0.9331, 0.9016, 0.8751, 0.7697, 0.9260, 0.8432, 0.8692, 0.8782,\n",
            "        0.8858, 0.7977, 0.6391, 0.6383, 0.7179, 0.7588, 0.7770, 0.6468, 0.6550,\n",
            "        0.7781, 0.7676, 0.8538, 0.7585, 0.6094, 0.6616, 0.3415, 0.6870, 0.6077,\n",
            "        0.4569, 0.3474, 0.4198, 0.5844, 0.6965, 0.8169, 0.8606, 0.8385, 0.5662,\n",
            "        0.7377, 0.7739, 0.6883, 0.7537, 0.7672, 0.7486, 0.5014, 0.6151, 0.4895,\n",
            "        0.9111, 0.8640, 0.7175, 0.8320, 0.9348, 0.8878, 0.8433, 0.7940, 0.9224,\n",
            "        0.6920, 0.7046, 0.6900, 0.7595, 0.8863, 0.6558, 0.6533, 0.7230, 0.7662,\n",
            "        0.5726, 0.3188, 0.2593], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
            "tensor([0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0.], device='cuda:0')\n",
            "71\n",
            "tensor([0.3470, 0.6878, 0.5974, 0.4430, 0.3686, 0.4351, 0.3624, 0.6843, 0.7856,\n",
            "        0.8251, 0.7962, 0.6681, 0.8037, 0.7533, 0.6032, 0.6428, 0.7242, 0.7365,\n",
            "        0.4561, 0.5539, 0.3158, 0.7031, 0.6034, 0.5417, 0.7287, 0.8905, 0.7605,\n",
            "        0.7911, 0.7794, 0.8993, 0.8252, 0.8519, 0.7561, 0.6717, 0.7881, 0.5582,\n",
            "        0.6034, 0.6623, 0.5517, 0.6848, 0.4186, 0.4424, 0.6703, 0.6607, 0.5550,\n",
            "        0.5889, 0.3998, 0.3459, 0.6894, 0.6384, 0.4693, 0.3898, 0.4382, 0.4005,\n",
            "        0.5324, 0.8039, 0.8825, 0.8378, 0.7146, 0.8615, 0.8740, 0.7699, 0.6972,\n",
            "        0.6954, 0.6945, 0.4936, 0.6115, 0.4456, 0.7683, 0.6526, 0.3898, 0.5273,\n",
            "        0.7860, 0.6626, 0.8296, 0.7359, 0.9095, 0.7710, 0.8427, 0.8565, 0.8979,\n",
            "        0.9197, 0.8096, 0.8075, 0.7778, 0.8010, 0.6387, 0.4386, 0.3876, 0.5102,\n",
            "        0.5122, 0.7215, 0.7033, 0.6124, 0.7733, 0.7080, 0.7820, 0.7476, 0.6238,\n",
            "        0.5040, 0.7393, 0.3526, 0.7051, 0.6259, 0.4679, 0.3755, 0.4329, 0.3745,\n",
            "        0.7046, 0.8256, 0.8884, 0.8309, 0.6909, 0.7295, 0.7644, 0.6551, 0.7246,\n",
            "        0.7758, 0.7340, 0.4562, 0.5222, 0.4709, 0.8057, 0.8473, 0.6102, 0.7532,\n",
            "        0.9185, 0.9072, 0.9148, 0.8285, 0.9228, 0.8715, 0.8048, 0.7515, 0.8008,\n",
            "        0.8146, 0.6039, 0.6707, 0.8134, 0.8122, 0.8024, 0.6279, 0.3867, 0.4387,\n",
            "        0.3945, 0.3458, 0.6728, 0.6130, 0.6619, 0.5889, 0.5294, 0.4893, 0.5471,\n",
            "        0.6835, 0.9038, 0.8566, 0.8154, 0.9220, 0.8090, 0.6713, 0.7723, 0.7925,\n",
            "        0.6772, 0.3598, 0.3426, 0.6798, 0.6268, 0.6726, 0.5704, 0.5632, 0.4891,\n",
            "        0.6433, 0.7207, 0.8106, 0.8655, 0.7472, 0.8936, 0.9120, 0.8681, 0.7824,\n",
            "        0.8512, 0.8808, 0.7207, 0.6619, 0.4094, 0.3503, 0.6923, 0.6216, 0.4696,\n",
            "        0.3649, 0.6357, 0.5874, 0.7124, 0.8197, 0.8772, 0.7526, 0.6056, 0.7513,\n",
            "        0.7649, 0.6091, 0.6481, 0.6360, 0.6961, 0.6770, 0.7812, 0.6993, 0.9150,\n",
            "        0.8566, 0.5842, 0.7528, 0.9181, 0.6679, 0.5606, 0.4605, 0.7593, 0.6020,\n",
            "        0.6896, 0.7301, 0.8247, 0.7557, 0.3766, 0.3400, 0.6710, 0.6094, 0.6717,\n",
            "        0.5870, 0.6603, 0.4879, 0.6013, 0.7287, 0.7735, 0.6779, 0.6983, 0.8007,\n",
            "        0.8441, 0.7751, 0.8190, 0.7169, 0.7635, 0.5775, 0.7447, 0.6240, 0.7066,\n",
            "        0.4890, 0.3480, 0.6943, 0.6422, 0.4644, 0.3804, 0.4438, 0.3735, 0.4556,\n",
            "        0.6170, 0.7009, 0.7934, 0.6923, 0.8266, 0.8290, 0.7091, 0.7652, 0.8232,\n",
            "        0.7301, 0.4566, 0.5600, 0.4834, 0.7151, 0.6282, 0.3891, 0.5546, 0.7767,\n",
            "        0.6865, 0.5855, 0.4111, 0.7252, 0.5870, 0.7794, 0.7894, 0.8161, 0.9059,\n",
            "        0.7200, 0.7242, 0.8449, 0.8542, 0.8620, 0.7466, 0.6714, 0.7607, 0.6919,\n",
            "        0.6966, 0.6741, 0.5738, 0.7460, 0.6534, 0.6877, 0.6845, 0.7885, 0.6487,\n",
            "        0.8845, 0.6922, 0.7585, 0.9247, 0.9140, 0.7107, 0.3690, 0.6199, 0.5726,\n",
            "        0.3256, 0.6676, 0.7781, 0.5428, 0.4266, 0.4750, 0.4359, 0.7532, 0.8754,\n",
            "        0.9070, 0.6824, 0.6069, 0.7360, 0.6336, 0.3499, 0.6983, 0.6451, 0.4672,\n",
            "        0.3756, 0.4337, 0.4012, 0.5060, 0.8272, 0.8510, 0.8185, 0.6970, 0.8460,\n",
            "        0.8520, 0.7196, 0.6354, 0.6892, 0.7059, 0.5566, 0.6097, 0.4168, 0.6682,\n",
            "        0.5632, 0.3424, 0.5442, 0.7502, 0.6368, 0.7599, 0.6332, 0.8519, 0.7673,\n",
            "        0.8042, 0.8325, 0.8466, 0.9359, 0.7874, 0.8417, 0.8507, 0.8924, 0.7360,\n",
            "        0.4299, 0.3753, 0.4912, 0.4046, 0.6593, 0.6835, 0.6012, 0.7718, 0.7036,\n",
            "        0.7624, 0.7879, 0.7325, 0.5860, 0.8239, 0.3502, 0.6824, 0.6330, 0.4990,\n",
            "        0.4186, 0.6798, 0.6340, 0.7537, 0.8486, 0.8226, 0.7616, 0.5812, 0.7583,\n",
            "        0.7609, 0.6398, 0.7317, 0.8019, 0.9169, 0.8128, 0.8731, 0.8643, 0.9541,\n",
            "        0.8958, 0.7363, 0.6714, 0.8801, 0.7951, 0.7594, 0.7075, 0.9204, 0.7360,\n",
            "        0.8057, 0.6750, 0.6226, 0.3423, 0.6959, 0.6274, 0.4849, 0.3904, 0.4477,\n",
            "        0.6299, 0.7132, 0.8261, 0.8670, 0.8072, 0.5513, 0.7404, 0.8006, 0.7049,\n",
            "        0.7354, 0.7630, 0.7450, 0.5294, 0.6313, 0.7242, 0.9195, 0.8566, 0.6275,\n",
            "        0.8035, 0.9160, 0.9055, 0.9064, 0.6682, 0.8397, 0.6597, 0.6513, 0.6620,\n",
            "        0.6911, 0.8365, 0.6915, 0.7042, 0.6570, 0.6802, 0.5450, 0.3413, 0.6669,\n",
            "        0.5854, 0.4438, 0.5652, 0.6254, 0.4574, 0.5776, 0.7014, 0.7374, 0.7111,\n",
            "        0.5394, 0.8535, 0.8845, 0.8351, 0.8924, 0.9227, 0.9198, 0.5681, 0.6363,\n",
            "        0.5991, 0.8189, 0.7342, 0.3001, 0.4207, 0.3434, 0.6899, 0.6415, 0.4969,\n",
            "        0.3819, 0.4722, 0.3959, 0.5232, 0.6099, 0.6986, 0.6475, 0.4423, 0.7842,\n",
            "        0.8122, 0.7515, 0.8255, 0.8179, 0.8484, 0.7052, 0.7342, 0.5047, 0.7399,\n",
            "        0.6671, 0.4545, 0.6180, 0.8411, 0.7216, 0.6843, 0.4667, 0.7628, 0.6417,\n",
            "        0.6918, 0.6481, 0.7247, 0.7483, 0.5109, 0.7299, 0.8372, 0.8854, 0.8891,\n",
            "        0.7346, 0.7201, 0.8061, 0.7899, 0.8789, 0.8861, 0.7855, 0.9073, 0.8269,\n",
            "        0.8549, 0.8775, 0.9068, 0.6191, 0.8304, 0.6386, 0.6598, 0.8511, 0.7823,\n",
            "        0.6541, 0.4209, 0.7421, 0.7565, 0.7271, 0.6812, 0.7829, 0.7790, 0.7348,\n",
            "        0.7007, 0.6351, 0.5197, 0.6560, 0.5948, 0.5733, 0.3490, 0.6686, 0.6144,\n",
            "        0.4627, 0.5690, 0.6288, 0.6013, 0.5991, 0.7201, 0.7888, 0.6468, 0.5205,\n",
            "        0.7099, 0.8987, 0.8449, 0.8713, 0.9127, 0.9070, 0.7526, 0.5729, 0.4969,\n",
            "        0.7552, 0.6963, 0.4856, 0.6122, 0.7179, 0.5450, 0.3223, 0.6694, 0.7842,\n",
            "        0.6684, 0.4493, 0.4523, 0.4097, 0.7398, 0.8120, 0.9068, 0.6874, 0.5159,\n",
            "        0.7504, 0.7569, 0.4454], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
            "tensor([0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0.], device='cuda:0')\n",
            "74\n",
            "tensor([0.3421, 0.6968, 0.6499, 0.4787, 0.3751, 0.4363, 0.4078, 0.5227, 0.6655,\n",
            "        0.7367, 0.8242, 0.7294, 0.8528, 0.8823, 0.7707, 0.8196, 0.8698, 0.8040,\n",
            "        0.5345, 0.6827, 0.5637, 0.8132, 0.7275, 0.4434, 0.5610, 0.7849, 0.6770,\n",
            "        0.6664, 0.5850, 0.8485, 0.7004, 0.7596, 0.8577, 0.8379, 0.8791, 0.7970,\n",
            "        0.8298, 0.8700, 0.8721, 0.9033, 0.8116, 0.8432, 0.9123, 0.9059, 0.9266,\n",
            "        0.9035, 0.5147, 0.7090, 0.5710, 0.6156, 0.6667, 0.7155, 0.5693, 0.9070,\n",
            "        0.7823, 0.7536, 0.9098, 0.8806, 0.7826, 0.5619, 0.6116, 0.6441, 0.5673,\n",
            "        0.3652, 0.3348, 0.6872, 0.6080, 0.4472, 0.3586, 0.6171, 0.5772, 0.7166,\n",
            "        0.8372, 0.8607, 0.7309, 0.5240, 0.7478, 0.7487, 0.6130, 0.6629, 0.7274,\n",
            "        0.6881, 0.7227, 0.7960, 0.7025, 0.9031, 0.8152, 0.6519, 0.7807, 0.9259,\n",
            "        0.6982, 0.6315, 0.4130, 0.8141, 0.6264, 0.6746, 0.6597, 0.6704, 0.6422,\n",
            "        0.3287, 0.3530, 0.6804, 0.6304, 0.4987, 0.4107, 0.6634, 0.6581, 0.7505,\n",
            "        0.8569, 0.8368, 0.7946, 0.6108, 0.7784, 0.8241, 0.7140, 0.7775, 0.7981,\n",
            "        0.9088, 0.7406, 0.8308, 0.7659, 0.9229, 0.8788, 0.7761, 0.7533, 0.8959,\n",
            "        0.8253, 0.7245, 0.5896, 0.8591, 0.7539, 0.8284, 0.6748, 0.6610, 0.3465,\n",
            "        0.6751, 0.6278, 0.6832, 0.5964, 0.6396, 0.4576, 0.5743, 0.7125, 0.7607,\n",
            "        0.7090, 0.5908, 0.8826, 0.9076, 0.8832, 0.9036, 0.9336, 0.8233, 0.5734,\n",
            "        0.7464, 0.6548, 0.8508, 0.6564, 0.2642, 0.3452, 0.6903, 0.6311, 0.4798,\n",
            "        0.3747, 0.4350, 0.3880, 0.5219, 0.6549, 0.7429, 0.6765, 0.5143, 0.8325,\n",
            "        0.8565, 0.7717, 0.8097, 0.8385, 0.8709, 0.7015, 0.7831, 0.7188, 0.8385,\n",
            "        0.7175, 0.4117, 0.6153, 0.8195, 0.7359, 0.7247, 0.5651, 0.7713, 0.6129,\n",
            "        0.6934, 0.6929, 0.7074, 0.7935, 0.5481, 0.5902, 0.6579, 0.8582, 0.8649,\n",
            "        0.7394, 0.7122, 0.7914, 0.7825, 0.8635, 0.8639, 0.7680, 0.9003, 0.8530,\n",
            "        0.8694, 0.8803, 0.9300, 0.8193, 0.9520, 0.8478, 0.7164, 0.9026, 0.8227,\n",
            "        0.7455, 0.5021, 0.7491, 0.7250, 0.7381, 0.6812, 0.8145, 0.8206, 0.8471,\n",
            "        0.8255, 0.8551, 0.8584, 0.7595, 0.8100, 0.7522, 0.6480, 0.7768, 0.8424,\n",
            "        0.4304, 0.3383, 0.6725, 0.6054, 0.6361, 0.5550, 0.4958, 0.4567, 0.5108,\n",
            "        0.6523, 0.8758, 0.8397, 0.7831, 0.8900, 0.7854, 0.6361, 0.8068, 0.7956,\n",
            "        0.7198, 0.3786, 0.3452, 0.6910, 0.6106, 0.4368, 0.3215, 0.4006, 0.5624,\n",
            "        0.6846, 0.7748, 0.8326, 0.6407, 0.5004, 0.6846, 0.7410, 0.5667, 0.6007,\n",
            "        0.6302, 0.6604, 0.4122, 0.7731, 0.6229, 0.8501, 0.7648, 0.6200, 0.7358,\n",
            "        0.9053, 0.8321, 0.5552, 0.4891, 0.7688, 0.6089, 0.7093, 0.6769, 0.7438,\n",
            "        0.8448, 0.4620, 0.5007, 0.5229, 0.3505, 0.6742, 0.6037, 0.4608, 0.5654,\n",
            "        0.6393, 0.6190, 0.6100, 0.7433, 0.8092, 0.7260, 0.5768, 0.7566, 0.8785,\n",
            "        0.8085, 0.8267, 0.8862, 0.8696, 0.7326, 0.5909, 0.4232, 0.7564, 0.7600,\n",
            "        0.4800, 0.5817, 0.6516, 0.4643, 0.3366, 0.6669, 0.6121, 0.4572, 0.5822,\n",
            "        0.6321, 0.5938, 0.5781, 0.7154, 0.7634, 0.6920, 0.5455, 0.7353, 0.9010,\n",
            "        0.8018, 0.8655, 0.9018, 0.8564, 0.6360, 0.5928, 0.4599, 0.7754, 0.6814,\n",
            "        0.4652, 0.6020, 0.6816, 0.5993, 0.3493, 0.6952, 0.6268, 0.4638, 0.3643,\n",
            "        0.6421, 0.6197, 0.7361, 0.8201, 0.8550, 0.7083, 0.5753, 0.7465, 0.7327,\n",
            "        0.5925, 0.6889, 0.7528, 0.7331, 0.7290, 0.7817, 0.6229, 0.8915, 0.8453,\n",
            "        0.6786, 0.7984, 0.9156, 0.7381, 0.6392, 0.5877, 0.8329, 0.6936, 0.7566,\n",
            "        0.7490, 0.7917, 0.7473, 0.4128, 0.3508, 0.6959, 0.6277, 0.4790, 0.3907,\n",
            "        0.4486, 0.6375, 0.7399, 0.8313, 0.8736, 0.7503, 0.6050, 0.7830, 0.8170,\n",
            "        0.6944, 0.7359, 0.7929, 0.7792, 0.5331, 0.8140, 0.7000, 0.8896, 0.8641,\n",
            "        0.7365, 0.8046, 0.9187, 0.8772, 0.8580, 0.5406, 0.7392, 0.5806, 0.7116,\n",
            "        0.7321, 0.7916, 0.8540, 0.6709, 0.5416, 0.4808, 0.5438, 0.3415, 0.6846,\n",
            "        0.6202, 0.4307, 0.3260, 0.3993, 0.3457, 0.4676, 0.6243, 0.6895, 0.7806,\n",
            "        0.6103, 0.7868, 0.8171, 0.6968, 0.7463, 0.7817, 0.7205, 0.4745, 0.5545,\n",
            "        0.4274, 0.7532, 0.5964, 0.4125, 0.4937, 0.7525, 0.5739, 0.5534, 0.4036,\n",
            "        0.6502, 0.5063, 0.5843, 0.7651, 0.8281, 0.8835, 0.6862, 0.7276, 0.7295,\n",
            "        0.7859, 0.8476, 0.7026, 0.6476, 0.7686, 0.7849, 0.8452, 0.8269, 0.5096,\n",
            "        0.6981, 0.5127, 0.5630, 0.5892, 0.6817, 0.4916, 0.8400, 0.5643, 0.6144,\n",
            "        0.8607, 0.8102, 0.6594, 0.4150, 0.6195, 0.5317, 0.4861, 0.2888, 0.3472,\n",
            "        0.6750, 0.6151, 0.6566, 0.5578, 0.5181, 0.4658, 0.5491, 0.6889, 0.7501,\n",
            "        0.8262, 0.6979, 0.8693, 0.8756, 0.6165, 0.6304, 0.7808, 0.7678, 0.4305,\n",
            "        0.4563, 0.3166, 0.8194, 0.6849, 0.4980, 0.4177, 0.6973, 0.7008, 0.6072,\n",
            "        0.7919, 0.7038, 0.3466, 0.6715, 0.6002, 0.6463, 0.5716, 0.5372, 0.5228,\n",
            "        0.6531, 0.6940, 0.7735, 0.8493, 0.7385, 0.9016, 0.8966, 0.8323, 0.6952,\n",
            "        0.7629, 0.8324, 0.5834, 0.4265, 0.3032, 0.3520, 0.6963, 0.6340, 0.4775,\n",
            "        0.3912, 0.4549, 0.3956, 0.5255, 0.6602, 0.7168, 0.6270, 0.6637, 0.8223,\n",
            "        0.8493, 0.7508, 0.8189, 0.8370, 0.8373, 0.6543, 0.6177, 0.4689, 0.7930,\n",
            "        0.6841, 0.4304, 0.5429, 0.8022, 0.6894, 0.6285, 0.5189, 0.8026, 0.6422,\n",
            "        0.7288, 0.7069, 0.7044, 0.9134, 0.7318, 0.7362, 0.7570, 0.7996, 0.8221,\n",
            "        0.6564, 0.6444, 0.8228, 0.7912, 0.8491, 0.8763, 0.7585, 0.8836, 0.7710,\n",
            "        0.6431, 0.5562, 0.7579, 0.5868, 0.8604, 0.6326, 0.6597, 0.9182, 0.8909,\n",
            "        0.7701, 0.5012, 0.7956, 0.8020, 0.7669, 0.6158, 0.8086, 0.6483, 0.6398,\n",
            "        0.6273, 0.6312], device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
            "tensor([0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')\n",
            "65\n",
            "tensor([0.3427, 0.6654, 0.6026, 0.6482, 0.5680, 0.6411, 0.4623, 0.5682, 0.6652,\n",
            "        0.7409, 0.6758, 0.7310, 0.8419, 0.8869, 0.8600, 0.8946, 0.8169, 0.7803,\n",
            "        0.5100, 0.6805, 0.5831, 0.6898, 0.4369, 0.3480, 0.7057, 0.6555, 0.4885,\n",
            "        0.3992, 0.4575, 0.4089, 0.5364, 0.8282, 0.8672, 0.8161, 0.6975, 0.8493,\n",
            "        0.8889, 0.7970, 0.7533, 0.7987, 0.7875, 0.5841, 0.6857, 0.5188, 0.7923,\n",
            "        0.6587, 0.3874, 0.5110, 0.8003, 0.6842, 0.8186, 0.7419, 0.9165, 0.8004,\n",
            "        0.8540, 0.8784, 0.8819, 0.9127, 0.7312, 0.8375, 0.8626, 0.8568, 0.7193,\n",
            "        0.4849, 0.4419, 0.5976, 0.5667, 0.7148, 0.7481, 0.6273, 0.8175, 0.7488,\n",
            "        0.7547, 0.7378, 0.6706, 0.4391, 0.6852, 0.3453, 0.6672, 0.7730, 0.6514,\n",
            "        0.5429, 0.4639, 0.3563, 0.4903, 0.6397, 0.8706, 0.8413, 0.7126, 0.8638,\n",
            "        0.7354, 0.7437, 0.7417, 0.7793, 0.6073, 0.3439, 0.6905, 0.6107, 0.4617,\n",
            "        0.3780, 0.4401, 0.4021, 0.5302, 0.8209, 0.8752, 0.8123, 0.7065, 0.8306,\n",
            "        0.8127, 0.6951, 0.7572, 0.8202, 0.8356, 0.6315, 0.6533, 0.5738, 0.8363,\n",
            "        0.7970, 0.5826, 0.8861, 0.9615, 0.9105, 0.8807, 0.7268, 0.9292, 0.8785,\n",
            "        0.9014, 0.8919, 0.9104, 0.9635, 0.6443, 0.6682, 0.6598, 0.7312, 0.7595,\n",
            "        0.6186, 0.6557, 0.7230, 0.7702, 0.8666, 0.8397, 0.6239, 0.7861, 0.5295,\n",
            "        0.3232, 0.6753, 0.7787, 0.6458, 0.4185, 0.4881, 0.4133, 0.5176, 0.8353,\n",
            "        0.9018, 0.8685, 0.5789, 0.7279, 0.8261, 0.7126, 0.5765, 0.3492, 0.6817,\n",
            "        0.6184, 0.6686, 0.6098, 0.6838, 0.5424, 0.6945, 0.7642, 0.8327, 0.7414,\n",
            "        0.7972, 0.8831, 0.8968, 0.8780, 0.8722, 0.7620, 0.7887, 0.5756, 0.7306,\n",
            "        0.6191, 0.7248, 0.5081, 0.3437, 0.6913, 0.6242, 0.4614, 0.3706, 0.4176,\n",
            "        0.3736, 0.4600, 0.6459, 0.8416, 0.8064, 0.6721, 0.8255, 0.8291, 0.7333,\n",
            "        0.7053, 0.7191, 0.7329, 0.5024, 0.5792, 0.4669, 0.7066, 0.5645, 0.3668,\n",
            "        0.5546, 0.7818, 0.6875, 0.6364, 0.6904, 0.8973, 0.7912, 0.8487, 0.8236,\n",
            "        0.8551, 0.9494, 0.8022, 0.7998, 0.8325, 0.8336, 0.8052, 0.4051, 0.3688,\n",
            "        0.5089, 0.5621, 0.6469, 0.7292, 0.6482, 0.8817, 0.7662, 0.8085, 0.7938,\n",
            "        0.8429, 0.5236, 0.8076, 0.5586, 0.5489, 0.3255, 0.6756, 0.7904, 0.5636,\n",
            "        0.4589, 0.4405, 0.6363, 0.7895, 0.8820, 0.7921, 0.7803, 0.6480, 0.6394,\n",
            "        0.3485, 0.6792, 0.6198, 0.4660, 0.3659, 0.6249, 0.5851, 0.7211, 0.8113,\n",
            "        0.7802, 0.7260, 0.5889, 0.7569, 0.7635, 0.6069, 0.6576, 0.7339, 0.8812,\n",
            "        0.7049, 0.8103, 0.7370, 0.8974, 0.8234, 0.6125, 0.8080, 0.8207, 0.7546,\n",
            "        0.7428, 0.6135, 0.8613, 0.7215, 0.7750, 0.7194, 0.5621, 0.6622, 0.3251,\n",
            "        0.6797, 0.7874, 0.6710, 0.4703, 0.5017, 0.4653, 0.7719, 0.8514, 0.9104,\n",
            "        0.7298, 0.5838, 0.8061, 0.8152, 0.5188, 0.3455, 0.6851, 0.6047, 0.4527,\n",
            "        0.3609, 0.4383, 0.5934, 0.6991, 0.8184, 0.8623, 0.8301, 0.6049, 0.7325,\n",
            "        0.8013, 0.5873, 0.7406, 0.7848, 0.7734, 0.5084, 0.6008, 0.4739, 0.8978,\n",
            "        0.8515, 0.6456, 0.7545, 0.9152, 0.8691, 0.8511, 0.7084, 0.8993, 0.6496,\n",
            "        0.6300, 0.6517, 0.7389, 0.8643, 0.6716, 0.6933, 0.7704, 0.7626, 0.6366,\n",
            "        0.3780, 0.3199, 0.3370, 0.6908, 0.6331, 0.4780, 0.3471, 0.4099, 0.3769,\n",
            "        0.4949, 0.6318, 0.7239, 0.6412, 0.6735, 0.8130, 0.8256, 0.7079, 0.7501,\n",
            "        0.8236, 0.8356, 0.6223, 0.7664, 0.5604, 0.8200, 0.7143, 0.5143, 0.6324,\n",
            "        0.7947, 0.7085, 0.5875, 0.5247, 0.7911, 0.6459, 0.6756, 0.6304, 0.6965,\n",
            "        0.8030, 0.5714, 0.8151, 0.8042, 0.8386, 0.8161, 0.6027, 0.5797, 0.6925,\n",
            "        0.7880, 0.8196, 0.8446, 0.7637, 0.8776, 0.8290, 0.8421, 0.8137, 0.8775,\n",
            "        0.5805, 0.8282, 0.6397, 0.6888, 0.9029, 0.8478, 0.7377, 0.6459, 0.8334,\n",
            "        0.7951, 0.7284, 0.6062, 0.7868, 0.7446, 0.7045, 0.7141, 0.6589, 0.6526,\n",
            "        0.5709, 0.5139, 0.3447, 0.6737, 0.6219, 0.4639, 0.5643, 0.6110, 0.5827,\n",
            "        0.5935, 0.7392, 0.7905, 0.6995, 0.5811, 0.7539, 0.8057, 0.8451, 0.8560,\n",
            "        0.8917, 0.9116, 0.7693, 0.8536, 0.5472, 0.8408, 0.7112, 0.5624, 0.7537,\n",
            "        0.8837, 0.7546, 0.5749, 0.3336, 0.6866, 0.6250, 0.4792, 0.3858, 0.4481,\n",
            "        0.4222, 0.7297, 0.8457, 0.8765, 0.8427, 0.7156, 0.7428, 0.7831, 0.6631,\n",
            "        0.7142, 0.7762, 0.7606, 0.4988, 0.6294, 0.4663, 0.7295, 0.6464, 0.6627,\n",
            "        0.7629, 0.9167, 0.8909, 0.8585, 0.8056, 0.9398, 0.8850, 0.8815, 0.8438,\n",
            "        0.7293, 0.8431, 0.7309, 0.6700, 0.7004, 0.7503, 0.8207, 0.5509, 0.5022,\n",
            "        0.6461, 0.5445, 0.5104, 0.5299, 0.3437, 0.6695, 0.6026, 0.4394, 0.5541,\n",
            "        0.6309, 0.5948, 0.6942, 0.7192, 0.8052, 0.7627, 0.5302, 0.7138, 0.7471,\n",
            "        0.8272, 0.8197, 0.8655, 0.8868, 0.7183, 0.8484, 0.5410, 0.7677, 0.6561,\n",
            "        0.4405, 0.7009, 0.8398, 0.7572, 0.5261, 0.3584, 0.3238, 0.8259, 0.6937,\n",
            "        0.5032, 0.4023, 0.6829, 0.7020, 0.5778, 0.7819, 0.6994],\n",
            "       device='cuda:0', grad_fn=<SqueezeBackward0>)\n",
            "tensor([0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
            "        0., 0., 1., 1., 0., 0., 0.], device='cuda:0')\n",
            "66\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-cedbe60cf2b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdoc_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-c488cfe15c7f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, doc_lens)\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0mrel_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrel_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0;31m# if self.args.device is not None:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                 \u001b[0mrel_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrel_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m                 \u001b[0mrel_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrel_pos_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrel_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkN9z5PmChia",
        "outputId": "19d9be3e-c963-4ff1-97c8-83afe163db53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        " l = [ 4, 25, 24, 36, 31, 16, 26,  1,  8, 20,  6, 15, 15, 12, 13, 13, 12, 31,\n",
        "        34, 21, 12, 30, 16, 28, 22, 17, 16, 10, 56, 27, 16, 24, 41, 32, 28, 23,\n",
        "        14,  3, 17, 22, 20, 30, 22, 14,  3, 17, 26, 32, 22, 12,  6,  5, 18, 10,\n",
        "         3, 23, 30, 11, 20, 32, 13,  2, 25, 31, 24, 21,  3, 25, 10, 20,  9, 41,\n",
        "        20,  4, 34, 27, 19, 23, 29,  3, 31, 41, 24, 12,  3, 15, 15, 21, 17, 21,\n",
        "        36, 22, 44, 13,  7, 25,  4, 20, 20, 21, 25,  4,  8, 22,  8,  4,  7,  8,\n",
        "        11, 14,  9, 14,  7, 33, 20, 12, 15, 50, 27, 11, 22, 26, 30, 24, 29,  3,\n",
        "        21, 31, 38,  7, 10, 12, 36, 21,  1, 11, 12, 17, 14, 12, 23, 16, 17, 21,\n",
        "         5,  9,  3, 39, 15,  4, 27, 34, 18, 11, 21, 23, 30, 20, 13, 15, 13, 32,\n",
        "        40, 17, 18, 12,  4, 12, 27, 32, 17, 33, 12,  2,  6, 18, 19, 14, 24, 33,\n",
        "        17, 16, 13, 35, 53, 26, 34, 26, 13,  8,  4, 29, 22, 16, 12, 45, 13, 12,\n",
        "        33,  7, 19, 11, 13, 26,  7,  3, 11, 19, 16, 10, 12, 21, 35, 30, 38, 24,\n",
        "        27, 15, 19, 39,  5, 12, 35, 20, 11,  8, 10,  3, 24, 23,  6, 21, 20,  7,\n",
        "        18, 16,  9, 21, 14, 27, 17,  9, 13,  3, 25, 26, 27, 38, 17, 21, 21,  9,\n",
        "         7, 26, 16,  8, 11, 14,  8,  9,  7, 18,  3, 11, 14, 11,  9,  7,  7, 21,\n",
        "        12, 10,  2, 18, 12, 25,  5,  8, 22, 16, 17,  8, 10,  6,  7,  5, 18,  5,\n",
        "         5,  7, 23, 25, 12, 20,  7,  7,  7, 23, 15, 12, 15,  4,  7,  6, 20,  8,\n",
        "        14, 11, 13, 22,  7,  4,  2, 34, 12, 31,  2, 39, 29,  5, 11,  6,  4, 29,\n",
        "        28, 23, 16, 29,  8, 17,  9, 24, 41, 20, 36, 34,  8,  8, 19,  1, 17,  3,\n",
        "        14, 27, 13, 25, 13, 28, 21, 13,  6,  3, 22, 25, 24, 34, 20, 16,  2, 25,\n",
        "        12, 22, 28, 12, 26,  2, 29, 11, 40, 12, 19,  2, 16, 22, 35, 21, 15,  2,\n",
        "        21, 25,  9, 17, 14, 17,  7, 41, 28, 38, 14, 11, 22,  4, 25, 27,  9, 25,\n",
        "        37,  2, 17, 17, 23, 19,  2, 13, 13, 25, 23, 29,  5, 35, 32, 12, 25, 33,\n",
        "         9, 37, 45, 15, 33, 50, 27, 32, 35, 30, 33, 17, 17, 13, 23,  4, 38, 31,\n",
        "        29,  5, 44, 28, 30, 47, 46, 17, 45, 23, 29, 21, 17, 25, 51,  7, 12, 34,\n",
        "        14, 35, 15, 36, 32, 11, 29, 54, 11, 22, 14, 52, 42, 18,  7, 12, 24, 20,\n",
        "        31, 43, 32,  9, 37, 31, 21, 11, 25, 25, 14, 17, 17, 17, 17, 26, 10,  6,\n",
        "         9, 14,  8, 26, 10, 23,  3, 24,  1, 30,  7,  3, 29, 30, 18, 12, 17, 24,\n",
        "         9,  4, 34, 33, 33, 33, 22,  5, 17,  3, 27, 12, 30, 13, 21, 47, 26, 16,\n",
        "        20, 26,  7, 14, 20, 11, 32, 26, 36, 16, 35, 13, 38, 15, 20, 46, 26, 38,\n",
        "        29, 13, 33,  8, 11, 17, 33,  9, 21, 22, 28, 16, 28, 32,  3, 46, 33, 15,\n",
        "        33, 39, 20, 38, 25,  5, 10, 45, 11, 36, 19, 28, 39, 20, 38, 24,  8, 14,\n",
        "        38, 29, 29, 26,  5, 33, 19, 15, 22, 23, 16, 14, 17, 23, 21, 14, 17, 22,\n",
        "        13, 12, 38, 45, 23,  4, 31, 18, 17, 16,  9,  6,  4, 19, 41, 27, 28, 45,\n",
        "        29, 10, 17, 19, 15, 22, 19, 26, 18,  7, 31, 28, 15, 38, 19, 19, 20, 16,\n",
        "        14, 11, 36, 17, 34, 14, 25, 30, 23, 20, 30, 33, 24, 17, 29, 40, 38, 15,\n",
        "        21, 18,  8]\n",
        "len(l)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "651"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFY5YJY6koTP"
      },
      "source": [
        "torch.manual_seed(233)\n",
        "\n",
        "class SummaRuNNer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        # Parameters\n",
        "        self.vocab_size = config.vocab_size        \n",
        "        self.embedding_dim = config.embedding_dim\n",
        "        self.position_size = config.position_size\n",
        "        self.position_dim = config.position_dim\n",
        "        self.word_input_size = config.word_input_size\n",
        "        self.sent_input_size = config.sent_input_size\n",
        "        self.word_GRU_hidden_units = config.word_GRU_hidden_units\n",
        "        self.sent_GRU_hidden_units = config.sent_GRU_hidden_units\n",
        "        \n",
        "        # Network\n",
        "        self.word_embedding = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
        "        self.word_embedding.weight.data.copy_(torch.from_numpy(config.pretrained_embedding))\n",
        "        self.position_embedding = nn.Embedding(self.position_size, self.position_dim)\n",
        "\n",
        "        self.word_GRU = nn.GRU(\n",
        "            input_size = self.word_input_size,\n",
        "            hidden_size = self.word_GRU_hidden_units,\n",
        "            batch_first = True,\n",
        "            bidirectional = True)\n",
        "        self.sent_GRU = nn.GRU(\n",
        "            input_size = self.sent_input_size,\n",
        "            hidden_size = self.sent_GRU_hidden_units,\n",
        "            batch_first = True,\n",
        "            bidirectional = True)\n",
        "        \n",
        "        self.tanh = nn.Tanh()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "        self.fc1 = nn.Linear(400, 300)\n",
        "        self.fc2 = nn.Linear(400, 300)\n",
        "        \n",
        "        # Parameters of Classification Layer\n",
        "        self.Wc = Parameter(torch.randn(1, 300))\n",
        "        self.Ws = Parameter(torch.randn(300, 300))\n",
        "        self.Wr = Parameter(torch.randn(00, 300))\n",
        "        self.Wp = Parameter(torch.randn(1, 50))\n",
        "        self.b = Parameter(torch.randn(1))\n",
        "\n",
        "    def _avg_pooling(self, x, sequence_length):\n",
        "        result = []\n",
        "        for index, data in enumerate(x):\n",
        "            avg_pooling = torch.mean(data[:sequence_length[index][0], :], dim = 0)\n",
        "            result.append(avg_pooling)\n",
        "        return torch.cat(result, dim = 0) \n",
        "     \n",
        "    def forward(self, x):\n",
        "        sequence_length = torch.sum(torch.sign(x), dim = 1).data\n",
        "        print(sequence_length)\n",
        "        sequence_num = sequence_length.size()[0]\n",
        "\n",
        "        # word level GRU\n",
        "        word_features = self.word_embedding(x)\n",
        "        word_outputs, _ = self.word_GRU(word_features)\n",
        "        # sentence level GRU\n",
        "        sent_features = self._avg_pooling(word_outputs, sequence_length)\n",
        "        sent_outputs, _ = self.sent_GRU(sent_features.view(1, -1, self.sent_input_size))\n",
        "        # document representation\n",
        "        doc_features = self._avg_pooling(sent_outputs, [[x.size(0)]])\n",
        "        doc = torch.transpose(self.tanh(self.fc1(doc_features)), 0, 1)\n",
        "        # classifier layer\n",
        "        outputs = []\n",
        "        sent_outputs = sent_outputs.view(-1, 2 * self.sent_GRU_hidden_units)\n",
        "        \n",
        "        s = Variable(torch.zeros(100, 1)).cuda()\n",
        "        \n",
        "        for position, sent_hidden in enumerate(sent_outputs):\n",
        "            h = torch.transpose(self.tanh(self.fc2(sent_hidden.view(1, -1))), 0, 1)\n",
        "            position_index = Variable(torch.LongTensor([[position]])).cuda()\n",
        "            p = self.position_embedding(position_index).view(-1, 1)\n",
        "            \n",
        "            content = torch.mm(self.Wc, h)\n",
        "            salience = torch.mm(torch.mm(h.view(1, -1), self.Ws), doc)\n",
        "            novelty = -1 * torch.mm(torch.mm(h.view(1, -1), self.Wr), self.tanh(s))\n",
        "            position = torch.mm(self.Wp, p)\n",
        "            bias = self.b\n",
        "            Prob = self.sigmoid(content + salience + novelty + position + bias)\n",
        "            s = s + torch.mm(h, Prob)\n",
        "            outputs.append(Prob)\n",
        "        \n",
        "        return torch.cat(outputs, dim = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wegyQ8Npq84v",
        "outputId": "7b1d796c-705a-4573-df20-1cffee9f6971",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "weights.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3000002, 300)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNFI6ZMyq9hX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3bsteJkqhCe"
      },
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "Config = namedtuple('parameters',\n",
        "        ['vocab_size', 'embedding_dim', 'position_size','position_dim','word_input_size','sent_input_size',\n",
        "        'word_GRU_hidden_units','sent_GRU_hidden_units','pretrained_embedding'])\n",
        "\n",
        "config = Config(\n",
        "        vocab_size = weights.shape[0],\n",
        "        embedding_dim = weights.shape[1],\n",
        "        position_size = 500,\n",
        "        position_dim = 50,\n",
        "        word_input_size = 300,\n",
        "        sent_input_size = 2 * 200,\n",
        "        word_GRU_hidden_units = 200,\n",
        "        sent_GRU_hidden_units = 200,\n",
        "        pretrained_embedding = weights)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}