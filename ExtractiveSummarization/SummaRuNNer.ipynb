{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IRE_25GB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHiloSDrTlZc"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext import data\n",
        "from torchtext.data import Field\n",
        "from torchtext.data import BucketIterator\n",
        "from torchtext.vocab import GloVe\n",
        "# from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "from time import time\n",
        "import math\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import gensim\n",
        "\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Parameter\n",
        "from torch.nn.utils import clip_grad_norm_\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LXF_u9eTRyF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edfdf8a1-be45-4eb3-a887-7a8900ff18ce"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPU0rzacTSPL"
      },
      "source": [
        "from gensim import models\n",
        "model = models.KeyedVectors.load_word2vec_format('/content/gdrive/My Drive/IRE_PROJECT/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS9xB6I4Ve1x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50aa4702-1f12-4416-91df-9a0be4e6e6dc"
      },
      "source": [
        "model = GloVe(name='6B', dim=50)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:28, 2.22MB/s]                          \n",
            "100%|█████████▉| 399849/400000 [00:14<00:00, 29079.79it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD7x2ulbfNOf",
        "outputId": "02ebb637-a067-4268-edf1-5abb8d7d2d3f"
      },
      "source": [
        "m.vectors.numpy()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.418   ,  0.24968 , -0.41242 , ..., -0.18411 , -0.11514 ,\n",
              "        -0.78581 ],\n",
              "       [ 0.013441,  0.23682 , -0.16899 , ..., -0.56657 ,  0.044691,\n",
              "         0.30392 ],\n",
              "       [ 0.15164 ,  0.30177 , -0.16763 , ..., -0.35652 ,  0.016413,\n",
              "         0.10216 ],\n",
              "       ...,\n",
              "       [-0.51181 ,  0.058706,  1.0913  , ..., -0.25003 , -1.125   ,\n",
              "         1.5863  ],\n",
              "       [-0.75898 , -0.47426 ,  0.4737  , ...,  0.78954 , -0.014116,\n",
              "         0.6448  ],\n",
              "       [ 0.072617, -0.51393 ,  0.4728  , ..., -0.18907 , -0.59021 ,\n",
              "         0.55559 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtVfBAy9Tr9i"
      },
      "source": [
        "max_sent = -1\n",
        "train_dataset={'story':[],'labels':[]}\n",
        "for line in open('/content/gdrive/My Drive/IRE_PROJECT/train_data.json'):\n",
        "  data=json.loads(line)\n",
        "  max_sent = max(max_sent,len(data['story']))\n",
        "\n",
        "  story = []\n",
        "  for d in data['story']:\n",
        "    story.append(str.lower(d))\n",
        "\n",
        "  # print(data['story'])\n",
        "  # print(data['labels'])\n",
        "  train_dataset['story'].append(story)\n",
        "  train_dataset['labels'].append(data['labels'])\n",
        "# train=json.loads(pathlib.Path('/content/train_data.json').read_text())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjE5v_XWwc7Q"
      },
      "source": [
        "validation_dataset={'story':[],'labels':[]}\n",
        "for line in open('/content/gdrive/My Drive/IRE_PROJECT/validation_data.json'):\n",
        "  data=json.loads(line)\n",
        "  # print(data['story'])\n",
        "  # print(data['labels'])\n",
        "  story = []\n",
        "  for d in data['story']:\n",
        "    story.append(str.lower(d))\n",
        "\n",
        "  max_sent = max(max_sent,len(data['story']))\n",
        "\n",
        "  validation_dataset['story'].append(story)\n",
        "  \n",
        "  validation_dataset['labels'].append(data['labels'])\n",
        "# train=json.loads(pathlib.Path('/content/train_data.json').read_text())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eB7blGp2E_4C"
      },
      "source": [
        "test_dataset={'story':[],'labels':[],'summary':[]}\n",
        "max_summ = -1\n",
        "for line in open('/content/gdrive/My Drive/IRE_PROJECT/test_data.json'):\n",
        "  data=json.loads(line)\n",
        "  # print(data)\n",
        "  # print(data['story'])\n",
        "  # print(data['labels'])\n",
        "  story = []\n",
        "  for d in data['story']:\n",
        "    story.append(str.lower(d))\n",
        "  summary=[]\n",
        "  for d in data['summary']:\n",
        "    summary.append(str.lower(d))\n",
        "\n",
        "  max_sent = max(max_sent,len(data['story']))\n",
        "  max_summ = max(max_summ,len(data['summary']))\n",
        "  test_dataset['story'].append(story)\n",
        "  \n",
        "  test_dataset['labels'].append(data['labels'])\n",
        "\n",
        "  test_dataset['summary'].append(summary)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oATQGnfc37LT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4909c932-60e4-4b5c-fece-1b5c40e3bda2"
      },
      "source": [
        "max_summ"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwQXy5EqTvA1"
      },
      "source": [
        "train_df = pd.DataFrame(train_dataset, columns=[\"story\", \"labels\"])\n",
        "validation_df=pd.DataFrame(validation_dataset, columns=[\"story\", \"labels\"])\n",
        "test_df = pd.DataFrame(test_dataset, columns=[\"story\", \"labels\",\"summary\"])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjRqssoIT9lY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "9b890212-2a8c-4dfe-dbfe-7be319ff43bd"
      },
      "source": [
        "test_df"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>story</th>\n",
              "      <th>labels</th>\n",
              "      <th>summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[washington ( cnn ) --, nearly 16 years after ...</td>\n",
              "      <td>[0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
              "      <td>[the faa says boeing has failed to meet a dead...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[( cnn ) --, cockiness and swagger serve him o...</td>\n",
              "      <td>[0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[northern ireland native won u.s. open by eigh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[in the world of military strategy , every con...</td>\n",
              "      <td>[1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[military strategy requires examining worst-ca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[philadelphia ( cnn ) --, on a warm , late sum...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
              "      <td>[the home field of the north philadelphia azte...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[an interactive map showing the names and addr...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[the publisher issues a statement supporting t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>395</th>\n",
              "      <td>[( cnn ) --, a town engulfed by fighting in so...</td>\n",
              "      <td>[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[a decision to switch a local government headq...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>396</th>\n",
              "      <td>[san juan , puerto rico ( cnn ) --, investigat...</td>\n",
              "      <td>[0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[blaze started with massive explosion early fr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>397</th>\n",
              "      <td>[( cnn ) --, the islamist rebels fighting to o...</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[french airstrikes hit behind islamist rebel l...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>398</th>\n",
              "      <td>[( cnn ), -- headlines and pundits once again ...</td>\n",
              "      <td>[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
              "      <td>[north korea 's artillery barrage was serious ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399</th>\n",
              "      <td>[( cnn ) --, a landlord wants the ohio civil r...</td>\n",
              "      <td>[0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[jamie hein is asking the state civil rights c...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>400 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 story  ...                                            summary\n",
              "0    [washington ( cnn ) --, nearly 16 years after ...  ...  [the faa says boeing has failed to meet a dead...\n",
              "1    [( cnn ) --, cockiness and swagger serve him o...  ...  [northern ireland native won u.s. open by eigh...\n",
              "2    [in the world of military strategy , every con...  ...  [military strategy requires examining worst-ca...\n",
              "3    [philadelphia ( cnn ) --, on a warm , late sum...  ...  [the home field of the north philadelphia azte...\n",
              "4    [an interactive map showing the names and addr...  ...  [the publisher issues a statement supporting t...\n",
              "..                                                 ...  ...                                                ...\n",
              "395  [( cnn ) --, a town engulfed by fighting in so...  ...  [a decision to switch a local government headq...\n",
              "396  [san juan , puerto rico ( cnn ) --, investigat...  ...  [blaze started with massive explosion early fr...\n",
              "397  [( cnn ) --, the islamist rebels fighting to o...  ...  [french airstrikes hit behind islamist rebel l...\n",
              "398  [( cnn ), -- headlines and pundits once again ...  ...  [north korea 's artillery barrage was serious ...\n",
              "399  [( cnn ) --, a landlord wants the ohio civil r...  ...  [jamie hein is asking the state civil rights c...\n",
              "\n",
              "[400 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxwWx_mOyam0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "533c5c73-4cb6-4666-e76d-9a8bff37eb39"
      },
      "source": [
        "validation_df"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>story</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[( cnn ) --, sporting a black t-shirt proudly ...</td>\n",
              "      <td>[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[washington ( cnn ) -- how does the american p...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[( cnn ) --, basketball star dennis rodman see...</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[( cnn ), -- check with your airline before yo...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[riyadh , saudi arabia --, ahmad al shayea is ...</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>355</th>\n",
              "      <td>[( cnn ) --, the world health organization rai...</td>\n",
              "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>356</th>\n",
              "      <td>[dali , guizhou , china ( cnn ) -- shi wenchan...</td>\n",
              "      <td>[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>357</th>\n",
              "      <td>[cairo , egypt ( cnn ) -- one man died and ano...</td>\n",
              "      <td>[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>358</th>\n",
              "      <td>[london ( cnn ) -- the cleanup operation conti...</td>\n",
              "      <td>[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>359</th>\n",
              "      <td>[washington ( cnn ) -- what a difference ., ba...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>360 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 story                                             labels\n",
              "0    [( cnn ) --, sporting a black t-shirt proudly ...  [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...\n",
              "1    [washington ( cnn ) -- how does the american p...  [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, ...\n",
              "2    [( cnn ) --, basketball star dennis rodman see...  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...\n",
              "3    [( cnn ), -- check with your airline before yo...  [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...\n",
              "4    [riyadh , saudi arabia --, ahmad al shayea is ...  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...\n",
              "..                                                 ...                                                ...\n",
              "355  [( cnn ) --, the world health organization rai...  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...\n",
              "356  [dali , guizhou , china ( cnn ) -- shi wenchan...  [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...\n",
              "357  [cairo , egypt ( cnn ) -- one man died and ano...  [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
              "358  [london ( cnn ) -- the cleanup operation conti...  [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\n",
              "359  [washington ( cnn ) -- what a difference ., ba...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, ...\n",
              "\n",
              "[360 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ne0kfTYTxGg"
      },
      "source": [
        "# weights = model.vectors\n",
        "dim = 50\n",
        "weights = model.vectors.numpy()\n",
        "weights = np.insert(weights, 0,np.zeros(dim),axis = 0)\n",
        "weights = np.insert(weights, 0,np.zeros(dim),axis = 0)\n",
        "# weights = torch.FloatTensor(weights)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09N-JpHvUABM"
      },
      "source": [
        "word2id = {model.itos[i]:i+2 for i in range(len(model.itos))}\n",
        "word2id['<pad>'] = 0\n",
        "word2id['<unk>'] = 1"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08kSheSShq8z"
      },
      "source": [
        "# np.save('/content/gdrive/My Drive/IRE_PROJECT/weights.npy',weights)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PI68Nt9IHwn_"
      },
      "source": [
        "# with open('/content/gdrive/My Drive/IRE_PROJECT/word2id.json', 'w') as fp:\n",
        "    # json.dump(word2id, fp)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sx94SjLlJhdz"
      },
      "source": [
        "weights = np.load('/content/gdrive/My Drive/IRE_PROJECT/weights.npy')\n",
        "weights = torch.FloatTensor(weights)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU21QntoJ7d3"
      },
      "source": [
        "word2id = json.load(open('/content/gdrive/My Drive/IRE_PROJECT/word2id.json'))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjejcmkdWtMa"
      },
      "source": [
        "class Dataset():\n",
        "    def __init__(self, data_list):\n",
        "        self._data = data_list\n",
        "    def __len__(self):\n",
        "        return len(self._data)\n",
        "        \n",
        "    def __call__(self, batch_size, shuffle = True):\n",
        "        max_len = len(self)\n",
        "        \n",
        "        indices = [i for i in range(0,max_len,batch_size)]\n",
        "        \n",
        "        if shuffle:\n",
        "            indices = np.random.randint(0,max_len-1,math.ceil(max_len/batch_size))\n",
        "            \n",
        "        batchs = [self._data[index:index + batch_size] for index in indices]\n",
        "        return batchs\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        return self._data.iloc[index,:].to_numpy()\n",
        "    \n",
        "class DataLoader():\n",
        "    def __init__(self, dataset, batch_size = 1, shuffle = True):\n",
        "        assert isinstance(dataset, Dataset)\n",
        "        assert len(dataset) >= batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "    def __iter__(self):\n",
        "        return iter(self.dataset(self.batch_size, self.shuffle))\n",
        "    \n",
        "    "
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyu4n2zx_nXq"
      },
      "source": [
        "class RNN_RNN(nn.Module):\n",
        "    def __init__(self, embed=None):\n",
        "        super().__init__()\n",
        "        \n",
        "\n",
        "        \n",
        "        V = 400002\n",
        "        D = 50\n",
        "        H = 50\n",
        "        S = 10\n",
        "        \n",
        "        P_V = 250\n",
        "        P_D = 10\n",
        "\n",
        "        self.abs_pos_embed = nn.Embedding(P_V,P_D)\n",
        "        self.rel_pos_embed = nn.Embedding(S,P_D)\n",
        "        \n",
        "        self.embed = nn.Embedding(V,D,padding_idx=0)\n",
        "\n",
        "        if embed is not None:\n",
        "            self.embed.weight.data.copy_(embed)\n",
        "\n",
        "        self.word_RNN = nn.GRU(\n",
        "                        input_size = D,\n",
        "                        hidden_size = H,\n",
        "                        batch_first = True,\n",
        "                        bidirectional = True\n",
        "                        )\n",
        "        self.sent_RNN = nn.GRU(\n",
        "                        input_size = 2*H,\n",
        "                        hidden_size = H,\n",
        "                        batch_first = True,\n",
        "                        bidirectional = True\n",
        "                        )\n",
        "        self.fc = nn.Linear(2*H,2*H)\n",
        "\n",
        "        # Parameters of Classification Layer\n",
        "        self.content = nn.Linear(2*H,1,bias=False)\n",
        "        self.salience = nn.Bilinear(2*H,2*H,1,bias=False)\n",
        "        self.novelty = nn.Bilinear(2*H,2*H,1,bias=False)\n",
        "        \n",
        "        self.abs_pos = nn.Linear(P_D,1,bias=False)\n",
        "        \n",
        "        self.rel_pos = nn.Linear(P_D,1,bias=False)\n",
        "\n",
        "        self.bias = nn.Parameter(torch.FloatTensor(1).uniform_(-0.1,0.1))\n",
        "\n",
        "    def max_pool1d(self,x,seq_lens):\n",
        "\n",
        "        out = []\n",
        "        for index,t in enumerate(x):\n",
        "            t = t[:seq_lens[index],:]\n",
        "            t = torch.t(t).unsqueeze(0)\n",
        "            out.append(torch.max_pool1d(t,t.size(2)))\n",
        "\n",
        "        out = torch.cat(out).squeeze(2)\n",
        "        return out\n",
        "        \n",
        "    def avg_pool1d(self,x,seq_lens):\n",
        "\n",
        "        out = []\n",
        "        for index,t in enumerate(x):\n",
        "            t = t[:seq_lens[index],:]\n",
        "            t = torch.t(t).unsqueeze(0)\n",
        "            out.append(torch.avg_pool1d(t,t.size(2)))\n",
        "        \n",
        "        out = torch.cat(out).squeeze(2)\n",
        "        return out\n",
        "\n",
        "\n",
        "    def pad_doc(self,words_out,doc_lens):\n",
        "        pad_dim = words_out.size(1)\n",
        "        max_doc_len = max(doc_lens)\n",
        "        sent_input = []\n",
        "        start = 0\n",
        "        for doc_len in doc_lens:\n",
        "            stop = start + doc_len\n",
        "            valid = words_out[start:stop]                                       # (doc_len,2*H)\n",
        "            start = stop\n",
        "            if doc_len == max_doc_len:\n",
        "                sent_input.append(valid.unsqueeze(0))\n",
        "            else:\n",
        "                pad = Variable(torch.zeros(max_doc_len-doc_len,pad_dim))\n",
        "\n",
        "                pad = pad.cuda()\n",
        "                \n",
        "                sent_input.append(torch.cat([valid,pad]).unsqueeze(0))          # (1,max_len,2*H)\n",
        "                del pad\n",
        "\n",
        "        sent_input = torch.cat(sent_input,dim=0)                                # (B,max_len,2*H)\n",
        "        return sent_input\n",
        "\n",
        "    def forward(self,x,doc_lens):\n",
        "        sent_lens = torch.sum(torch.sign(x),dim=1).data\n",
        "\n",
        "        x = x.cuda()\n",
        " \n",
        "        x = self.embed(x)                                                      # (N,L,D)\n",
        "        # word level GRU\n",
        "        H = 50\n",
        "\n",
        "        x = self.word_RNN(x)[0]                                                 # (N,2*H,L)\n",
        "        \n",
        "        #word_out = self.avg_pool1d(x,sent_lens)\n",
        "        word_out = self.max_pool1d(x,sent_lens)\n",
        "\n",
        "        # make sent features(pad with zeros)\n",
        "        x = self.pad_doc(word_out,doc_lens)\n",
        "\n",
        "        sent_out = self.sent_RNN(x)[0]                                           # (B,max_doc_len,2*H)\n",
        "        #docs = self.avg_pool1d(sent_out,doc_lens)                               # (B,2*H)\n",
        "\n",
        "        docs = self.max_pool1d(sent_out,doc_lens)                                # (B,2*H)\n",
        "        \n",
        "        probs = []\n",
        "        for index,doc_len in enumerate(doc_lens):\n",
        "            valid_hidden = sent_out[index,:doc_len,:]                            # (doc_len,2*H)\n",
        "            doc = torch.tanh(self.fc(docs[index])).unsqueeze(0)\n",
        "            s = Variable(torch.zeros(1,2*H))\n",
        "            \n",
        "\n",
        "            s = s.cuda()\n",
        "\n",
        "            for position, h in enumerate(valid_hidden):\n",
        "                h = h.view(1, -1)                                                # (1,2*H)\n",
        "\n",
        "                abs_index = Variable(torch.LongTensor([[position]]))\n",
        "\n",
        "                abs_index = abs_index.cuda()\n",
        "                \n",
        "                abs_features = self.abs_pos_embed(abs_index).squeeze(0)\n",
        "                \n",
        "                rel_index = int(round((position + 1) * 9.0 / doc_len))\n",
        "                rel_index = Variable(torch.LongTensor([[rel_index]]))\n",
        "\n",
        "                rel_index = rel_index.cuda()\n",
        "                \n",
        "                rel_features = self.rel_pos_embed(rel_index).squeeze(0)\n",
        "                \n",
        "                # classification layer\n",
        "                content = self.content(h) \n",
        "                salience = self.salience(h,doc)\n",
        "                novelty = -1 * self.novelty(h,torch.tanh(s))\n",
        "                abs_p = self.abs_pos(abs_features)\n",
        "                rel_p = self.rel_pos(rel_features)\n",
        "                prob = torch.sigmoid(content + salience + novelty + abs_p + rel_p + self.bias)# \n",
        "                \n",
        "                s = s + torch.mm(prob,h)\n",
        "                probs.append(prob)\n",
        "        return torch.cat(probs).squeeze()\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7sY4o7pdWon"
      },
      "source": [
        "import copy\n",
        "class Batch:\n",
        "  def __init__(self,batch,word2id):\n",
        "    self.batch=copy.deepcopy(batch.to_numpy())\n",
        "    self.word2id=word2id\n",
        "\n",
        "  def preprocess(self):\n",
        "    idx=0\n",
        "    doc_lens = []\n",
        "    features = []\n",
        "    targets = []\n",
        "\n",
        "    sent_list = []\n",
        "    for doc in self.batch[:,0]:\n",
        "        sent_list += doc\n",
        "        doc_lens.append(len(doc))\n",
        "        targets += list(self.batch[idx,1])\n",
        "        idx+=1\n",
        "\n",
        "    maxlen=0\n",
        "    for sentence in sent_list:\n",
        "      s=sentence.split()\n",
        "      maxlen=max(maxlen,len(s))\n",
        "\n",
        "    for sentence in sent_list:\n",
        "      words = sentence.strip().split()\n",
        "      sentence = [self.word2id[word] if word in self.word2id else 1 for word in words]\n",
        "      sentence += [0 for _ in range(maxlen - len(sentence))]\n",
        "      features.append(sentence)\n",
        "\n",
        "\n",
        "    features = torch.LongTensor(features)    \n",
        "    targets = torch.LongTensor(targets)\n",
        "    \n",
        "    return features,targets,doc_lens\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRFSS6eOKV5L"
      },
      "source": [
        "def eval(net,data_iter,criterion):\n",
        "    net.eval()\n",
        "    total_loss = 0\n",
        "    batch_num = 0\n",
        "    for batch in data_iter:\n",
        "        features,targets,doc_lens = Batch(batch,word2id).preprocess()\n",
        "        features,targets = Variable(features), Variable(targets.float())\n",
        "  \n",
        "        targets = targets.cuda()\n",
        "        \n",
        "        probs = net(features,doc_lens)\n",
        "        loss = criterion(probs,targets)\n",
        "        \n",
        "        del features\n",
        "        del targets\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "        total_loss += loss.cpu().data\n",
        "        batch_num += 1\n",
        "    loss = total_loss / batch_num\n",
        "    net.train()\n",
        "    return loss"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cG1sW-mEqyjZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efc5e255-a5ab-47bc-dac2-bfcb8d3b7bf0"
      },
      "source": [
        "# del net\n",
        "# torch.cuda.empty_cache()\n",
        "with torch.no_grad():\n",
        "  net = RNN_RNN(weights)\n",
        "  criterion = nn.BCELoss()  \n",
        "  optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
        "  loss_sum = 0\n",
        "  min_loss = float('Inf')\n",
        "\n",
        "net.cuda()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN_RNN(\n",
              "  (abs_pos_embed): Embedding(250, 10)\n",
              "  (rel_pos_embed): Embedding(10, 10)\n",
              "  (embed): Embedding(400002, 50, padding_idx=0)\n",
              "  (word_RNN): GRU(50, 50, batch_first=True, bidirectional=True)\n",
              "  (sent_RNN): GRU(100, 50, batch_first=True, bidirectional=True)\n",
              "  (fc): Linear(in_features=100, out_features=100, bias=True)\n",
              "  (content): Linear(in_features=100, out_features=1, bias=False)\n",
              "  (salience): Bilinear(in1_features=100, in2_features=100, out_features=1, bias=False)\n",
              "  (novelty): Bilinear(in1_features=100, in2_features=100, out_features=1, bias=False)\n",
              "  (abs_pos): Linear(in_features=10, out_features=1, bias=False)\n",
              "  (rel_pos): Linear(in_features=10, out_features=1, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZorpX9DWwPG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7964daf7-e237-463b-dd08-96aa3fd6d835"
      },
      "source": [
        "train_loader = DataLoader(Dataset(train_df),batch_size=16)\n",
        "validation_loader=DataLoader(Dataset(validation_df),shuffle=False)\n",
        "\n",
        " \n",
        "counter=0\n",
        "debug=True\n",
        "net.train()\n",
        "for epoch in range(5):\n",
        "    t1 = time() \n",
        "    for step, docs in enumerate(train_loader):\n",
        "        features,targets,doc_lens = Batch(docs,word2id).preprocess()\n",
        "\n",
        "        features,targets = Variable(features), Variable(targets.float())\n",
        "        # features = features.cuda()\n",
        "        targets = targets.cuda()\n",
        "\n",
        "        probs = net(features,doc_lens)\n",
        "        loss = criterion(probs,targets)\n",
        "\n",
        "        # del features\n",
        "        del targets\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        loss.backward()\n",
        "        # print(loss.data.size())\n",
        "        #print(loss.cpu().data)\n",
        "\n",
        "        clip_grad_norm_(net.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        \n",
        "        # if debug:\n",
        "        #       print('Batch ID:%d Loss:%f' %(step,loss.cpu().data))\n",
        "              # continue\n",
        "        if (step+1) % 75 == 0:\n",
        "              cur_loss = eval(net,validation_loader,criterion)\n",
        "              if cur_loss < min_loss:\n",
        "                  min_loss = cur_loss\n",
        "                  print('Min Loss:%f' %(min_loss))\n",
        "                  torch.save(net.state_dict(), 'SummaRuNNer.pt')\n",
        "    \n",
        "    cur_loss = eval(net,validation_loader,criterion)\n",
        "    if cur_loss < min_loss:\n",
        "      min_loss = cur_loss\n",
        "      print('Min Loss:%f' %(min_loss))\n",
        "      torch.save(net.state_dict(), 'SummaRuNNer.pt')\n",
        "    \n",
        "    t2 = time()\n",
        "    print(\"Time Taken for each epoch:- \",t2-t1) "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Min Loss:0.377916\n",
            "Min Loss:0.365657\n",
            "Min Loss:0.353974\n",
            "Time Taken for each epoch:-  386.7997417449951\n",
            "Min Loss:0.346797\n",
            "Min Loss:0.340952\n",
            "Min Loss:0.340456\n",
            "Time Taken for each epoch:-  378.17206144332886\n",
            "Min Loss:0.335843\n",
            "Min Loss:0.334676\n",
            "Min Loss:0.332131\n",
            "Time Taken for each epoch:-  380.823548078537\n",
            "Min Loss:0.330531\n",
            "Time Taken for each epoch:-  379.35919308662415\n",
            "Min Loss:0.329459\n",
            "Min Loss:0.329010\n",
            "Min Loss:0.327670\n",
            "Time Taken for each epoch:-  388.20346188545227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wegyQ8Npq84v"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0z92qC363fKW",
        "outputId": "0349bb5e-b6cf-4c83-916c-82cef31c449b"
      },
      "source": [
        "# net.load_state_dict(torch.load('/content/gdrive/My Drive/IRE_PROJECT/SummaRuNNer.pt'))\n",
        "net.eval()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN_RNN(\n",
              "  (abs_pos_embed): Embedding(250, 10)\n",
              "  (rel_pos_embed): Embedding(10, 10)\n",
              "  (embed): Embedding(400002, 50, padding_idx=0)\n",
              "  (word_RNN): GRU(50, 50, batch_first=True, bidirectional=True)\n",
              "  (sent_RNN): GRU(100, 50, batch_first=True, bidirectional=True)\n",
              "  (fc): Linear(in_features=100, out_features=100, bias=True)\n",
              "  (content): Linear(in_features=100, out_features=1, bias=False)\n",
              "  (salience): Bilinear(in1_features=100, in2_features=100, out_features=1, bias=False)\n",
              "  (novelty): Bilinear(in1_features=100, in2_features=100, out_features=1, bias=False)\n",
              "  (abs_pos): Linear(in_features=10, out_features=1, bias=False)\n",
              "  (rel_pos): Linear(in_features=10, out_features=1, bias=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC07Ka9VOUpY"
      },
      "source": [
        "test_loader=DataLoader(Dataset(test_df),shuffle=False)\n",
        "\n",
        "actual = []\n",
        "predicted = []\n",
        "for batch in test_loader:\n",
        "        features,targets,doc_lens = Batch(batch,word2id).preprocess()\n",
        "        #features,targets,_,doc_lens = vocab.make_features(batch)\n",
        "        features,targets = Variable(features), Variable(targets.float())\n",
        "        # if use_gpu:\n",
        "        # features = features.cuda()\n",
        "        \n",
        "        \n",
        "        probs = net(features,doc_lens)\n",
        "        # print(probs,targets)\n",
        "\n",
        "        sentences = \"\"\n",
        "\n",
        "        for indices in probs.topk(min(doc_lens[0],5))[1].cpu().data:\n",
        "          sentences += (batch['story'].to_numpy()[0][indices])+'\\n'\n",
        "        \n",
        "        predicted.append(sentences[:-1])\n",
        "\n",
        "\n",
        "        actual.append('\\n'.join(batch['summary'].to_numpy()[0]))\n",
        "        del features\n",
        "        del targets\n",
        "        torch.cuda.empty_cache()\n",
        "        # break"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QCuGybweCJQ",
        "outputId": "89beaf3f-444c-42b0-f048-f1e5da49f994"
      },
      "source": [
        "!pip install py-rouge"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting py-rouge\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/1d/0bdbaf559fb7afe32308ebc84a2028600988212d7eb7fb9f69c4e829e4a0/py_rouge-1.1-py3-none-any.whl (56kB)\n",
            "\r\u001b[K     |█████▊                          | 10kB 25.0MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 20kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 30kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 40kB 8.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 51kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.9MB/s \n",
            "\u001b[?25hInstalling collected packages: py-rouge\n",
            "Successfully installed py-rouge-1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_A6d-z1erNd",
        "outputId": "f258cc5d-0ed3-44cb-f16a-e1ba689179d3"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2L208dIkeIR4",
        "outputId": "4161eb63-fdbe-4734-e955-2384b258dcb8"
      },
      "source": [
        "import rouge\n",
        "\n",
        "\n",
        "def prepare_results(p, r, f):\n",
        "    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)\n",
        "\n",
        "\n",
        "for aggregator in ['Avg','Best']:\n",
        "    print('Evaluation with {}'.format(aggregator))\n",
        "    apply_avg = aggregator == 'Avg'\n",
        "    apply_best = aggregator == 'Best'\n",
        "\n",
        "    evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
        "                           max_n=4,\n",
        "                           limit_length=True,\n",
        "                           length_limit=100,\n",
        "                           length_limit_type='words',\n",
        "                           apply_avg=apply_avg,\n",
        "                           apply_best=apply_best,\n",
        "                           alpha=0.5, # Default F1_score\n",
        "                           weight_factor=1.2,\n",
        "                           stemming=True)\n",
        "    \n",
        "    scores = evaluator.get_scores(predicted, actual)\n",
        "\n",
        "    for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
        "            if not apply_avg and not apply_best: # value is a type of list as we evaluate each summary vs each reference\n",
        "                for hypothesis_id, results_per_ref in enumerate(results):\n",
        "                    nb_references = len(results_per_ref['p'])\n",
        "                    for reference_id in range(nb_references):\n",
        "                        print('\\tHypothesis #{} & Reference #{}: '.format(hypothesis_id, reference_id))\n",
        "                        print('\\t' + prepare_results(results_per_ref['p'][reference_id], results_per_ref['r'][reference_id], results_per_ref['f'][reference_id]))\n",
        "                print()\n",
        "            else:\n",
        "                print(prepare_results(results['p'], results['r'], results['f']))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluation with Avg\n",
            "\trouge-1:\tP: 23.90\tR: 49.85\tF1: 31.95\n",
            "\trouge-2:\tP:  8.76\tR: 18.73\tF1: 11.79\n",
            "\trouge-3:\tP:  4.54\tR:  9.86\tF1:  6.13\n",
            "\trouge-4:\tP:  2.74\tR:  6.01\tF1:  3.71\n",
            "\trouge-l:\tP: 27.68\tR: 51.19\tF1: 35.63\n",
            "\trouge-w:\tP: 15.84\tR: 20.28\tF1: 17.50\n",
            "Evaluation with Best\n",
            "\trouge-1:\tP: 23.90\tR: 49.85\tF1: 31.95\n",
            "\trouge-2:\tP:  8.76\tR: 18.73\tF1: 11.79\n",
            "\trouge-3:\tP:  4.54\tR:  9.86\tF1:  6.13\n",
            "\trouge-4:\tP:  2.74\tR:  6.01\tF1:  3.71\n",
            "\trouge-l:\tP: 27.68\tR: 51.19\tF1: 35.63\n",
            "\trouge-w:\tP: 15.84\tR: 20.28\tF1: 17.50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNFI6ZMyq9hX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58c60dd8-aba5-4281-9910-49b8dd09dc9c"
      },
      "source": [
        "for docs in range(10):\n",
        "  print(\"Predicted Summary:-\",''.join(predicted[docs].split('.')))\n",
        "  print()\n",
        "  \n",
        "  print(\"Actual Summary:-\",''.join(actual[docs].split('.')))\n",
        "  print('\\n\\n')\n"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Summary:- nearly 16 years after a fuel tank explosion destroyed twa flight 800 , killing all 230 aboard , the federal aviation administration on friday proposed to fine boeing co $ 1357 million for failing to meet a deadline intended to prevent similar catastrophes \n",
            "the faa said boeing failed to meet a 2010 deadline to give airlines information on how to reduce fuel tank flammability , missing the deadline by 301 days for its b-747 aircraft , and by 406 days for its b-757 planes \n",
            "some 383 boeing aircraft in the united states are affected by the delays , it said \n",
            "because of the missed deadline , airlines have asked the faa for extensions to make necessary fixes , the faa said \n",
            "the july 17 , 1996 , explosion of twa 800 , a boeing 747 , was one of the deadliest accidents in aviation history , and was among the most difficult to solve \n",
            "\n",
            "Actual Summary:- the faa says boeing has failed to meet a deadline to prevent similar crashes as twa flight 800 \n",
            "the flight crashed in july 1996 after an explosion in the central fuel tank \n",
            "boeing has n't yet given airlines information on how to reduce fuel tank flammability , faa says \n",
            "\n",
            "\n",
            "\n",
            "Predicted Summary:- cockiness and swagger serve him on the golf course , but there 's much more to golf 's young phenom , rory mcilroy \n",
            "the us open winner might hang with tennis great rafael nadal , knock back some heinekens or slip the electronic dance sounds of swedish house mafia into his ipod \n",
            "such tidbits are rolling out these days from the 22-year-old hero of holywood , northern ireland , who won the tournament by eight strokes and instantly drew comparisons to tiger woods \n",
            "while the young mcilroy has one major championship , woods has 14 , including three us open wins \n",
            "woods , 35 , has had no pga wins since taking a brief leave from the game in late 2009 after admitting infidelity \n",
            "\n",
            "Actual Summary:- northern ireland native won us open by eight strokes \n",
            "rory mcilroy says he is playing for history , not the money \n",
            "he tells \" piers morgan tonight \" he has swagger on the golf course \n",
            "\n",
            "\n",
            "\n",
            "Predicted Summary:- lt gen michael flynn , the head of the defense intelligence agency , made that clear when he told national public radio in an interview broadcast friday how us officials must plan for the possibility that vladimir putin 's russia has access to american battle plans and other secrets possibly taken by classified leaker edward snowden \n",
            "\" if i 'm concerned about anything , i 'm concerned about defense capabilities that he may have stolen from where he worked , and does that knowledge then get into the hands of our adversaries -- in this case , of course , russia , \" flynn said of the former national security agency contractor who fled to moscow to seek asylum \n",
            "in the world of military strategy , every contingency must be examined , especially the worst-case scenario \n",
            "a hero to some and traitor to others , snowden last year disclosed details of the vast us surveillance network put in place after the september 11 , 2001 , terrorist attacks , including how the government keeps records on billions of phone calls for possible use in terrorism investigations \n",
            "flynn told the panel that \" the greatest cost that is unknown today but that we will likely face is the cost of human lives on tomorrow 's battlefield or in some place where we will put our military forces when we ask them to go into harm 's way  \"\n",
            "\n",
            "Actual Summary:- military strategy requires examining worst-case scenarios \n",
            "lt gen flynn says classified leaker edward snowden could have military plans \n",
            "flynn : if russia does n't have snowden 's information yet , it 's trying to get it \n",
            "snowden is living in russia while he seeks political asylum \n",
            "\n",
            "\n",
            "\n",
            "Predicted Summary:- vick , who spent about 18 months in federal prison after pleading guilty to dogfighting charges in 2007 , said the donation is a gift to the city of brotherly love for embracing him after the eagles signed him following his release from prison \n",
            "at the sound of his coach 's whistle , the 12-year-old charges a tee , kicking a football and dirt into the setting sun \n",
            "but this week , the aztecs -- the first inner-city youth football team to win a pop warner championship -- got word they 're getting a new field , compliments of philadelphia eagles ' quarterback michael vick \n",
            "on a warm , late summer evening , nas scott readies himself atop patchy grass and waits \n",
            "the aztecs practice on a field at the center of hunting park in the heart of north philadelphia \n",
            "\n",
            "Actual Summary:- the home field of the north philadelphia aztecs is in disrepair \n",
            "the aztecs are the first inner-city youth team to win pop warner championship \n",
            "michael vick 's nonprofit has donated $ 200,000 to build the team a new field \n",
            "other athletes helping city parks program are ryan howard , billie jean king \n",
            "\n",
            "\n",
            "\n",
            "Predicted Summary:- an interactive map showing the names and addresses of all handgun permit holders in new york 's westchester and rockland counties has infuriated many readers since it was posted saturday on a newspaper 's website \n",
            "the map , published by the journal news , allows readers to zoom in on red dots that indicate which residents are licensed to own pistols or revolvers \n",
            "blue dots indicate permit holders who \" have purchased a firearm or updated the information on a permit in the past five years  \"\n",
            "in an article about the uproar , the journal news says many of the thousands of people who \" have taken to their computers and phones in rage \" live outside the counties covered by the map \n",
            "the map came about in the wake of the massacre in newtown , connecticut , the journal news said \n",
            "\n",
            "Actual Summary:- the publisher issues a statement supporting the decision \n",
            "other news agencies have published similar databases in the past \n",
            "poynter institute senior faculty member calls newspaper 's move \" journalist arrogance \"\n",
            "\" readers are understandably interested \" in gun data , newspaper editor says \n",
            "\n",
            "\n",
            "\n",
            "Predicted Summary:- islamabad , pakistan ( cnn ) -- at least 43 people were killed and 72 others were injured as widespread violence broke out overnight following the killing of a provincial lawmaker in the southern pakistani port city of karachi tuesday , police said \n",
            "syed raza haider , a leader of the mqm party , was shot monday evening at a mosque where he was attending the funeral of a relative , according to rafiq gul , karachi 's deputy superintendent of police \n",
            "the mqm is part of the ruling coalition backing president asif ali zardari 's pakistan people 's party \n",
            "gul said haider 's death triggered political and ethnic violence in the city , as mobs set fire to vehicles and gunfire erupted \n",
            "gul said 48 vehicles , eight shops and several gas stations were set ablaze in the mayhem \n",
            "\n",
            "Actual Summary:- a provincial politician was attending a funeral when he was killed \n",
            "his death sparked political and ethnic violence across karachi \n",
            "\n",
            "\n",
            "\n",
            "Predicted Summary:- -- zambian president michael sata , who was nicknamed \" king cobra \" for his fiery comebacks and larger-than-life personality , has died \n",
            "sata took office in september 2011 after the incumbent president tearfully conceded in a televised speech , a rarity in a continent known for volatile elections and leaders fighting their defeat tooth-and-nail \n",
            "sata had traveled to london for unspecified medical treatment last week , and died at a hospital there tuesday evening \n",
            "he is ineligible to be elected president because his parents were not born in zambia , msiska said \n",
            "scott , who is of scottish descent , will be the first white president in sub-saharan africa since apartheid \n",
            "\n",
            "Actual Summary:- new : vice president guy scott will take over until elections , cabinet secretary says \n",
            "sata took office in 2011 after the incumbent president tearfully conceded \n",
            "he was nicknamed \" king cobra \" because of his fiery tongue \n",
            "speculation over his health has intensified since he took office \n",
            "\n",
            "\n",
            "\n",
            "Predicted Summary:- thailand 's general election on sunday pitted the democrats , led by prime minister abhisit vejjajiva , against the opposition pheu thai party , led by yingluck shinawatra , youngest sister of thaksin shinawatra , who was ousted in a bloodless coup in 2006 \n",
            "abhisit on sunday conceded the election , making yingluck the country 's first female premier \n",
            "march 9 - thaksin begins second term as pm after he and the trt win landslide victories in the february elections \n",
            "january 6 - thaksin 's thai rak thai ( trt ) party wins in general election \n",
            "february 24 - thaksin dissolves parliament , calls for snap elections on april 2 amid protests and mounting criticism over his family 's sale of shares in shin corp \n",
            "\n",
            "Actual Summary:- vote on sunday is first general election since 2007 \n",
            "since then two prime ministers of the people 's power party were removed , party banned \n",
            "parliamentary vote in 2008 put democrat party leader abhisit vejjajiva in office \n",
            "he faces yingluck shinawatra , sister of ousted prime minister thaksin shinawatra \n",
            "\n",
            "\n",
            "\n",
            "Predicted Summary:- smoking is also forbidden at bars and restaurants within the olympic park -- a step ahead of the nation \n",
            "outdoor events such as skiing and bobsleighing are being hosted within the krasnaya polyana resort area , about 50 kilometers away in the western caucasus mountains \n",
            "this will be a trial in a country still in love with smoking -- nearly 60 % of adult males and 40 % of the total adult population admitted to smoking regularly in 2012 , according to the world health organization \n",
            "as one of the warmest cities in russia -- a country with no shortage of snowy terrain -- sochi seems an unusual place to host the winter olympics \n",
            "it will also impose a minimum price ( so long $ 2 packs ) , all in an effort to quell smoking-related deaths in russia , which totaled 400,000 in 2012 \n",
            "\n",
            "Actual Summary:- sochi has gay bars -- despite putin \n",
            "cossacks are on street patrol \n",
            "you might find chacha in your vodka glass -- a bit like grappa \n",
            "cigarette-loving russians have to butt out for the games \n",
            "\n",
            "\n",
            "\n",
            "Predicted Summary:- the opinions expressed in this commentary are solely those of lz granderson \n",
            "i hope she can at last have the peace that seemed to avoid her over the last 15 years of her life : the marriage to bobby brown , the reality show , the erratic interviews and appearances , and the heartbreaking live performances that served only to remind us that her voice , the voice , was gone , and whitney was lost \n",
            "los angeles ( cnn ) -- i had just pulled up in front of my hotel in los angeles when i heard whitney houston had died at the beverly hilton , just a few blocks from where i was staying \n",
            "that 's why if you did n't get goosebumps during jennifer hudson 's grammy tribute on sunday , you may not be fully human \n",
            "later that night , a large crowd of people hovered outside the site of her passing , some holding lit candles , others holding one another \n",
            "\n",
            "Actual Summary:- we do n't know why whitney houston died ; toxicology report due in a few weeks \n",
            "lz granderson : her struggles with addiction over the past years were well-known \n",
            "but nobody matched her voice , he writes , and nobody can sing a whitney song \n",
            "lz says houston 's tragedy is a prime example of money not buying happiness \n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}