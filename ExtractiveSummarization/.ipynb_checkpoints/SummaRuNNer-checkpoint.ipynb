{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHiloSDrTlZc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext.data import Field\n",
    "from torchtext.data import BucketIterator\n",
    "from torchtext.vocab import GloVe\n",
    "# from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from time import time\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Parameter\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6LXF_u9eTRyF",
    "outputId": "aaa0b766-67d4-4011-8fe1-be05af5ef1ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zPU0rzacTSPL"
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "model = models.KeyedVectors.load_word2vec_format('/content/gdrive/My Drive/IRE_PROJECT/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hS9xB6I4Ve1x",
    "outputId": "8120aca3-a935-4f16-9561-a16411e3dd35"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".vector_cache/glove.6B.zip: 862MB [06:27, 2.22MB/s]                           \n",
      "100%|█████████▉| 399660/400000 [00:15<00:00, 23471.40it/s]"
     ]
    }
   ],
   "source": [
    "m = GloVe(name='6B', dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MtVfBAy9Tr9i"
   },
   "outputs": [],
   "source": [
    "max_sent = -1\n",
    "train_dataset={'story':[],'labels':[]}\n",
    "for line in open('/content/gdrive/My Drive/IRE_PROJECT/train_data.json'):\n",
    "  data=json.loads(line)\n",
    "  max_sent = max(max_sent,len(data['story']))\n",
    "  # print(data['story'])\n",
    "  # print(data['labels'])\n",
    "  train_dataset['story'].append(data['story'])\n",
    "  train_dataset['labels'].append(data['labels'])\n",
    "# train=json.loads(pathlib.Path('/content/train_data.json').read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjE5v_XWwc7Q"
   },
   "outputs": [],
   "source": [
    "validation_dataset={'story':[],'labels':[]}\n",
    "for line in open('/content/gdrive/My Drive/IRE_PROJECT/validation_data.json'):\n",
    "  data=json.loads(line)\n",
    "  # print(data['story'])\n",
    "  # print(data['labels'])\n",
    "  max_sent = max(max_sent,len(data['story']))\n",
    "  validation_dataset['story'].append(data['story'])\n",
    "  validation_dataset['labels'].append(data['labels'])\n",
    "# train=json.loads(pathlib.Path('/content/train_data.json').read_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oATQGnfc37LT",
    "outputId": "d911944d-984d-4d90-d075-a1739328dddc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwQXy5EqTvA1"
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_dataset, columns=[\"story\", \"labels\"])\n",
    "validation_df=pd.DataFrame(validation_dataset, columns=[\"story\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "tjRqssoIT9lY",
    "outputId": "d15f086f-fb25-436e-db6f-8f1a5518ef28"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Paris ( CNN ) -- Flamboyant fashion designer ...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[( CNN ), -- I spent much of Sunday in touch w...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[New York ( CNN ) --, The head of the Internat...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[( CNN ) --, All the self-righteous huffing an...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Los Angeles ( CNN ) --, Tour manager Paul Gon...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3235</th>\n",
       "      <td>[( CNN ), -- South Korea will attempt a second...</td>\n",
       "      <td>[0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3236</th>\n",
       "      <td>[( Mental Floss ) --, It 's hard to walk down ...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3237</th>\n",
       "      <td>[Yangon , Myanmar ( CNN ) -- Myanmar oppositio...</td>\n",
       "      <td>[1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3238</th>\n",
       "      <td>[( CNN ) -- Not only is Sheikha Lubna Al Qasim...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3239</th>\n",
       "      <td>[Rio de Janeiro ( CNN ) --, A massive crowd of...</td>\n",
       "      <td>[0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3240 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  story                                             labels\n",
       "0     [Paris ( CNN ) -- Flamboyant fashion designer ...  [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...\n",
       "1     [( CNN ), -- I spent much of Sunday in touch w...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "2     [New York ( CNN ) --, The head of the Internat...  [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...\n",
       "3     [( CNN ) --, All the self-righteous huffing an...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, ...\n",
       "4     [Los Angeles ( CNN ) --, Tour manager Paul Gon...  [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "...                                                 ...                                                ...\n",
       "3235  [( CNN ), -- South Korea will attempt a second...  [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, ...\n",
       "3236  [( Mental Floss ) --, It 's hard to walk down ...  [0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...\n",
       "3237  [Yangon , Myanmar ( CNN ) -- Myanmar oppositio...  [1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...\n",
       "3238  [( CNN ) -- Not only is Sheikha Lubna Al Qasim...  [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...\n",
       "3239  [Rio de Janeiro ( CNN ) --, A massive crowd of...  [0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "\n",
       "[3240 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "VxwWx_mOyam0",
    "outputId": "f50779ef-f793-4e6d-edb7-ae07dabcf2d3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[( CNN ) --, Sporting a black t-shirt proudly ...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[WASHINGTON ( CNN ) -- How does the American p...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[( CNN ) --, Basketball star Dennis Rodman see...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[( CNN ), -- Check with your airline before yo...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[RIYADH , Saudi Arabia --, Ahmad al Shayea is ...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>[( CNN ) --, The World Health Organization rai...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>[Dali , Guizhou , China ( CNN ) -- Shi Wenchan...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>[Cairo , Egypt ( CNN ) -- One man died and ano...</td>\n",
       "      <td>[1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>[London ( CNN ) -- The cleanup operation conti...</td>\n",
       "      <td>[1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>[Washington ( CNN ) -- What a difference ., Ba...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>360 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 story                                             labels\n",
       "0    [( CNN ) --, Sporting a black t-shirt proudly ...  [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...\n",
       "1    [WASHINGTON ( CNN ) -- How does the American p...  [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, ...\n",
       "2    [( CNN ) --, Basketball star Dennis Rodman see...  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, ...\n",
       "3    [( CNN ), -- Check with your airline before yo...  [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...\n",
       "4    [RIYADH , Saudi Arabia --, Ahmad al Shayea is ...  [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...\n",
       "..                                                 ...                                                ...\n",
       "355  [( CNN ) --, The World Health Organization rai...  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...\n",
       "356  [Dali , Guizhou , China ( CNN ) -- Shi Wenchan...  [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...\n",
       "357  [Cairo , Egypt ( CNN ) -- One man died and ano...  [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, ...\n",
       "358  [London ( CNN ) -- The cleanup operation conti...  [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...\n",
       "359  [Washington ( CNN ) -- What a difference ., Ba...  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, ...\n",
       "\n",
       "[360 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ne0kfTYTxGg"
   },
   "outputs": [],
   "source": [
    "weights = model.vectors\n",
    "weights = np.insert(weights, 0,np.zeros(300),axis = 0)\n",
    "weights = np.insert(weights, 0,np.zeros(300),axis = 0)\n",
    "weights = torch.FloatTensor(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09N-JpHvUABM"
   },
   "outputs": [],
   "source": [
    "word2id = {model.index2word[i]:i+2 for i in range(len(model.index2word))}\n",
    "word2id['<pad>'] = 0\n",
    "word2id['<unk>'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjejcmkdWtMa"
   },
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    def __init__(self, data_list):\n",
    "        self._data = data_list\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "        \n",
    "    def __call__(self, batch_size, shuffle = True):\n",
    "        max_len = len(self)\n",
    "        \n",
    "        indices = [i for i in range(0,max_len,batch_size)]\n",
    "        \n",
    "        if shuffle:\n",
    "            indices = np.random.randint(0,max_len-1,math.ceil(max_len/batch_size))\n",
    "            \n",
    "        batchs = [self._data[index:index + batch_size] for index in indices]\n",
    "        return batchs\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self._data.iloc[index,:].to_numpy()\n",
    "    \n",
    "class DataLoader():\n",
    "    def __init__(self, dataset, batch_size = 1, shuffle = True):\n",
    "        assert isinstance(dataset, Dataset)\n",
    "        assert len(dataset) >= batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "    def __iter__(self):\n",
    "        return iter(self.dataset(self.batch_size, self.shuffle))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lyu4n2zx_nXq"
   },
   "outputs": [],
   "source": [
    "class RNN_RNN(nn.Module):\n",
    "    def __init__(self, embed=None):\n",
    "        super(RNN_RNN, self).__init__()\n",
    "        self.model_name = 'RNN_RNN'\n",
    "        # self.args = args\n",
    "        \n",
    "        V = 3000002\n",
    "        D = 300\n",
    "        H = 50\n",
    "        S = 10\n",
    "        \n",
    "        P_V = 250\n",
    "        P_D = 10\n",
    "\n",
    "        self.abs_pos_embed = nn.Embedding(P_V,P_D)\n",
    "        self.rel_pos_embed = nn.Embedding(S,P_D)\n",
    "        \n",
    "        self.embed = nn.Embedding(V,D,padding_idx=0)\n",
    "\n",
    "        if embed is not None:\n",
    "            self.embed.weight.data.copy_(embed)\n",
    "\n",
    "        self.word_RNN = nn.GRU(\n",
    "                        input_size = D,\n",
    "                        hidden_size = H,\n",
    "                        batch_first = True,\n",
    "                        bidirectional = True\n",
    "                        )\n",
    "        self.sent_RNN = nn.GRU(\n",
    "                        input_size = 2*H,\n",
    "                        hidden_size = H,\n",
    "                        batch_first = True,\n",
    "                        bidirectional = True\n",
    "                        )\n",
    "        self.fc = nn.Linear(2*H,2*H)\n",
    "\n",
    "        # Parameters of Classification Layer\n",
    "        self.content = nn.Linear(2*H,1,bias=False)\n",
    "        self.salience = nn.Bilinear(2*H,2*H,1,bias=False)\n",
    "        self.novelty = nn.Bilinear(2*H,2*H,1,bias=False)\n",
    "        \n",
    "        self.abs_pos = nn.Linear(P_D,1,bias=False)\n",
    "        \n",
    "        self.rel_pos = nn.Linear(P_D,1,bias=False)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.FloatTensor(1).uniform_(-0.1,0.1))\n",
    "\n",
    "    def max_pool1d(self,x,seq_lens):\n",
    "        # x:[N,L,O_in]\n",
    "        out = []\n",
    "        for index,t in enumerate(x):\n",
    "            #print(index)\n",
    "            t = t[:seq_lens[index],:]\n",
    "            t = torch.t(t).unsqueeze(0)\n",
    "            out.append(torch.max_pool1d(t,t.size(2)))\n",
    "            # print(out[-1])\n",
    "        out = torch.cat(out).squeeze(2)\n",
    "        return out\n",
    "        \n",
    "    def avg_pool1d(self,x,seq_lens):\n",
    "        # x:[N,L,O_in]\n",
    "        out = []\n",
    "        for index,t in enumerate(x):\n",
    "            t = t[:seq_lens[index],:]\n",
    "            t = torch.t(t).unsqueeze(0)\n",
    "            out.append(torch.avg_pool1d(t,t.size(2)))\n",
    "        \n",
    "        out = torch.cat(out).squeeze(2)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def pad_doc(self,words_out,doc_lens):\n",
    "        pad_dim = words_out.size(1)\n",
    "        max_doc_len = max(doc_lens)\n",
    "        sent_input = []\n",
    "        start = 0\n",
    "        for doc_len in doc_lens:\n",
    "            stop = start + doc_len\n",
    "            valid = words_out[start:stop]                                       # (doc_len,2*H)\n",
    "            start = stop\n",
    "            if doc_len == max_doc_len:\n",
    "                sent_input.append(valid.unsqueeze(0))\n",
    "            else:\n",
    "                pad = Variable(torch.zeros(max_doc_len-doc_len,pad_dim))\n",
    "                \n",
    "                #if self.args.device is not None:\n",
    "                pad = pad.cuda()\n",
    "                \n",
    "                sent_input.append(torch.cat([valid,pad]).unsqueeze(0))          # (1,max_len,2*H)\n",
    "                del pad\n",
    "\n",
    "        sent_input = torch.cat(sent_input,dim=0)                                # (B,max_len,2*H)\n",
    "        return sent_input\n",
    "\n",
    "    def forward(self,x,doc_lens):\n",
    "        sent_lens = torch.sum(torch.sign(x),dim=1).data\n",
    "\n",
    "        x = x.cuda()\n",
    "        # targets = targets.cuda() \n",
    "        x = self.embed(x)                                                      # (N,L,D)\n",
    "        # word level GRU\n",
    "        H = 50\n",
    "\n",
    "        x = self.word_RNN(x)[0]                                                 # (N,2*H,L)\n",
    "        \n",
    "        #word_out = self.avg_pool1d(x,sent_lens)\n",
    "        word_out = self.max_pool1d(x,sent_lens)\n",
    "\n",
    "        # make sent features(pad with zeros)\n",
    "        x = self.pad_doc(word_out,doc_lens)\n",
    "    \n",
    "        # sent level GRU\n",
    "        sent_out = self.sent_RNN(x)[0]                                           # (B,max_doc_len,2*H)\n",
    "        #docs = self.avg_pool1d(sent_out,doc_lens)                               # (B,2*H)\n",
    "\n",
    "        docs = self.max_pool1d(sent_out,doc_lens)                                # (B,2*H)\n",
    "        \n",
    "        probs = []\n",
    "        for index,doc_len in enumerate(doc_lens):\n",
    "            valid_hidden = sent_out[index,:doc_len,:]                            # (doc_len,2*H)\n",
    "            doc = torch.tanh(self.fc(docs[index])).unsqueeze(0)\n",
    "            s = Variable(torch.zeros(1,2*H))\n",
    "            \n",
    "            # if self.args.device is not None:\n",
    "            s = s.cuda()\n",
    "\n",
    "            for position, h in enumerate(valid_hidden):\n",
    "                h = h.view(1, -1)                                                # (1,2*H)\n",
    "                # get position embeddings\n",
    "                abs_index = Variable(torch.LongTensor([[position]]))\n",
    "                \n",
    "                # if self.args.device is not None:\n",
    "                abs_index = abs_index.cuda()\n",
    "                \n",
    "                abs_features = self.abs_pos_embed(abs_index).squeeze(0)\n",
    "                \n",
    "                rel_index = int(round((position + 1) * 9.0 / doc_len))\n",
    "                rel_index = Variable(torch.LongTensor([[rel_index]]))\n",
    "                \n",
    "                # if self.args.device is not None:\n",
    "                rel_index = rel_index.cuda()\n",
    "                \n",
    "                rel_features = self.rel_pos_embed(rel_index).squeeze(0)\n",
    "                \n",
    "                # classification layer\n",
    "                content = self.content(h) \n",
    "                salience = self.salience(h,doc)\n",
    "                novelty = -1 * self.novelty(h,torch.tanh(s))\n",
    "                abs_p = self.abs_pos(abs_features)\n",
    "                rel_p = self.rel_pos(rel_features)\n",
    "                prob = torch.sigmoid(content + salience + novelty + abs_p + rel_p + self.bias)# \n",
    "                \n",
    "                s = s + torch.mm(prob,h)\n",
    "                probs.append(prob)\n",
    "        return torch.cat(probs).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7sY4o7pdWon"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "class Batch:\n",
    "  def __init__(self,batch,word2id):\n",
    "    self.batch=copy.deepcopy(batch.to_numpy())\n",
    "    self.word2id=word2id\n",
    "\n",
    "  def preprocess(self):\n",
    "    idx=0\n",
    "    doc_lens = []\n",
    "    features = []\n",
    "    targets = []\n",
    "\n",
    "    sent_list = []\n",
    "    for doc in self.batch[:,0]:\n",
    "        sent_list += doc\n",
    "        doc_lens.append(len(doc))\n",
    "        targets += list(self.batch[idx,1])\n",
    "        idx+=1\n",
    "\n",
    "    maxlen=0\n",
    "    for sentence in sent_list:\n",
    "      s=sentence.split()\n",
    "      maxlen=max(maxlen,len(s))\n",
    "  \n",
    "    #print(maxlen)\n",
    "\n",
    "    for sentence in sent_list:\n",
    "      words = sentence.strip().split()\n",
    "      sentence = [self.word2id[word] if word in self.word2id else 1 for word in words]\n",
    "      sentence += [0 for _ in range(maxlen - len(sentence))]\n",
    "      features.append(sentence)\n",
    "\n",
    "      \n",
    "\n",
    "    # print(doc_lens)\n",
    "    features = torch.LongTensor(features)    \n",
    "    targets = torch.LongTensor(targets)\n",
    "    \n",
    "    return features,targets,doc_lens\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRFSS6eOKV5L"
   },
   "outputs": [],
   "source": [
    "def eval(net,data_iter,criterion):\n",
    "    net.eval()\n",
    "    total_loss = 0\n",
    "    batch_num = 0\n",
    "    for batch in data_iter:\n",
    "        features,targets,doc_lens = Batch(docs,word2id).preprocess()\n",
    "        #features,targets,_,doc_lens = vocab.make_features(batch)\n",
    "        features,targets = Variable(features), Variable(targets.float())\n",
    "        # if use_gpu:\n",
    "        # features = features.cuda()\n",
    "        targets = targets.cuda()\n",
    "        \n",
    "        probs = net(features,doc_lens)\n",
    "        loss = criterion(probs,targets)\n",
    "        \n",
    "        del features\n",
    "        del targets\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        total_loss += loss.data[0]\n",
    "        batch_num += 1\n",
    "    loss = total_loss / batch_num\n",
    "    net.train()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "id": "mScdfAPZAyVz",
    "outputId": "12474c18-3d9f-4fe1-c47d-336d1c0a412d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Active memory         |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from large pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|       from small pool |       0 B  |       0 B  |       0 B  |       0 B  |\\n|---------------------------------------------------------------------------|\\n| Allocations           |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |       0    |       0    |       0    |       0    |\\n|       from large pool |       0    |       0    |       0    |       0    |\\n|       from small pool |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cG1sW-mEqyjZ",
    "outputId": "61a6f18d-3519-45b7-9715-09e580260295"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN_RNN(\n",
       "  (abs_pos_embed): Embedding(250, 10)\n",
       "  (rel_pos_embed): Embedding(10, 10)\n",
       "  (embed): Embedding(3000002, 300, padding_idx=0)\n",
       "  (word_RNN): GRU(300, 50, batch_first=True, bidirectional=True)\n",
       "  (sent_RNN): GRU(100, 50, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (content): Linear(in_features=100, out_features=1, bias=False)\n",
       "  (salience): Bilinear(in1_features=100, in2_features=100, out_features=1, bias=False)\n",
       "  (novelty): Bilinear(in1_features=100, in2_features=100, out_features=1, bias=False)\n",
       "  (abs_pos): Linear(in_features=10, out_features=1, bias=False)\n",
       "  (rel_pos): Linear(in_features=10, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# del net\n",
    "# torch.cuda.empty_cache()\n",
    "with torch.no_grad():\n",
    "  net = RNN_RNN(weights)\n",
    "  criterion = nn.BCELoss()  \n",
    "  optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)\n",
    "  loss_sum = 0\n",
    "  min_eval_loss = float('Inf')\n",
    "\n",
    "net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "id": "8ZorpX9DWwPG",
    "outputId": "b92ed5d4-4b7b-4b41-bc26-1ce8bc902dc0"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-663c2de892da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                    \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                    )\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 3.35 GiB (GPU 0; 15.90 GiB total capacity; 13.42 GiB already allocated; 1.66 GiB free; 13.42 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "Votrain_loader = DataLoader(Dataset(train_df),batch_size=4)\n",
    "validation_loader=DataLoader(Dataset(validation_df),shuffle=False)\n",
    "# for step, docs in enumerate(train_loader):\n",
    "#         for doc in Batch(docs,word2id).preprocess():\n",
    " \n",
    "counter=0\n",
    "debug=True\n",
    "\n",
    "for epoch in range(5):\n",
    "    for step, docs in enumerate(train_loader):\n",
    "        t1 = time() \n",
    "        features,targets,doc_lens = Batch(docs,word2id).preprocess()\n",
    "\n",
    "        features,targets = Variable(features), Variable(targets.float())\n",
    "        # features = features.cuda()\n",
    "        targets = targets.cuda()\n",
    "\n",
    "        probs = net(features,doc_lens)\n",
    "        loss = criterion(probs,targets)\n",
    "\n",
    "        # del features\n",
    "        del targets\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        clip_grad_norm_(net.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        if debug:\n",
    "              print('Batch ID:%d Loss:%f' %(i,loss.data[0]))\n",
    "              continue\n",
    "        if step % 10 == 0:\n",
    "              cur_loss = eval(net,validation_loader,criterion)\n",
    "              if cur_loss < min_loss:\n",
    "                  min_loss = cur_loss\n",
    "                  best_path = net.save()\n",
    "        t2 = time()\n",
    "        print(t2-t1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wegyQ8Npq84v"
   },
   "outputs": [],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VNFI6ZMyq9hX",
    "outputId": "77f874ab-c29b-4aa0-fcb8-460307359488"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "l = [-0.3456,  0.0395, -0.0858,  0.2474,  0.0617,  0.3307, -0.1371, -0.0514,\n",
    "          0.1422, -0.0682, -0.0608,  0.0484,  0.1597,  0.0441, -0.0443,  0.0722,\n",
    "         -0.0157,  0.2087,  0.2931,  0.0231,  0.0235, -0.2207,  0.3934,  0.0764,\n",
    "          0.2062,  0.0560,  0.2815, -0.0310,  0.2766, -0.0771, -0.0704, -0.0367,\n",
    "          0.3984,  0.0881, -0.2218,  0.0314,  0.2266,  0.2753,  0.3265, -0.2605,\n",
    "          0.2406,  0.2137,  0.0098, -0.0617, -0.1548,  0.3264,  0.4069, -0.1522,\n",
    "         -0.0457,  0.1678,  0.0010, -0.1477, -0.2027,  0.0427,  0.0481, -0.3325,\n",
    "          0.1351, -0.0322,  0.0362,  0.1349, -0.1467,  0.0858, -0.0896,  0.3395,\n",
    "          0.0389,  0.1627,  0.5114,  0.1099,  0.2093, -0.2633, -0.1190,  0.0251,\n",
    "          0.3912,  0.2445,  0.0876,  0.0094, -0.1283,  0.3036, -0.1143,  0.3958,\n",
    "         -0.0818, -0.1621,  0.3846, -0.0903, -0.0272, -0.0226,  0.0713,  0.3382,\n",
    "         -0.1727,  0.1072, -0.1186, -0.3351, -0.4209,  0.3748,  0.4324,  0.0218,\n",
    "          0.0287,  0.1116,  0.2158, -0.0568]\n",
    "len(l)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "IRE_25GB.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
