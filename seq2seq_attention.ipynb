{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext import data\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = []\n",
    "summaries = []\n",
    "num = 1000\n",
    "for idx, f in enumerate(os.listdir('./cnn/cnn_story')):\n",
    "    file = './cnn/cnn_story/' + str(f)\n",
    "    file = open(file)\n",
    "    lines = ''.join(file.readlines())\n",
    "    stories.append(lines)\n",
    "    if idx == num:\n",
    "        break\n",
    "for idx, f in enumerate(os.listdir('./cnn/cnn_summary')):\n",
    "    file = './cnn/cnn_summary/' + str(f)\n",
    "    file = open(file)\n",
    "    lines = ''.join(file.readlines())\n",
    "    summaries.append(lines)\n",
    "    if idx == num:\n",
    "        break\n",
    "assert len(stories) == len(summaries)\n",
    "DATA = {\"story\": [story for story in stories], \"summary\": [summary for summary in summaries]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(DATA, columns=[\"story\", \"summary\"])\n",
    "train, test = train_test_split(df, test_size=0.1)\n",
    "train, validation = train_test_split(train, test_size=0.1)\n",
    "\n",
    "train.to_json(\"train_data.json\", orient=\"records\", lines=True)\n",
    "test.to_json(\"test_data.json\", orient=\"records\", lines=True)\n",
    "validation.to_json(\"validation_data.json\", orient=\"records\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vasu0403/.local/lib/python3.8/site-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "story_field = Field(tokenize = tokenize, init_token = '<sos>', eos_token = '<eos>', lower = True)\n",
    "summary_field = Field(tokenize =tokenize, init_token = '<sos>', eos_token = '<eos>', lower = True)\n",
    "fields = {'story': ('story', story_field), 'summary': ('summary', summary_field)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vasu0403/.local/lib/python3.8/site-packages/torchtext/data/example.py:13: UserWarning: Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('Example class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.', UserWarning)\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data, test_data = data.TabularDataset.splits(path = '',\n",
    "                                        train = 'train_data.json',\n",
    "                                        validation = 'validation_data.json',\n",
    "                                        test = 'test_data.json',\n",
    "                                        format = 'json',\n",
    "                                        fields = fields)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vasu0403/.local/lib/python3.8/site-packages/torchtext/data/iterator.py:48: UserWarning: BucketIterator class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, validation_data, test_data), batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_field.build_vocab(train_data, min_freq = 2)\n",
    "summary_field.build_vocab(train_data, min_freq = 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, encoder_hidden_dim, decoder_hidden_dim, dropout_p):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, encoder_hidden_dim, bidirectional = True)\n",
    "        self.encoder_hidden_to_context = nn.Linear(2*encoder_hidden_dim, decoder_hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        #inp --> [source_len, batch_size]\n",
    "        \n",
    "        Embedding = self.dropout(self.embedding(inp))\n",
    "        \n",
    "        output, hidden = self.rnn(Embedding)\n",
    "        #hidden --> [n_layers, batch, hidden_dim]\n",
    "        \n",
    "        context = torch.tanh(self.encoder_hidden_to_context(torch.cat((hidden[0,:,:], hidden[1,:,:]), dim = 1)))\n",
    "        return output, context "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_hidden, decoder_hidden):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear(2*encoder_hidden + decoder_hidden, decoder_hidden)\n",
    "        self.v = nn.Linear(decoder_hidden, 1)\n",
    "        \n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        \n",
    "        #encoder_outputs --> [source_len, batch_size, encoder_hidden * 2]\n",
    "        #decoder_hidden --> [batch_size, decoder_hidden]\n",
    "        \n",
    "        source_len = encoder_outputs.shape[0]\n",
    "    \n",
    "        hidden = hidden.unsqueeze(1).repeat(1, source_len, 1)\n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((encoder_outputs, decoder_hidden), dim = 2)))\n",
    "        \n",
    "        \n",
    "        weights = self.v(energy).squeeze(2)\n",
    "        \n",
    "        return F.softmax(weights, dim = 1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_size, decoder_emb_size, decoder_hidden_size, encoder_hidden_size, dropout_p, attention):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attention = attention\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, decoder_emb_size)\n",
    "        self.rnn = nn.GRU(encoder_hidden_size*2 + decoder_emb_size, decoder_hidden_size)\n",
    "        self.fc_out = nn.Linear(encoder_hidden_size*2 + decoder_hidden_size + decoder_emb_size, output_size)\n",
    "        self.droput = nn.Dropout(dropout_p)\n",
    "    \n",
    "    def forward(self, inp, hidden, encoder_outputs):\n",
    "        \n",
    "        inp.unsqueeze(0)\n",
    "        embedding = self.embedding(inp)\n",
    "        weights = self.attention(hidden, encoder_outputs)\n",
    "        \n",
    "        weights = weights.unsqueeze(1)\n",
    "        #weights --> [batch_size, 1, src_len]\n",
    "        \n",
    "        #encoder_outputs --> [src_len, batch_size, encoder_hidden_dim*2]\n",
    "        encoder_outputs.permute(1, 0, 2)\n",
    "        #encoder_outputs --> [batch_size, src_len, encoder_hidden_dim*2]\n",
    "        \n",
    "        weighted = torch.bmm(weights, encoder_outputs)\n",
    "        #weighted --> [batch_size, 1, encoder_hidden_dim*2]\n",
    "        weighted.permute(1, 0, 2)\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        #rnn_input --> [1, batch_size, decoder_emb_size + 2*encoder_hidden_size]\n",
    "        \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        output = output.squeeze(0)\n",
    "        embedding = embedding.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = slef.fc_out(torch.cat((output, hidden, weighted), dim = 1))\n",
    "        \n",
    "        return prediction, hidden.squeeze(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, inp, expected_out, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #inp --> [source_len, batch_size]\n",
    "        #expected_out --> [target_len, batch_size]\n",
    "        \n",
    "        batch_size = inp.shape[0]\n",
    "        target_len = expected_out.shape[0]\n",
    "        target_vocab_size = self.decoder.output_size\n",
    "        \n",
    "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n",
    "        \n",
    "        encoder_outputs, hidden = self.encoder(inp)\n",
    "        \n",
    "        inp = expected_out[0, :]\n",
    "        for i in range(1, target_len):\n",
    "            output, hidden = self.decoder(inp, hidden, encoder_outputs)\n",
    "            outputs[i] = output\n",
    "            \n",
    "            teacher_force = True if random.random() > teacher_forcing_ration else False\n",
    "            \n",
    "            top1 = output.argmax(1)\n",
    "            \n",
    "            if teacher_force:\n",
    "                inp = expected_out[i]\n",
    "            else:\n",
    "                inp = top1\n",
    "                \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(story_field.vocab)\n",
    "OUTPUT_DIM = len(summary_field.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, ENC_HID_DIM, DEC_DROPOUT, attn)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(17866, 256)\n",
       "    (rnn): GRU(256, 512, bidirectional=True)\n",
       "    (encoder_hidden_to_context): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
       "      (v): Linear(in_features=512, out_features=1, bias=True)\n",
       "    )\n",
       "    (embedding): Embedding(3717, 256)\n",
       "    (rnn): GRU(1280, 512)\n",
       "    (fc_out): Linear(in_features=1792, out_features=3717, bias=True)\n",
       "    (droput): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG_PAD_IDX = summary_field.vocab.stoi[summary_field.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        story = batch.story\n",
    "        summary = batch.summary\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(story, summary)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        target = summary.view(-1)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vasu0403/.local/lib/python3.8/site-packages/torchtext/data/batch.py:23: UserWarning: Batch class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.\n",
      "  warnings.warn('{} class will be retired in the 0.8.0 release and moved to torchtext.legacy. Please see 0.7.0 release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 1\n",
    "CLIP = 1\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    validation_loss = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if validation_loss < best_valid_loss:\n",
    "        best_valid_loss = validation_loss\n",
    "        torch.save(model.state_dict(), 'seq2seq_attention.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
