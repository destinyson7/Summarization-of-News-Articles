{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZ9P8kk08gHx"
   },
   "source": [
    "## IRE Major Project - Summarization using Pointer Generator Networks and Coverage Mechanism \n",
    "Contributed by:<br>\n",
    "**Vasu Singhal** (2018101074)<br>\n",
    "**Tanish Lad** (2018114005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKHEAVTV9KBy"
   },
   "source": [
    "### Downloading Dataset\n",
    "The preprocessed data is already hosted on a google drive link. We are getting the dataset from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r0UJo2ZH8qQN"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n",
    "\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once per notebook.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "file_id = '0BzQ6rtO2VN95a0c3TlZCWkl3aU0' # URL id. \n",
    "downloaded = drive.CreateFile({'id': file_id})\n",
    "downloaded.GetContentFile('finished_files.zip')\n",
    "!unzip finished_files.zip\n",
    "\n",
    "!pip install pyrouge\n",
    "!mkdir log\n",
    "!pip install tensorflow==1.15.0\n",
    "!pip install tensorflow-gpu==1.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_mgy0UJu-GOc"
   },
   "source": [
    "Importing the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g9gVYLcCP8Jp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import config\n",
    "from numpy import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from dataset import Vocab\n",
    "from dataset import Batcher\n",
    "from utils import get_input_from_batch\n",
    "from utils import get_output_from_batch\n",
    "from utils import calc_running_avg_loss\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import time\n",
    "import os\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVHcpASNKMeP"
   },
   "source": [
    "### BasicModule class\n",
    "This class initializes the parameters of model with a uniform or normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fKKzikd7RCVa"
   },
   "outputs": [],
   "source": [
    "class BasicModule(nn.Module):\n",
    "    def __init__(self, init='uniform'):\n",
    "        super(BasicModule, self).__init__()\n",
    "        self.init = init\n",
    "\n",
    "    def init_params(self):\n",
    "        for param in self.parameters():\n",
    "            if param.requires_grad and len(param.shape) > 0:\n",
    "                stddev = 1 / math.sqrt(param.shape[0])\n",
    "                if self.init == 'uniform':\n",
    "                    torch.nn.init.uniform_(param, a=-0.05, b=0.05)\n",
    "                elif self.init == 'normal':\n",
    "                    torch.nn.init.normal_(param, std=stddev)\n",
    "                elif self.init == 'truncated_normal':\n",
    "                    self.truncated_normal_(param, mean=0,std=stddev)\n",
    "\n",
    "    def truncated_normal_(self, tensor, mean=0, std=1.):\n",
    "        size = tensor.shape\n",
    "        tmp = tensor.new_empty(size + (4,)).normal_()\n",
    "        valid = (tmp < 2) & (tmp > -2)\n",
    "        ind = valid.max(-1, keepdim=True)[1]\n",
    "        tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n",
    "        tensor.data.mul_(std).add_(mean)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ORMveviL-VNg"
   },
   "source": [
    "### Encoder class.<br> \n",
    "The encoder class of this model is exactly similar to the encoder class of a normal seq2seq model.<br>Here also, the input sequence which is passed to the encoder is already padded with the pad token. We first pack this padded sequence before passing it through the LSTM. The output of the LSTM is the packed output, which is again padded later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L4QbzH4wQQmf"
   },
   "outputs": [],
   "source": [
    "class Encoder(BasicModule):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.src_word_emb = nn.Embedding(config.vocab_size, config.emb_dim)\n",
    "        self.lstm = nn.LSTM(config.emb_dim, config.hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(config.hidden_dim * 2, config.hidden_dim * 2, bias=False)\n",
    "        self.init_params()\n",
    "\n",
    "    def forward(self, input, seq_lens):\n",
    "        embedded = self.src_word_emb(input)\n",
    "        packed = pack_padded_sequence(embedded, seq_lens, batch_first=True)\n",
    "        output, hidden = self.lstm(packed)\n",
    "        encoder_outputs, _ = pad_packed_sequence(output, batch_first=True)  # shape = batch_size x sequence_length x 2*hidden_dim\n",
    "        encoder_outputs = encoder_outputs.contiguous()\n",
    "        encoder_feature = encoder_outputs.view(-1, 2 * config.hidden_dim)   # batch_size*sequence_length x 2*hidden_dim\n",
    "        encoder_feature = self.fc(encoder_feature)\n",
    "        return encoder_outputs, encoder_feature, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-TBQGmOBC2h"
   },
   "source": [
    "### Attention class.\n",
    "Attention class of this model, is also very similar to that of a seq2seq2 model.<br>\n",
    "There is only one major difference. The attention weights are calculated by keeping in mind the attention that was given to all the words in all the previous time stamps. The intution behind this is that, we do not want to give attention to the same word again and again. This should help is to get rid of the repeating word problem that we faced in the previous model.<br>\n",
    "The information about previous attention weights is kept in a coverage vector, which is updated once the new attention weights are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aVmilN7aRhlD"
   },
   "outputs": [],
   "source": [
    "class Attention(BasicModule):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        self.fc = nn.Linear(config.hidden_dim * 2, 1, bias=False)\n",
    "        self.dec_fc = nn.Linear(config.hidden_dim * 2, config.hidden_dim * 2)  \n",
    "        self.init_params()\n",
    "                        \n",
    "    def forward(self, s_t, enc_out, enc_fea, enc_padding_mask):\n",
    "        b, l, n = list(enc_out.size())\n",
    "        dec_fea = self.dec_fc(s_t)  # Batch_size x 2*hidden_dim\n",
    "        dec_fea_expanded = dec_fea.unsqueeze(1).expand(b, l, n).contiguous() \n",
    "        dec_fea_expanded = dec_fea_expanded.view(-1, n)    \n",
    "        \n",
    "        # Combining encoder ouputs and decoder hidden state to calulate new attenion weights  \n",
    "        att_features = enc_fea + dec_fea_expanded                         \n",
    "        e = torch.tanh(att_features)                        \n",
    "        scores = self.fc(e)                                \n",
    "        scores = scores.view(-1, l)                        \n",
    "        attn_dist_ = F.softmax(scores, dim=1) * enc_padding_mask \n",
    "        normalization_factor = attn_dist_.sum(1, keepdim=True)\n",
    "        attn_dist = attn_dist_ / normalization_factor\n",
    "        attn_dist = attn_dist.unsqueeze(1)                        \n",
    "        c_t = torch.bmm(attn_dist, enc_out)                     \n",
    "        c_t = c_t.view(-1, config.hidden_dim * 2)                 \n",
    "        attn_dist = attn_dist.view(-1, l)                           \n",
    "\n",
    "        return c_t, attn_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yU1VdNFKCniJ"
   },
   "source": [
    "### Decoder class\n",
    "The deocder class, like in any seq2seq model, is used to give the next word of the summary by taking into account the last output word (or the last correct word, in case of teacher forcing) and the weighted sum of encoder outputs, which is given by the attention class. <br>\n",
    "The key difference here is that, instead of just using the probability distribution over the entire vocabulary like in the previous seq2seq model, we sometimes use the probability distrubtion given by attention weights over the input sequence. <br>\n",
    "This has two advantages:<br>\n",
    "**1.** We increase the probability of a word being directly copied from the original article, which is likely to produce factually correct summaries.<br>\n",
    "**2.** The model will now be able to produce summaries which can contain out of vocubulary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DEx5mN3RF3Z"
   },
   "outputs": [],
   "source": [
    "class Decoder(BasicModule):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.attention_network = Attention()\n",
    "        self.tgt_word_emb = nn.Embedding(config.vocab_size, config.emb_dim)\n",
    "        self.con_fc = nn.Linear(config.hidden_dim * 2 + config.emb_dim, config.emb_dim)\n",
    "        self.lstm = nn.LSTM(config.emb_dim, config.hidden_dim, batch_first=True, bidirectional=False)\n",
    "        self.p_gen_fc = nn.Linear(config.hidden_dim * 4 + config.emb_dim, 1)\n",
    "        self.fc1 = nn.Linear(config.hidden_dim * 3, config.hidden_dim)\n",
    "        self.fc2 = nn.Linear(config.hidden_dim, config.vocab_size)\n",
    "\n",
    "        self.init_params()\n",
    "\n",
    "    def forward(self, y_t, s_t, enc_out, enc_fea, enc_padding_mask, c_t, extra_zeros, enc_batch_extend_vocab, step):\n",
    "\n",
    "        if not self.training and step == 0:\n",
    "            dec_h, dec_c = s_t\n",
    "            s_t_hat = torch.cat((dec_h.view(-1, config.hidden_dim), dec_c.view(-1, config.hidden_dim)), 1)         # Batch_size x 2*hidden_dim\n",
    "            c_t, _ = self.attention_network(s_t_hat, enc_out, enc_fea, enc_padding_mask)\n",
    "\n",
    "        y_t_embd = self.tgt_word_emb(y_t)                                       # embedding of input word\n",
    "        x = self.con_fc(torch.cat((c_t, y_t_embd), 1))\n",
    "        lstm_out, s_t = self.lstm(x.unsqueeze(1), s_t)\n",
    "        dec_h, dec_c = s_t\n",
    "        s_t_hat = torch.cat((dec_h.view(-1, config.hidden_dim), dec_c.view(-1, config.hidden_dim)), 1)             # Batch_size x 2*hidden_dim\n",
    "        c_t, attn_dist= self.attention_network(s_t_hat, enc_out, enc_fea, enc_padding_mask)\n",
    "\n",
    "\n",
    "        # Calculation of pointer generation probability\n",
    "        p_gen_inp = torch.cat((c_t, s_t_hat, x), 1)                             # Batch_size x (2*2*hidden_dim + emb_dim)    Calculated from decoder hidden state, encoder outputs and input word\n",
    "        p_gen = self.p_gen_fc(p_gen_inp)\n",
    "        p_gen = torch.sigmoid(p_gen)\n",
    "\n",
    "        # Probability distribution over vocab\n",
    "        output = torch.cat((lstm_out.view(-1, config.hidden_dim), c_t), 1)      # Batch_size x hidden_dim*3\n",
    "        output = self.fc1(output)                                               # Batch_size x hidden_dim\n",
    "        output = self.fc2(output)                                               # Batch_size x vocab_size\n",
    "        vocab_dist = F.softmax(output, dim=1)                                   # Converting to a probability disribution\n",
    "        vocab_dist_ = p_gen * vocab_dist                      \n",
    "        attn_dist_ = (1 - p_gen) * attn_dist                                    # Probaility distribution over input sequence\n",
    "        final_dist = vocab_dist_.scatter_add(1, enc_batch_extend_vocab, attn_dist_)           # Combining both the probability distributions\n",
    "        return final_dist, s_t, c_t, attn_dist, p_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrPAM0G-G75Z"
   },
   "source": [
    "### Model class\n",
    "This is just a wrapper class which combines all the different components of model in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DE_cVBrNQOiU"
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, model_path=None, is_eval=False):\n",
    "        encoder = Encoder()\n",
    "        decoder = Decoder()\n",
    "        reduce_state = ReduceState()\n",
    "        decoder.tgt_word_emb.weight = encoder.src_word_emb.weight\n",
    "\n",
    "        if is_eval:\n",
    "            encoder = encoder.eval()\n",
    "            decoder = decoder.eval()\n",
    "            reduce_state = reduce_state.eval()\n",
    "\n",
    "        if use_cuda:\n",
    "            encoder = encoder.cuda()\n",
    "            decoder = decoder.cuda()\n",
    "            reduce_state = reduce_state.cuda()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.reduce_state = reduce_state\n",
    "\n",
    "        if model_path is not None:\n",
    "            state = torch.load(model_path, map_location=lambda storage, location: storage)\n",
    "            self.encoder.load_state_dict(state['encoder_state_dict'])\n",
    "            self.decoder.load_state_dict(state['decoder_state_dict'], strict=False)\n",
    "            self.reduce_state.load_state_dict(state['reduce_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wITrwkEWSjvO"
   },
   "outputs": [],
   "source": [
    "class Train(object):\n",
    "    def __init__(self):\n",
    "        print(config.vocab_path)\n",
    "        self.vocab = Vocab(config.vocab_path, config.vocab_size)\n",
    "        self.batcher = Batcher(self.vocab, config.train_data_path, config.batch_size, single_pass=False, mode='train')\n",
    "        time.sleep(10)\n",
    "        train_dir = os.path.join(config.log_root, 'train_%d' % (int(time.time())))\n",
    "        if not os.path.exists(train_dir):\n",
    "            os.mkdir(train_dir)\n",
    "\n",
    "        self.model_dir = os.path.join(train_dir, 'models')\n",
    "        if not os.path.exists(self.model_dir):\n",
    "            os.mkdir(self.model_dir)\n",
    "\n",
    "        self.summary_writer = tf.summary.FileWriter(train_dir)\n",
    "\n",
    "    def save_model(self, running_avg_loss, iter):\n",
    "        state = {\n",
    "            'iter': iter,\n",
    "            'encoder_state_dict': self.model.encoder.state_dict(),\n",
    "            'decoder_state_dict': self.model.decoder.state_dict(),\n",
    "            'reduce_state_dict': self.model.reduce_state.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'current_loss': running_avg_loss\n",
    "        }\n",
    "        model_save_path = os.path.join(self.model_dir, 'model_%d_%d' % (iter, int(time.time())))\n",
    "        torch.save(state, model_save_path)\n",
    "\n",
    "    def setup_train(self, model_path=None):\n",
    "        self.model = Model(model_path)\n",
    "        initial_lr = config.lr\n",
    "\n",
    "        params = list(self.model.encoder.parameters()) + list(self.model.decoder.parameters()) + \\\n",
    "                 list(self.model.reduce_state.parameters())\n",
    "        total_params = sum([param[0].nelement() for param in params])\n",
    "        print('The Number of params of model: %.3f million' % (total_params))\n",
    "        self.optimizer = optim.Adagrad(params, lr=initial_lr, initial_accumulator_value=config.adagrad_init_acc)\n",
    "\n",
    "        start_iter, start_loss = 0, 0\n",
    "\n",
    "        if model_path is not None:\n",
    "            state = torch.load(model_path, map_location=lambda storage, location: storage)\n",
    "            start_iter = state['iter']\n",
    "            start_loss = state['current_loss']\n",
    "            \n",
    "            self.optimizer.load_state_dict(state['optimizer'])\n",
    "                if use_cuda:\n",
    "                    for state in self.optimizer.state.values():\n",
    "                        for k, v in state.items():\n",
    "                            if torch.is_tensor(v):\n",
    "                                state[k] = v.cuda()\n",
    "\n",
    "        return start_iter, start_loss\n",
    "\n",
    "    def train_one_batch(self, batch):\n",
    "        enc_batch, enc_lens, enc_pos, enc_padding_mask, enc_batch_extend_vocab, extra_zeros, c_t = get_input_from_batch(batch, use_cuda)\n",
    "        dec_batch, dec_lens, dec_pos, dec_padding_mask, max_dec_len, tgt_batch = get_output_from_batch(batch, use_cuda)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        enc_out, enc_fea, enc_h = self.model.encoder(enc_batch, enc_lens)\n",
    "\n",
    "        s_t = self.model.reduce_state(enc_h)\n",
    "\n",
    "        step_losses, cove_losses = [], []\n",
    "        for di in range(min(max_dec_len, config.max_dec_steps)):\n",
    "            y_t = dec_batch[:, di]\n",
    "            final_dist, s_t, c_t, attn_dist, p_gen = self.model.decoder(y_t, s_t, enc_out, enc_fea, enc_padding_mask, c_t, extra_zeros, enc_batch_extend_vocab, di)\n",
    "            tgt = tgt_batch[:, di]\n",
    "            step_mask = dec_padding_mask[:, di]\n",
    "            gold_probs = torch.gather(final_dist, 1, tgt.unsqueeze(1)).squeeze()\n",
    "            step_loss = -torch.log(gold_probs + config.eps)\n",
    "\n",
    "            step_loss = step_loss * step_mask\n",
    "            step_losses.append(step_loss)\n",
    "\n",
    "        sum_losses = torch.sum(torch.stack(step_losses, 1), 1)\n",
    "        batch_avg_loss = sum_losses / dec_lens\n",
    "        loss = torch.mean(batch_avg_loss)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        clip_grad_norm_(self.model.encoder.parameters(), config.max_grad_norm)\n",
    "        clip_grad_norm_(self.model.decoder.parameters(), config.max_grad_norm)\n",
    "        clip_grad_norm_(self.model.reduce_state.parameters(), config.max_grad_norm)\n",
    "\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item(), 0\n",
    "\n",
    "\n",
    "    def run(self, n_iters, model_path=None):\n",
    "        iter, running_avg_loss = self.setup_train(model_path)\n",
    "        start = time.time()\n",
    "        interval = 100\n",
    "\n",
    "        while iter < n_iters:\n",
    "            batch = self.batcher.next_batch()\n",
    "            loss, cove_loss = self.train_one_batch(batch)\n",
    "\n",
    "            running_avg_loss = calc_running_avg_loss(loss, running_avg_loss, self.summary_writer, iter)\n",
    "            iter += 1\n",
    "\n",
    "            if iter % interval == 0:\n",
    "                self.summary_writer.flush()\n",
    "                print(\n",
    "                    'step: %d, second: %.2f , loss: %f, cover_loss: %f' % (iter, time.time() - start, loss, cove_loss))\n",
    "                start = time.time()\n",
    "            if iter % 5000 == 0:\n",
    "                self.save_model(running_avg_loss, iter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WiEgDr2MG5a6"
   },
   "source": [
    "### Reduce state class\n",
    "Used to reduce the dimension of the vector which is passed as an argument to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9pXte-aQpM9"
   },
   "outputs": [],
   "source": [
    "class ReduceState(BasicModule):\n",
    "    def __init__(self):\n",
    "        super(ReduceState, self).__init__()\n",
    "        self.reduce_h = nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
    "        self.reduce_c = nn.Linear(config.hidden_dim * 2, config.hidden_dim)\n",
    "        self.init_params()\n",
    "\n",
    "\n",
    "    def forward(self, hidden):\n",
    "        h, c = hidden  # h, c dim = 2 x b x hidden_dim\n",
    "        h_in = h.transpose(0, 1).contiguous().view(-1, config.hidden_dim * 2)\n",
    "        hidden_reduced_h = F.relu(self.reduce_h(h_in))\n",
    "        c_in = c.transpose(0, 1).contiguous().view(-1, config.hidden_dim * 2)\n",
    "        hidden_reduced_c = F.relu(self.reduce_c(c_in))\n",
    "        return (hidden_reduced_h.unsqueeze(0), hidden_reduced_c.unsqueeze(0))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "id": "wW9A_RejSsTi",
    "outputId": "e8f7ecef-0b02-4749-e801-c1a4c351e828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./finished_files/vocab\n",
      "HERERERERERE\n",
      "Finished constructing vocabulary of 50 total words. Last word added: their\n",
      "INFO:tensorflow:Bucket queue size: 0, Input queue size: 0\n",
      "The Number of params of model: 314.000 million\n"
     ]
    }
   ],
   "source": [
    "train_processor = Train()\n",
    "train_processor.run(config.max_iterations)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Pointer_Generator_Coverage.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
